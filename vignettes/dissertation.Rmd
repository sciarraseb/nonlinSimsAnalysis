---
shorttitle        : "Measurement timing"
format          : "pandoc"
header-includes:
  - \usepackage{nccmath}
  - \usepackage{caption}
  - \usepackage{textcomp} #for copyright symbol on title page
  - \usepackage{longtable}
  - \usepackage{setspace}
  - \usepackage{biblatex}
  - \usepackage{booktabs}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \usepackage{xcolor}
  - \usepackage{amsthm} 
  - \usepackage{amsmath} ##needed for argmax
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \usepackage{setspace} #needed to doublespace caption text (using \doublespacing)
  - \usepackage[labelfont = {bf, up}]{caption} 
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \usepackage{upgreek}  #required for non-italicized Greek letters
  - \usepackage{subcaption}
  - \captionsetup[figure]{labelfont={normalfont, bf}, singlelinecheck=false, labelsep=newline}
  - \DeclareCaptionJustification{double}{\DoubleSpacing}
  - \DeclareCaptionFont{figCaptionFont}{\fontfamily{phv}\doublespacing} #sets caption font to sans serif font of Helvetica 
  - \DeclareCaptionFont{figCaptionSize}{\footnotesize} #set caption font size to footnote 
  - \DeclareCaptionFont{figCaptionStyle}{\textup}  #set caption font to non-italicized font  
  - \DeclareCaptionLabelSeparator{captionSep}{\newline\newline} #separates figure label and figure title with required white space
  - \captionsetup[figure]{labelfont={figCaptionStyle, bf}, font = {figCaptionFont,figCaptionSize, figCaptionStyle}, labelsep = captionSep, justification=raggedright}
  - \captionsetup[table]{font = {figCaptionFont,figCaptionSize,figCaptionStyle}, labelfont={bf}, labelsep=captionSep, justification = raggedright, margin = {0cm,0cm}}
  - \newenvironment{helvenv}{\fontfamily{phv}\selectfont}{}
  - \raggedbottom #ensures text starts from top of page and any white space is at the botom

#environment numbering 
  - \setcounter{section}{0} 
  - \makeatletter \renewcommand\thesection{} \renewcommand\thesubsection{\@arabic\c@section.\@arabic\c@subsection} \makeatother
  
  - \newtheorem{theorem}{Theorem}
  - \newtheorem{example}[theorem]{Example}
  
  #modifies heading levels of 4-5 to follow apa7
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

  
author: 
  - name          : "Sebastian Sciarra "
    affiliation   : "1"
    corresponding : yes    
    email         : "ssciarra@uoguelph.ca"
affiliation: 
  - id            : "1"
    institution   : "University of Guelph"
keywords          : "measurement timing, nonlinear "
wordcount         : "5554 words"
floatsintext      : yes
linkcolor         : blue
figsintext        : yes 
figurelist        : no
tablelist         : no
footnotelist      : no
numbersections    : yes
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa7"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: 72
bibliography: dissertation_references.bib
---

```{r package_loading, include=F}
#load packages
library(easypackages)
packages <- c('devtools','tidyverse', 'RColorBrewer', 'parallel', 'data.table', 'kableExtra', 'ggtext', 'egg', 'nonlinSims','papaja', 
              'ggbrace', 'cowplot')
libraries(packages)
load_all()
```


```{r knitting_setup, echo=F, message = F, warning = F}
#import raw data files (needed for computing variances)
exp_1_raw <- convert_raw_var_to_sd(raw_data = read_csv('data/exp_1_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "measurement_spacing", "midpoint"), factor)
  
exp_2_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_2_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "measurement_spacing", "sample_size"), factor)

exp_3_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_3_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "time_structuredness", "sample_size"), factor)

#unfiltered data 
param_summary_exp_1 <- readRDS(file = 'data/uf_param_summary_exp_1.RData')
param_summary_exp_2 <- readRDS(file = 'data/uf_param_summary_exp_2.RData')
param_summary_exp_3 <- readRDS(file = 'data/uf_param_summary_exp_3.RData')

#create analytical versions of summary data + converts vars to sds
exp_1_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_1, exp_num = '1')
exp_2_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_2, exp_num = '2')
exp_3_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_3, exp_num = '3')

combined_analytical_exp_1 <- rbind(exp_1_analytical$likert, exp_1_analytical$days)
combined_analytical_exp_2 <- rbind(exp_2_analytical$likert, exp_2_analytical$days)
combined_analytical_exp_3 <- rbind(exp_3_analytical$likert, exp_3_analytical$days)

#create condition summary data sets 
cond_summary_exp_1 <- compute_condition_summary(param_summary_data = combined_analytical_exp_1, facet_var = 'measurement_spacing', 
                          ind_vars = c('number_measurements', 'measurement_spacing', 'midpoint'))
cond_summary_exp_2 <- compute_condition_summary(param_summary_data = combined_analytical_exp_2, facet_var = 'measurement_spacing', 
                  ind_vars = c('number_measurements', 'measurement_spacing', 'sample_size'))

cond_summary_exp_3 <- compute_condition_summary(param_summary_data = combined_analytical_exp_3, facet_var = 'time_structuredness', 
                          ind_vars = c('number_measurements', 'sample_size', 'time_structuredness'))
```

```{r pre_knitting_setup_unfiltered, echo=F, eval=F, include=F}
#code should be computed before knitting to decrease knitting time 
#load data from experiments
exp_1 <- read_csv(file = 'data/exp_1_zero.csv')
exp_2 <- read_csv(file = 'data/exp_2_zero.csv')
exp_3 <- read_csv(file = 'data/exp_3_zero.csv')

#compute parameter summary statistics  
param_summary_exp_1 <- compute_parameter_summary(data = exp_1, exp_num = 1)
param_summary_exp_2 <- compute_parameter_summary(data = exp_2, exp_num = 2)
param_summary_exp_3 <- compute_parameter_summary(data = exp_3, exp_num = 3)

#necessary factor conversions 
param_summary_exp_1$number_measurements <- factor(param_summary_exp_1$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_1$midpoint <- factor(param_summary_exp_1$midpoint, levels = c(80, 180,280))

param_summary_exp_2$number_measurements <- factor(param_summary_exp_2$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_2$sample_size <- factor(param_summary_exp_2$sample_size, levels = c(30, 50, 100, 200, 500, 1000))

param_summary_exp_3$number_measurements <- factor(param_summary_exp_3$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_3$sample_size <- factor(param_summary_exp_3$sample_size, levels = c(30, 50, 100, 200, 500, 1000))

#write data sets 
#save parameter summary files as RData files so that metadata are correctly stored (e.g., factor levels, variable types)
saveRDS(object = param_summary_exp_1, file = 'data/uf_param_summary_exp_1.RData')
saveRDS(object = param_summary_exp_2, file = 'data/uf_param_summary_exp_2.RData')
saveRDS(object = param_summary_exp_3, file = 'data/uf_param_summary_exp_3.RData')
```

```{=tex}
\begin{titlepage}
  \begin{center}
    \vspace*{3cm}
    
  \textbf{Is Timing Everything? Measurement Timing and the Ability to Sccurately Model Longitudinal Data}
    
  \vspace{2cm} 
    by \\ Sebastian Sciarra 
    
      
  \vspace{2cm} 
   In partial fulfilment of requirements \\ 
   for the degree of \\
   Doctor of Philosophy \\ 
   in \\ 
   Psychology
    

  \vspace{2cm} 
    Guelph, Ontario, Canada \\ 
    \textcopyright \text{ Sebastian Sciarra, September 2022}

  
  \end{center}
\end{titlepage}
```


# Introduction

```{=tex}
\nointerlineskip
\vfill
\newpage
```

"Neither the behavior of human beings nor the activities of organizations can be defined without reference to time, and temporal aspects are critical for understanding them" [@navarro2015, p.136].

The topic of time has received a considerable amount of
attention in organizational psychology over the past 20 years. Examples
of well-received articles published around the beginning of the 21^st^
century discuss how investigating time is important for
understanding patterns of change and boundary conditions of theory
[@zaheer1999], how longitudinal research is necessary for disentangling
different types of causality [@mitchell2001], and explicate a pattern
of organizational change [or institutionalization\; @lawrence2001].
Since then, articles have emphasized the need to address time in
specific areas such as performance [@fisher2008; @dalal2014], teams [@roe2012], and goal setting [@fried2004] and, more generally, throughout organizational research [@george2000; @roe2008; @ployhart2010; @sonnentag2012; @navarro2015; @shipp2015; @kunisch2017; @vantilborgh2018; @aguinis2021]. 

The importance of time has also been recognized in organizational theory. In defining a theoretical contribution, @whetten1989 discussed that time must be discussed in regard to setting boundary conditions (i.e., under what circumstances does the theory apply) and in specifying relations between variables over time [see also @mitchell2001; @george2000]. Even if a considerable number of organizational theories do not adhere to the definition of @whetten1989, theoretical models in organizational psychology consist of path diagrams that delineate the causal underpinnings of a process. Given that temporal precedence is a necessary condition for establishing causality [@mill2011], time has a role, whether implicitly or explicitly, in organizational theory. 

(ref:maxwell2007) @maxwell2007
(ref:roe2014) @roe2014
(ref:mitchell2013) @mitchell2013

Despite the considerable emphasis that has been placed on investigating processes over time and its ubiquity in organizational theory, the prevalence of longitudinal research has historically remained low. One study examined the prevalence of longitudinal research from 1970--2006 across five organizational psychology journals and found that 4% of articles used longitudinal designs (Roe, 2014). Another survey of two applied psychology journals in 2005 found that approximaely 10% (10 of 105 studies) of studies used longitudinal designs [@roe2008]. Similarly, two surveys of studies employing longitudinal designs with mediation analysis found that, across five journals, only about 10% (7 of 72 studies) did so in 2005 [@maxwell2007] and approximately 16% (15 of 92 studies) did so in 2006 [@mitchell2013].\footnote{Note that the definition of a longitudinal design in (ref:maxwell2007) and (ref:mitchell2013) required that measurements be taken over at least three time points so that measurements of the predictor, mediator, and outcome variables were separated over time.} Thus, the prevalence of longitudinal research has remained low. 

In the six sections that follow, I will explain why longitudinal research is necessary and the factors that must be considered when conducting such research. In the first section, I will explain why conducting longitudinal research is essential for understanding the dynamics of psychological processes. In the second section, I will overview patterns of change that are likely to emerge over time. In the the third section, I will overview design and analytical issues involved in designing longitudinal studies. In the fourth section, I will explain how design and analytical issues encountered in conducting longitudinal research can be investigated. In the fifth section, I will provide a systematic review of the research that has investigated design and analytical issues involved in conducting longitudinal research. Finally, in the sixth section, I will briefly explain strategies for modelling nonlinear change. A summary of the three simulation experiments that I conducted in my dissertation will then be provided. 

## The Need to Conduct Longitudinal Research

Longitudinal research provides substantial advantages over cross-sectional research. Unfortunately, researchers commonly discuss the results of cross-sectional analyses as if they have been obtained with a longitudinal design. However, cross-sectional and longitudinal analyses often produce different results. Oneexample of the assumption that cross-sectional findings are equivalent to longitudinal findings comes from the large number of studies employing mediation analysis. Given that mediation is used to understand chains of causality in psychological processes [@baron1986], it would thus make sense to pair mediation analysis with a longitudinal design because understanding causality, after all, requires temporal precedence. Unfortunately, the majority of studies that have used mediation analysis have done so using cross-sectional designs---with estimates of approximately 90% [@maxwell2007] and 84% [@mitchell2013]---and have often discussed the results as if they were longitudinal. Investigations into whether mediation results remain equivalent across cross-sectional and longitudinal designs have repeatedly concluded that using mediation analysis on cross-sectional data can return different, and sometimes completely opposite, results from using it on longitudinal data [@cole2003; @maxwell2007; @maxwell2011; @mitchell2013; @olaughlin2018]. Therefore, mediation analyses based on cross-sectional analyses may be misleading. 

The non-equivalence of cross-sectional and longitudinal results that occurs with mediation analysis is, unfortunately, not due to a specific set of circumstances that only arise with mediation analysis, but a consequence of a broader systematic cause that affects the results of almost every analysis. The concept of ergodicity explains why cross-sectional and longitudinal analyses seldom yield similar results. To understand ergodicity, it is first important to realize that variance is central to many statistical analyses---correlation, regression, factor analysis, and mediation are some examples. Thus, if variance remains unchanged across cross-sectional and longitudinal data sets, then analyses of either data set would return the same results. Importantly, variance only remains equal across cross-sectional and longitudinal data sets if two conditions put forth by ergodic theory are satisfied [homogeneity and stationarity\; @molenaar2004; @molenaar2009]. If these two conditions are met, then a process is said to be ergodic. Unfortunately, the two conditions required for ergodicity are highly unlikely to be satisfied and so cross-sectional findings will frequently deviate from longitudinal findings (see [Technical Appendix A][Technical Appendix A: Ergodicity and the Need to Conduct Longitudinal Research] for more information). 

(ref:fisher2018) @fisher2018

Given that cross-sectional and longitudinal analyses are, in general, unlikely to return equivalent findings, it is unsurprising that several investigations in organizational research---and psychology as a whole---have found these analyses to return different results. Beginning with an example from @curran2011, heart attacks are less likely to occur in people who exercise regularly (longitudinal finding), but more likely to happen when exercising (cross-sectional finding). Correlational studies find differences in correlation magnitudes between cross-sectional and longitudinal data sets [see @nixon2011 for a meta-analytic review\; @fisher2018].\footnote{Note that (ref:fisher2018) also found the variability of longitudinal correlations to be considerably larger than the variability of cross-sectional correlations.} Moving on to perhaps the most commonly employed analysis in organizational research of mediation, several articles have highlighted cross-sectional data can return different, and sometimes completely opposite, results to longitudinal data [@cole2003; @maxwell2007; @maxwell2011; @olaughlin2018]. Factor analysis is perhaps the most interesting example: The well-documented five-factor model of personality seldom arises when analyzing person-level data that was obtained by measuring personality on 90 consecutive days [@hamaker2005]. Therefore, cross-sectional analyses are rarely equivalent to longitudinal analyses. 

Fortunately, technological advancements have allowed researchers to more easily conduct longitudinal research in two ways. First, the use of the experience sampling method [@beal2015] in conjunction with modern information transmission technologies---whether through phone applications or short message services---allows data to sometimes be sampled over time with relative ease. Second, the development of analyses for longitudinal data (along with their integration in commonly used software) that enable person-level data to be modelled such as multilevel models [@raudenbush2002], growth mixture models [@wang2007], and dynamic factor analysis [@ram2013] provide researchers with avenues to explore the temporal dynamics of psychological processes. With one recent survey estimating that 43.3% of mediation studies (26 of 60 studies) used a longitudinal design [@olaughlin2018], it appears that the prevalence of longitudinal research has increased from the 9.5% [@roe2008] and 16.3% [@mitchell2013] values estimated at the beginning of the 21^st^ century. Although the frequency of longitudinal research appears to have increased over the past 20 years, several avenues exist where the quality of longitudinal research can be improved, and in my dissertation, I focus on investigating these avenues.  

## Understanding Patterns of Change That Emerge Over Time

Change can occur in many ways over time. One pattern of change commonly assumed to occur over time is that of linear change. When change follows a linear pattern, the rate of change over time remains constant. Unfortunately, a linear pattern places demanding restrictions on possible patterns of change. If change were to follow a linear pattern, then any pauses in change (or plateaus) or changes in direction would not occur and effects would simply grow over time. Unfortunately, effect sizes have been shown to diminish over time [for meta-analytic examples, see @cohen1993; @griffeth2000; @hom1992; @riketta2008; @steel1984; @steel1990]. Moreover, many variables display cyclic patterns of change over time, with mood [@larsen1990], daily stress [@bodenmann2010], and daily drinking behaviour [@huh2015] as some examples. Therefore, change over is unlikely to follow a linear pattern.

A more realistic pattern of change to occur over time is a nonlinear pattern [for a review, see @cudeck2007]. Nonlinear change allows nonconstant rates of change such that change may occur more rapidly during certain periods of time, stop altogether, or reverse direction. When looking at patterns of change observed across psychology, examples appear in the declining rate of speech errors throughout child development [@burchinal1991], forgetting rates in memory [@murre2015], development of habits over time [@fournier2017], and the formation of opinions over time [@xia2020]. Given nonlinear change appears more likely than linear change, my dissertation will assume change over time to be nonlinear. 

## Challenges Involved in Conducting Longitudinal Research

(ref:podsakoff2003ostroff2002) [@podsakoff2003; for an example, see @ostroff2002]

Conducting longitudinal research presents researchers with several challenges. Many challenges are those from cross-sectional research only amplified [for a review, see @bergman1993].\footnote{It should be noted that conducting a longitudinal study does alleviate some issues encountered in conducting cross-sectional research. For example, taking measurements over multiple time points likely reduces common method variance (ref:podsakoff2003ostroff2002).} For example, greater efforts have to be made to to prevent missing data which can increase over [@newman2008; @dillman2014]. Likewise, the adverse effects of well-documented biases such as demand characteristics [@orne1962] and social desirability [@nederhof1985] have to be countered at each time point. Outside challenges share with cross-sectional research, conducting longitudinal research also presents new challenges. Analyses of longitudinal data have to consider complications such as how to model error structures [@grimm2010a], check for measurement non-invariance over time [the extent to which a construct is measured with equivalent accuracy over time\; @vandeschoot2012], and how to center/process data to appropriately answer research questions [@enders2007; @wang2015]. 

Although researchers must contend with several issues in conducting longitudinal research, three issues are of particular interest in my dissertation. The first issue concerns how many measurements to use in a longitudinal design. The second issue concerns how to space the measurements. The third issue focuses on how much error is incurred if the time structuredness of the data is overlooked. The sections that follow will review each of these issues. 

### Number of Measurements

Researchers have to decide on the number of measurements to include in a longitudinal study. Although using more measurements increases the accuracy of results---as noted in the results of several studies [e.g., @coulombe2016; @timmons2015; @finch2017; @fine2019]---taking additional measurements often comes at a cost that a researcher may be unable account for with a limited budget. One important point to mention is that a researcher designing a longitudinal study must take at least three measurements to obtain a reliable estimate of change and, perhaps more importantly, to allow a nonlinear pattern of change to be modelled [@ployhart2010]. In my dissertation, I hope to determine whether an optimal number of measurements exists when modelling a nonlinear pattern of change. 

### Spacing of Measurements

Additionally, a researcher must decide on the spacing of measurements in a longitudinal study. Although discussions of measurement spacing often recommend that researchers use theory and previous studies to implement measurement spacings that  [@mitchell2001; @cole2003; @collins2006; @dormann2014, @dormann2015], organizational theories seldom delineate a period of time over which a process unfolds, and so the majority of longitudinal research uses intervals of convention and/or convenience to space measurements [@mitchell2001; @dormann2014]. Unfortunately, using measurement spacing lengths that do not account for the temporal pattern of change of a psychological process can lead to inaccurate results [e.g., @chen2014]. As an example, @cole2009 provide show how correlation magnitudes are affected by the choice of measurement spacing intervals. In my dissertation, I hope to determine whether an optimal measurement spacing schedule exists when modelling a nonlinear pattern of change. 

### Time Structuredness

Last, and perhaps most pernicious, analyses of longitudinal data are likely to incur error from an assumption they make about data collection conditions. Many analyses assume that, across all collection points, participants provide their data at the same time. Unfortunately, such a high level of regularity in the response patterns of participants is unlikely: Participants are more likely to provide their data over some period of time after a data collection window has opened. As an example, consider a study that collects data from participants at the beginning of each month. If participants respond with perfect regularity, then they would all provide their data at the exact same time (e.g., noon on the second day of each month). If the participants respond with imperfect regularity, then they would provide their at different times after the beginning of each month. The regularity of responding observed across participants in a longitudinal study determines the time structuredness of the data and the sections that follow will provide overview of time structuredness. 

#### Time-Structured Data

(ref:mehta2005mehta2000) [@mehta2005; @mehta2000]

Many analyses assume that data are *time structured*: Participants provide data at the same time at each collection point. By assuming time-structured data, an analysis can incur error because it will map time intervals of inappropriate lengths onto the time intervals that occurred between participant's responses.\footnote{It should be noted that, although seldom implemented, analyses can be accessorized to handle time-unstructured data by using definition variables (ref:mehta2005mehta2000).} As an example of the consequences of incorrectly assuming data to be time structured, consider a study that assessed the effects of an intervention on the development of leadership by collecting leadership ratings at four time points each separated by four weeks [@day2011]. The employed analysis assumed time-structured data; that is, each each participant provided ratings on the same day---more specifically, the exact same moment---each time these ratings were collected. Unfortunately, it is unlikely that the data collected from participants were time structured: At any given collection point, some participants may have provided leadership ratings at the beginning of the week, while others may only provide ratings two weeks after the survey opened. Importantly, ratings provided two weeks after the survey opened were likely influenced by changes in leadership that occurred over the two weeks. If an analysis incorrectly assumes time-structured data, then it assumes each participant has the same response rate and, therefore, will incorrectly attribute the amount of time that elapses between most participants' responses. For instance, if a participant only provides a leadership rating two weeks after having received a survey (and six weeks after providing their previous rating), then using an analysis that assumes time-structured data would incorrectly assume that each collection point of this participant is separated by four weeks (the interval used in the experiment) and would, consequently, model the observed change as if it had occurred over four weeks. Therefore, incorrectly assuming data to be time structured leads an analysis to overlook the unique response rates of participants across the collection points and, as a consequence, incur error [@mehta2000; @mehta2005; @coulombe2016]. 

#### Time-Unstructured Data

Conversely, some analyses assume that data are *time unstructured*: Participants provide data at different times at each collection point. Given the unlikelihood of one response pattern describing the response rates of all participants in a given study, the data
obtained in a study are unlikely to be time structured. Instead, and because participants are likely to exhibit unique response
patterns in their response rates, data are likely to be time unstructured. One way to conceptualize the distinction between time-structured and time-unstructured data is on a continuum. On one end of the continuum, participants all provide data with identical response patterns, thus giving time-structured data. When participants show unique response patterns, the resulting data are time unstructured, with the extent of time-unstructuredness depending on the length of the response windows. For example, if data are collected at the beginning of each month and participants only have one day to provide data at each time, then, assuming a unique response rate for each participant, the resulting data will have a low amount of time unstructuredness. Alternatively, if data are collected at the beginning of each month and participants have 30 days to provide data each time, then, assuming a unique response rate for each participant, the resulting data will have a high amount of time unstructuredness. Therefore, the continuum of time struturedness has time-structured data on one end and time-unstructured data with long response rates on another end. In my dissertation, I hope to determine how much error is incurred when time-unstructured data are assumed to be time structured. 

### Summary

In summary, researchers must contend with several issues when conducting longitudinal research. In addition to contending with issues encountered in conducting cross-sectional research, researchers must contend with new issues that arise from conducting longitudinal research. Three issues of particular importance in my dissertation are the number of measurements, the spacing of measurements, and incorrectly assuming data to be time structured. These issues will be serve as a basis for a systematic review of the simulation literature. 


## Using Simulations To Assess Modelling Accuracy

In the next section, I will present the results of the systematic review of the literature that has investigated the issues of measurement number, measurement spacing, and time structuredness. Before presenting the results of the systematic review, I will provide an overview of the Monte Carlo method used to investigate issues involved in conducting longitudinal research.  

To understand how the effects of longitudinal issues on modelling accuracy can be investigated, the inferential method commonly employed in psychological research will first be reviewed with an emphasis on its shortcomings (see Figure \ref{fig:MonteCarlo-comparison}). Consider an example where a researcher wants to estimate a population mean ($\upmu$) and understand how sampling error affects the accuracy of the estimate. Using the inferential method, the researcher samples data and then estimates the population mean ($\upmu$) by computing the mean of the sampled data. Because collected samples are almost always contaminated by a variety of methodological and/or statistical deficiencies (such as sampling error, measurement error, assumption violations, etc.), the estimation of the population parameter is likely to be imperfect. Unfortunately, to estimate the effect of sampling error on the accuracy of the population mean estimate ($\upmu$), the researcher would need to know the value of the population mean; without knowing the value of the population mean, it is impossible to know how much error was incurred in estimating the population mean and, as as a result, impossible to know the extent to which sampling error contributed to this error. Therefore, a study following the inferential approach can only provide estimates of population parameters.

The Monte Carlo method has a different goal. Whereas inferential methods focus on estimating parameters from sample data, the Monte Carlo method is used to understand the factors that influence the accuracy of the inferential approach. Figure \ref{fig:MonteCarlo-comparison} shows that the Monte Carlo method works in the opposite direction of the inferential approach: Instead of collecting a sample, the Monte Carlo method begins by assigning a value to at least one parameter to define a population. Many sample data sets are then generated from the defined population, with some methodological deficiency built in to each data set. In the current example, each data set is generated to have a specific amount of missing data. Each generated sample is then analyzed and the population estimates of each statistical model are averaged and compared to the pre-determined parameter value.\footnote{A statistical deficiency can also be introduced in the analysis of each generated data set.} The difference between the average of the estimates and the known population value constitutes bias in parameter estimation (i.e., parameter bias). In the current example, the missing data manipulation causes a systematic underestimation, on average, of the population parameter. By randomly generating data, the Monte Carlo method can determine how a variety of methodological and statistical factors affect the accuracy of a model [for a review, see @robert2010].

```{=tex}
\begin{figure}[H]
  \caption{Depiction of Monte Carlo Method}
  \label{fig:MonteCarlo-comparison}
  \includegraphics{Figures/Monte_Carlo_comparison} \hfill{}
    \caption*{Note. \textup{Comparison of inferential approach with the Monte Carlo approach. The inferential approach begins with a collected sample and then estimates the population parameter using an appropriate statistical model. The difference between the estimated and population value can be conceptualized as error. Because the population value is generally unknown in the inferential approach, it cannot estimate how much error is introduced by any given methodological or statistical deficiency. To estimate how much error is introduced by any given methodological or statistical deficiency, the Monte Carlo method needs to be used, which constitutes four steps. The Monte Carlo method first defines a population by setting parameter values. Second, many samples are generated from the pre-defined population, with some methodological deficiency built in to each data set (in this case, each sample has a specific amount of missing data). Third, each generated sample is then analyzed and the population estimates of each statistical model are averaged and compared to the pre-determined parameter value. Fourth, the difference between the estimate average and the known population value defines the extent to which the missing data manipulation affected parameter estimation (the difference between the population and average estimated population value is the parameter bias).}}
\end{figure}
```

Monte Carlo simulations have been used to evaluate a variety of methodological and statistical deficiencies. Beginning with the simple bivariate correlation, Monte Carlo simulations have shown that realistic values of sample size and measurement accuracy produce considerable variability in estimated correlation values [@stanley2014]. Monte Carlo simulations have also provided valuable insights into more complicated statistical analyses. In investigating more complex statistical analyses, simulations have shown that mediation analyses are biased to produce results of complete mediation because the statistical power to detect direct effects falls well below the statistical power to detect indirect effects [@kenny2014]. Finally, as an example of the utility of Monte Carlo simulations for evaluating growth mixture models, Monte Carlo simulations have shown that class enumeration accuracy (the ability to identify the correct number of response groups) decreases with nonnormal data [@bauer2003]. Given the ability of the Monte Carlo method to evaluate statistical methods, the   experiments in my dissertation used it to evaluate the effects of measurement number, measurement spacing, and time structuredness on modelling accuracy.\footnote{My simulation experiments also investigated the effects of sample size and nature of change on modelling accuracy.} 


## Systematic Review of Simulation Literature

To understand the extent to which issues involved in conducting longitudinal research had been investigated, I conducted a systematic review of the simulation literature. The sections that follow will first present the method I followed in systematically reviewing the literature and then summarize the findings of the review. 

### Systematic Review Methodology 

I identified the following keywords through citation searching and independent reading: "growth curve", "time-structured analysis", "time structure", "temporal design", "individual measurement occasions", "measurement intervals", "methods of timing", "longitudinal data analysis", "individually-varying time points", "measurement timing", "latent difference score models", "parameter bias", and "measurement spacing". I entered these keywords entered into the PsycINFO database (on July 23, 2021) and any paper that contained any one of these key words and the word "simulation" in any field was considered a viable paper (see Figure \@ref(fig:prismaDiagram) for a PRISMA diagram illustrating the filtering of the reports). The search returned 165 reports, which I screened by reading the abstracts. Initial screening led to the removal of 60 reports because they did not contain any simulation experiments. Of the remaining 105 papers, I removed 2 more popers  because they could not accessed [@stockdale2007; @tiberio2008]. Of the remaining 103 identified simulation studies, I deemed a paper as relevant if it investigated the effects of any design and/or analysis factor relating to conducting longitudinal research (i.e., number of measurements, spacing of measurements, and/or time structuredness) and did so using the Monte Carlo simulation method. Of the remaining 103 studies, I removed 89 studies being removed because they did not meet the inclusion criteria, leaving fourteen studies to be included the review, with. I also found an additional 3 studies through citation searching, giving a total of 17 studies. 

(ref:errorStructures) [@gasimova2014; @liu2021; @liu2015; @miller2017; @murphy2011]
(ref:fine2019) [@fine2019]
(ref:fine2019fine2020) [@fine2019; @fine2020]
(ref:fine2019text) @fine2019
(ref:fine2020text) @fine2020
(ref:coulombe2016miller2017) [and from previous simulation experiments of @coulombe2016; @miller2017]

The findings of my systematic review are summarized in Tables \@ref(tab:systematicReviewCount)--\@ref(tab:systematicReview). Tables \@ref(tab:systematicReviewCount)--\@ref(tab:systematicReview) differ in one way: Table \@ref(tab:systematicReviewCount) indicates how many studies investigated each effect, whereas Table \@ref(tab:systematicReview) provides the reference of each study and detailed information about each study's method. Otherwise, all other details of Tables \@ref(tab:systematicReviewCount)--\@ref(tab:systematicReview) are identical. The first column lists the longitudinal design factor (alongside with sample size) and the corresponding two- and three-way interactions. The second and third columns list whether each effect has been investigated with linear and nonlinear patterns of change, respectively. Shaded cells indicate effects that have not been investigated, with cells shaded in light blue indicating effects that have not been investigated with linear patterns of change and cells shaded in dark blue indicating effects that have not been investigated with nonlinear patterns of change.\footnote{Table \ref{tab:systematicReview} lists the effects that each study (identified by my systematic review) investigated and notes the following methodological details (using superscript letters and symbols): the type
of model used in each paper, assumption and/or manipulation of complex error structures
(heterogeneous variances and/or correlated residuals), manipulation of missing data,
and/or pseudo-time structuredness manipulation. Across all 17 simulation studies, 5 studies (29\%) assumed complex error structures (ref:errorStructures), 1 study (6\%) manipulated missing data (ref:fine2019), and 2 studies (12\%) contained a pseudo-time structuredness manipulation (ref:fine2019fine2020) . Importantly, the pseudo-time structuredness manipulation used in (ref:fine2019text) and (ref:fine2020text) differed from the manipulation of time
structuredness used in the current experiments (ref:coulombe2016miller2017) in that it randomly generated longitudinal data such that a given person could provide all their data before another person provided any data.}


```{=tex}
\begin{figure}[H]
  \caption{PRISMA Diagram Showing Study Filtering Strategy}
  \label{fig:prismaDiagram}
  \includegraphics{Figures/prisma_diagram} \hfill{}
    \caption*{Note. \textup{PRISMA diagram for systematic review of simulation research that investigates measurement timing}}
\end{figure}
```

### Systematic Review Results 

Although the previous research appeared to sufficiently fill some cells of Table \@ref(tab:systematicReviewCount), two patterns suggest that arguably the most important cells (or effects) have not been investigated. First, it appears that simulation research has invested more effort in investigating the effects of longitudinal design factors with linear patterns than with nonlinear patterns of change. In counting the number of effects that remain unaddressed with linear and nonlinear patterns of change, a total of five cells (or effects) have not been


```{r systematicReviewCount, echo=F}
#table_1 <-  data.frame('Effect' = c('\\textbf{Main effects}', 'Number of measurements (NM)', 'Spacing of measurements (SM)', 'Time structuredness #(TS)', 'Sample size (S)', 
#                                    '\\textbf{Two-way interactions}', 'NM x SM', 'NM x TS', 'NM x S', 'SM x TS', 'SM x S', 'TS x S',
#                                    '\\textbf{Three-way interactions}', 'NM x SM x TS', 'NM x SM x S', 'NM x TS x S', 'SM x TS x S'), 
#               'Linear pattern' = c('', '11 studies', '1 study', '2 studies','11 studies', '', '1 study',  '1 study', '9 studies', '\\textbf{Cell #2}', 
#                                    '\\textbf{Cell 4}', '1 study', '', '\\textbf{Cell 6}', '\\textbf{Cell 8}', 
#                                    '1 study', ' \\textbf{Cell 11}'),
#               'Nonlinear pattern' = c('', '7 studies', '1 study', '2 studies', '7 studies', '', '1 study', ' \\textbf{Cell 1}', '5 studies', 
#                                       ' \\textbf{Cell 3}', ' \\textbf{Cell 5}' , '2 studies', '', ' \\textbf{Cell 7}',
#                                       ' \\textbf{Cell 9}', ' \\textbf{Cell 10}', '\\textbf{Cell 12}'), check.names = F)

table_1 <-  data.frame('Effect' = c('\\textbf{Main effects}', 'Number of measurements (NM)', 'Spacing of measurements (SM)', 'Time structuredness (TS)', 'Sample size (S)', 
                                    '\\textbf{Two-way interactions}', 'NM x SM', 'NM x TS', 'NM x S', 'SM x TS', 'SM x S', 'TS x S',
                                    '\\textbf{Three-way interactions}', 'NM x SM x TS', 'NM x SM x S', 'NM x TS x S', 'SM x TS x S'), 
               'Linear pattern' = c('', '11 studies', '1 study', '2 studies','11 studies', '', '1 study',  '1 study', '9 studies', '\\textbf{Cell 2}', 
                                    '\\textbf{Cell 4}', '1 study', '', '\\textbf{Cell 6}', '\\textbf{Cell 8}', 
                                    '1 study', ' \\textbf{Cell 11}'),
               'Nonlinear pattern' = c('', '6 studies', '1 study', '1 study', '7 studies', '', '1 study', ' \\textbf{Cell 1 (\\hyperref[Exp3]{Exp. 3})}', '5 studies', 
                                       ' \\textbf{Cell 3}', ' \\textbf{Cell 5 (\\hyperref[Exp2]{Exp. 2})}' , '2 studies', '', ' \\textbf{Cell 7}',
                                       ' \\textbf{Cell 9 (\\hyperref[Exp2]{Exp. 2})}', ' \\textbf{Cell 10 (\\hyperref[Exp3]{Exp. 3})}', '\\textbf{Cell 12}'), check.names = F)

kbl(x = table_1, booktabs = TRUE, format = 'latex', longtable = TRUE, 
    linesep = c('\\cmidrule{1-3}',
        rep(' ', times = 3), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}', 
        rep(' ', times = 5), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}',
        rep(' ', times = 3)), 
    align = c('l', 'c', 'c'), 
    caption = 'Number of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\\textit{n} = 17)', 
    escape=F) %>%
   column_spec(2, background = c(rep('white', times = 9), 
                                 rep('#acddfa', times = 1), 
                                 '#acddfa',
                                 'white', 'white',
                                 rep('#acddfa', times = 1), 
                                 rep('#acddfa', times = 1),
                                 'white',
                                 '#acddfa'), 
              width = '9cm') %>%
  column_spec(3, background = c(rep('white', times = 7), 
                                '#9fc5e8',
                                'white', 
                                '#9fc5e8',
                                '#9fc5e8',
                                'white', 'white', 
                                 rep('#9fc5e8', times = 1), 
                                 rep('#9fc5e8', times = 2),
                                 '#9fc5e8'),  
              width = '9cm') %>% 
   kable_styling(latex_options= c('hold_position', 'repeat_header'), font_size = 10, position = 'left') %>%
  footnote(general = 'Cells are only numbered for effects that have not been investigated (Cell 5 is the only exception). Cells shaded in light blue indicate effects that have not been investigated for linear patterns of change and cells shaded in light green indicate effects that have not been investigated for nonlinear patterns of change.', general_title = '\\\\textit{Note.\\\\hspace{0pc}}', footnote_as_chunk = T, escape = F, threeparttable = T) %>%
  landscape(margin = '1cm')
```

(ref:Coulombe2016) @coulombe2016

(ref:Timmons2015) @timmons2015

(ref:ORourke2021) @orourke2021

(ref:Miller2017) @miller2017

(ref:Liu2020) @liu2019

(ref:Liu2021) @liu2021

(ref:Fine2019) @fine2019

(ref:Wu2017) @wu2017

(ref:Finch2017) @finch2017

(ref:Coulombe2016b) @coulombe2016b

(ref:Newsom2020) @newsom2020

(ref:Fine2020) @fine2020

(ref:Wu2014) @wu2014

(ref:Ye2016) @ye2016

(ref:Gasimova2014) @gasimova2014

(ref:Murphy2011) @murphy2011

(ref:Aydin2014) @aydin2014

(ref:Liu2015) @liu2015

```{r systematicReview, echo=F}
table_1 <-  data.frame(
  'Effect' = c('\\textbf{Main effects}', 'Number of measurements (NM)', 'Spacing of measurements (SM)', 'Time structuredness (TS)', 'Sample size (S)', 
               '\\textbf{Two-way interactions}', 'NM x SM', 'NM x TS', 'NM x S', 'SM x TS', 'SM x S', 'TS x S',
               '\\textbf{Three-way interactions}', 'NM x SM x TS', 'NM x SM x S', 'NM x TS x S', 'SM x TS x S'), 
                       
'Linear pattern' = c('', 
                     '(ref:Timmons2015)$^{a}$; (ref:Murphy2011)$^{{\\mho}b}$; (ref:Gasimova2014)$^{{\\mho}c}$; (ref:Wu2014)$^{a}$; (ref:Coulombe2016b)$^{a}$;(ref:Ye2016)$^{a}$; (ref:Finch2017)$^{a}$; (ref:ORourke2021)$^{d}$; (ref:Newsom2020)$^{a}$; (ref:Coulombe2016)$^{a}$', 
                     '(ref:Timmons2015)$^{a}$', 
                     '(ref:Aydin2014)$^{a}$; (ref:Coulombe2016)$^{a}$',
                     '(ref:Murphy2011)$^{\\mho}{b}$; (ref:Gasimova2014)$^{{\\mho}c}$; (ref:Wu2014)$^{a}$; (ref:Coulombe2016b)$^{a}$;(ref:Ye2016)$^{a}$; (ref:Finch2017)$^{a}$; (ref:ORourke2021)$^{d}$; (ref:Newsom2020)$^{a}$; (ref:Coulombe2016)$^{a}$;(ref:Aydin2014)$^{a}$; (ref:Coulombe2016)$^{a}$', 
                     '', 
                     '(ref:Timmons2015)$^{a}$',  
                     '(ref:Coulombe2016)$^{a}$', 
                     '(ref:Murphy2011)$^{{\\mho}b}$; (ref:Gasimova2014)$^{{\\mho}c}$; (ref:Wu2014)$^{a}$; (ref:Coulombe2016b)$^{a}$;(ref:Ye2016)$^{a}$; (ref:Finch2017)$^{a}$; (ref:ORourke2021)$^{d}$; (ref:Newsom2020)$^{a}$; (ref:Coulombe2016)$^{a}$', 
                     '\\textbf{Cell 2}', 
                     '\\textbf{Cell 4}', 
                     '(ref:Aydin2014)$^{a}$', 
                     '', 
                     '\\textbf{Cell 6}', 
                     '\\textbf{Cell 8}', 
                     '(ref:Coulombe2016)$^{a}$', 
                     '\\textbf{Cell 11}'),

'Nonlinear pattern' = c('', 
                        '(ref:Timmons2015)$^{a}$; (ref:Finch2017)$^{a}$; (ref:Fine2019)$^{e{\\circ}{\\triangledown}}$; (ref:Fine2020)$^{e,f{\\triangledown}}$;(ref:Liu2020)$^{g}$; (ref:Liu2021)$^{{\\mho}h}$; (ref:Liu2015)$^{{\\mho}g}$', 
                        '(ref:Timmons2015)$^{a}$', 
                        '(ref:Miller2017)$^{{\\mho}a}$; (ref:Liu2015)$^{{\\mho}g}$', 
                        '(ref:Finch2017)$^{a}$; (ref:Fine2019)$^e{{\\circ}{\\triangledown}}$; (ref:Fine2020)$^{e,f{\\triangledown}}$;(ref:Liu2020)$^{g}$; (ref:Liu2021)$^{{\\mho}h}$; (ref:Liu2015)$^{{\\mho}g}$; (ref:Miller2017)$^{{\\mho}a}$', 
                        '', 
                        '(ref:Timmons2015)$^{a}$', 
                        '\\textbf{Cell 1}', 
                        '(ref:Finch2017)$^{a}$; (ref:Fine2019)$^{e{\\circ}{\\triangledown}}$; (ref:Fine2020)$^{e,f{\\triangledown}}$;(ref:Liu2020)$^{g}$; (ref:Liu2021)$^{{\\mho}h}$',
                        '\\textbf{Cell 3}', 
                        '\\textbf{Cell 5}' , 
                        '(ref:Liu2015)$^{{\\mho}g}$; (ref:Miller2017)$^{{\\mho}a}$', 
                        '', 
                        '\\textbf{\\centering{\\arraybackslash{Cell 7}}}', 
                        '\\textbf{Cell 9}', 
                        '\\textbf{Cell 10}', 
                        '\\textbf{Cell 12}'), check.names = F)

kbl(x = table_1, booktabs = TRUE, format = 'latex', longtable = TRUE, 
  linesep = c('\\cmidrule{1-3}',
        rep(' ', times = 3), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}', 
        rep(' ', times = 5), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}',
        rep(' ', times = 3)), 
  align = c('l', 'c', 'c'), 
  caption = 'Summary of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\\textit{n} = 17)', 
  escape=F) %>%

  column_spec(2, background = c(rep('white', times = 9), 
                                 rep('#acddfa', times = 1), 
                                 '#acddfa',
                                 'white', 'white',
                                 rep('#acddfa', times = 1), 
                                 rep('#acddfa', times = 1),
                                 'white',
                                 '#acddfa'), 
              width = '9cm') %>%
  column_spec(3, background = c(rep('white', times = 7), 
                                '#9fc5e8',
                                'white', 
                                '#9fc5e8',
                                '#9fc5e8',
                                'white', 'white', 
                                 rep('#9fc5e8', times = 1), 
                                 rep('#9fc5e8', times = 2),
                                 '#9fc5e8'),  
              width = '9cm',  bold = ifelse(grepl(pattern = '^\\d+', x = table_1$Nonlinear),T, F)) %>% 
  
  kable_styling(latex_options= c('hold_position', 'repeat_header'), font_size = 10, position = 'left') %>%
  footnote(general = 'Cells are only numbered for effects that have not been investigated (Cell 5 is the only exception). Cells shaded in light blue indicate effects that have not been investigated for linear patterns of change and cells shaded in dark blue indicate effects that have not been investigated for nonlinear patterns of change.', 
           general_title = '\\\\textit{Note.\\\\hspace{-1.2pc}}', footnote_as_chunk = T, symbol_title = '\\\\newline', 
           alphabet_title = '\\\\newline', escape = F, threeparttable = T, 
           alphabet = c('Used latent growth curve model.', 
                        'Used second-order latent growth curve model.',
                        'Used hierarchical Bayesian model.',
                        'Used bivariate latent change score model', 
                        'Used functional mixed-effects model.',
                        'Used nonlinear mixed-effects model.',
                        'Used bilinear spline model.', 
                        'Used parallel bilinear spline model.'), 
           symbol_manual = c('$\\\\circ$', '$\\\\mho$', '$\\\\triangledown$'), 
           symbol = c('Manipulated missing data.', 'Assumed complex error structure (heterogeneous variances and/or correlated residuals).', 
                      'Contained pseudo-time structuredness manipulation.')) %>%
  landscape(margin = '1cm')
```

\noindent investigated with linear patterns of change, but a total of seven cells have not been investigated with nonlinear patterns of change. Given that change over time is more likely to follow a nonlinear than a linear pattern [for a review, see @cudeck2007], it could be argued that most simulation research has investigated the effect of longitudinal design factors under unrealistic linear conditions. Second, all the cells corresponding to the three-way interactions with nonlinear patterns of change had not been investigated (cells 7, 9, 10, and 12 of Table \ref{tab:systematicReviewCount}), meaning that almost no study had conducted a comprehensive investigation into longitudinal issues. Therefore, no simulation study has comprehensively investigated longitudinal issues under on modelling nonlinear patterns of change. 

### Next Steps

Given that longitudinal research is needed to understand the temporal dynamics of psychological processes, it is necessary to understand how longitudinal design and analysis factors interact with each other (and with sample size) in affecting the accuracy with which nonlinear patterns of change are modelled. With no study to my knowledge having conducted a comprehensive investigation of how longitudinal design and analysis factors affect the modelling of nonlinear change patterns, my simulation experiments are designed to address this gap in the literature. Specifically, my simulation experiments investigate how measurement number, measurement spacing, and time structuredness affect the accuracy with which a nonlinear change pattern is modelled (see Cells 1, 5, 9, and 10 of Table \ref{tab:systematicReviewCount}). 

## Methods of Modelling Nonlinear Patterns of Change Over Time

(ref:orourke2021) [e.g., @orourke2021]
(ref:fine2020) [e.g., @fine2020]

Because my simulation experiments assumed change over time to be nonlinear, it is important to provide an overview of how nonlinear change is modelled. In this section, I will provide a brief review on how nonlinear change can be modelled and will contrast the commonly employed polynomial approach with the lesser known nonlinear function approach that I use in my simulations.\footnote{It should be noted that nonlinear change can be modelled in a variety of ways, with latent change score models (ref:orourke2021) and spline models (ref:fine2020) offering some examples.}\footnote{The definition of a nonlinear function is mathematical in nature. Specifically, a nonlinear function contains at least one parameter that exists in the corresponding partial derivative. For example, in the logistic function $\uptheta + \frac{\upalpha - \uptheta}{1 + exp^(\frac{\upbeta - t}{\upgamma}}$ is nonlinear because $\upbeta$ exists in $\frac{\partial y}{\partial \upbeta}$ (in addition to $\upgamma$ existing in its corresponding partial derivative). The $n^{th}$ order polynomial function of $y = a + bx + cx^2 + ... + nx^n$ is linear because  the partial derivatives with respect to the parameters (i.e., $1, x^2, ..., x^n$) do not contain the associated parameter.}

```{r nonlinear-plot-code, echo=F, include=F}
#regress outcome_value on time using the nonlinear function

nonlin_data <- read_csv(file = 'data/nonlin_data.csv')

nonlin_output <- round(data.frame(summary(nls(
  formula = obs_score ~ SSfpl(input = measurement_day, A = theta, B = alpha, xmid = beta, scal = gamma), 
  data = nonlin_data, 
  control = nls.control(maxiter = 100)))$coefficients), digits = 3)

nonlin_output$parameter <- rownames(nonlin_output)

#extract coefficient values
theta <- as.numeric(nonlin_output$Estimate[nonlin_output$parameter =='theta'])
alpha <- round(nonlin_output$Estimate[nonlin_output$parameter == 'alpha'], 2)
beta <- round(nonlin_output$Estimate[nonlin_output$parameter == 'beta'])
gamma <- round(nonlin_output$Estimate[nonlin_output$parameter == 'gamma'])

#AIC_nonlin <- formatC(round(AIC(nls(formula = outcome_value ~
#SSdlf(time = time, asym = alpha, a2 = theta, xmid = beta, scal = gamma),
#data = pos_responders)), digits = 2), format = 'f', digits = 2)
```

```{r polynomial-vs-nonlinear-plot, echo=F, include=F}
#create function to roun to two decimal places
round_two_decimals <- function(number) {
  
  rounded_number <- as.numeric(formatC(round(number, digits = 2), format = 'f', digits = 3))
  return(rounded_number)
}

nonlin_data <- read_csv(file = 'data/nonlin_data.csv')

#regress outcome_value on time using the linear function
polynomial_output <- data.frame(summary(nls(
  formula = obs_score ~ a + b*measurement_day + c*measurement_day^2 + d*measurement_day^3, 
  data = nonlin_data,
  start = list(a = 1, b = 1, c = 1, d = 0.5)))$coefficients)

polynomial_output$parameter <- rownames(polynomial_output)

#extract coefficient values & AIC value
a <- 3.09 #round(polynomial_output$Estimate[polynomial_output$parameter == 'a'], 2)
b <- -0.0018 #round(polynomial_output$Estimate[polynomial_output$parameter == 'b'], 4)
c <- 2.02e-05 #round(polynomial_output$Estimate[polynomial_output$parameter == 'c'], 5)
d <- -3.54e-08 #round(polynomial_output$Estimate[polynomial_output$parameter == 'd'], 8)

#AIC_polynomial <- formatC(round(AIC(nls(
#  formula = obs_score ~ a + b*measurement_day + c*measurement_day^2 + d*measurement_day^3, 
#  data = nonlin_data,
#  start = list(a = 1, b = 1, c = 1, d = 0.5))), 2), format = 'f', digits = 3)

measurement_day <- seq(from = 1, to = 360, by = 1)

poly_nonlin_pred_scores <- data.frame('measurement_day' = measurement_day, 
                                      'pred_score' = a + b*measurement_day + c*measurement_day^2 + d*measurement_day^3)


font_size <- 15
title_font <- 45
axis_text_size <- 30
axis_title_size <- 40

poly_pred_plot <- ggplot(poly_nonlin_pred_scores, aes(x = measurement_day, y = pred_score)) +
  geom_line(size = 2) +
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(limits = c(2.9, 3.5), breaks = seq(from = 3, to = 3.5, by = 0.25)) +
  scale_x_continuous(breaks = seq(from = 0, to = 360, by = 60), limits = c(0, 360)) +
  labs(x = 'Day', y = 'Predicted value', size = 16) +
  ggtitle(label = 'A: Response pattern predicted \n by polynomial (linear) function') +
  annotate(geom = 'text', x = 100, y = 3.45, label = 'y == italic(a) + italic(b)*x + italic(c)*x^2 + italic(d)*x^3',
           parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 45, y = 3.35, label = paste('italic(a) == ', a), parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 55, y = 3.30, label = paste('italic(b) == ', b), parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 63, y = 3.25, label = paste('italic(c) == ', c), parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 70, y = 3.20, label = paste('italic(d) == ', d), parse = T, family = 'Helvetica', size = font_size) +
  #annotate(geom = 'text', x = 10, y = 1.70, label = paste('AIC == ', AIC_lin), parse = T) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'), 
        axis.line = element_line(size = 2))


nonlin_pred <- data.frame('measurement_day' =  measurement_day, 
                          'pred_score' = theta + (alpha - theta)/(1 + exp((beta - measurement_day)/gamma)))

nonlin_function_plot <- ggplot(nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) +
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(limits = c(2.90, 3.5), breaks = seq(from = 3, to = 3.5, by = .25)) +
  scale_x_continuous(breaks = seq(0, 360, by = 60), limits = c(0, 360)) +
  labs(x = 'Day', y = 'Predicted value', size = 16, tag = 'B') +
  ggtitle(label = 'B: Response pattern predicted \n by logistic (nonlinear) function') +
  annotate(geom = 'text', x = 80, y = 3.45, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) +
  #beta
  annotate(geom = 'text', x = 40, y = 3.35, label = paste('theta == 3.00'), parse = T, size = font_size) +
  annotate(geom = 'text', x = 55, y = 3.30, label = paste('alpha == ', alpha), parse = T, size = font_size) + 
  annotate(geom = 'text', x = 55, y = 3.25, label = paste('beta == ', beta), parse = T, size = font_size) + 
  annotate(geom = 'text', x = 50, y = 3.20, label = paste('gamma == ', gamma), parse = T, size = font_size) + 
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'), 
        axis.line = element_line(size = 2))

interpretation_plot <- ggarrange(poly_pred_plot, nonlin_function_plot, ncol = 2)
ggsave(plot = interpretation_plot, filename = 'Figures/polynomial_vs_nonlinear_plot.pdf', width = 24, height = 12)
```

Consider an example where an organization introduces a new incentive system with the goal of increasing the motivation of its employees. To assess the effectiveness of the incentive system, employees provide motivation ratings every month days over a period of 360 days. Over the 360-day period, the motivation levels of the employees increase following an s-shaped pattern of change over time. One analyst decides to model the observed change using a **polynomial function** shown below in Equation \ref{eq:polynomial}: 

```{=tex}
\begin{align}
  y = \mathit{a} + \mathit{b}x + \mathit{c}x^2 + \mathit{d}x^3.
  (\#eq:polynomial)
\end{align}
```

\noindent A second analyst decides to model the observed change using a **logistic function** shown below in Equation \ref{eq:logistic1}:

```{=tex}
\begin{align}
  y = \uptheta + \frac{\upalpha - \uptheta}{1 + e^{\frac{\upbeta -time}{\upgamma}}}
  (\#eq:logistic1)
\end{align}
```

\noindent  Figure \ref{fig:polynomial-vs-logistic}A shows the response pattern predicted by the polynomial function of Equation \ref{eq:polynomial} with the estimated values of each parameter ($a$, $b$, $c$, and $d$) and Figure \ref{fig:polynomial-vs-logistic}B shows the response pattern predicted by the logistic function (Equation \ref{eq:logistic1}) along with the values estimated for each parameter ($\uptheta$, $\upalpha$, $\upbeta$, and $\upgamma$). Although the logistic and polynomial functions predict nearly identical response patterns, the parameters of the logistic function have the following meaningful interpretations (see Figure \ref{fig:combined_plot_1}):

-   $\uptheta$ specifies the value at the first plateau (i.e., the starting value) and so is called the **baseline** parameter (see Figure \ref{fig:combined_plot_1}A).
-   $\upalpha$ specifies the value at the second plateau (i.e., the ending value) and so is called the the **maximal elevation** parameter (see Figure \ref{fig:combined_plot_1}B).
-   $\upbeta$ specifies the number of days required to reach the half the difference between the first and second plateau (i.e., the midway point) and so is called the **days-to-halfway-elevation** parameter (see Figure \ref{fig:combined_plot_1}C). 
-   $\upgamma$ specifies the number of days needed to move from the midway point to approximately 73% of the difference between the starting and ending values (i.e., satiation point) nd so is called the **halfway-triquarter delta** parameter (see Figure \ref{fig:combined_plot_1}D).

\noindent Applying the parameter meanings of the logistic function to the parameter values estimated by using the logistic function (Equation \ref{eq:logistic1}), the predicted response pattern begins at a value of `r theta` (baseline) and reaches a value of `r alpha` (maximal elevation) by the end of the 360-day period. The midway point of the curve is reached after `r beta` days (days-to-halfway elevation) and the satiation point is reached `r gamma`days later (halfway-triquarter delta; or `r beta + gamma` days after the beginning of the incentive system is introduced). When looking at the polynomial function, aside from the '$a$' parameter indicating the starting value, it is impossible to meaningfully interpret the values of any of the other parameter values. Therefore, using a nonlinear function such as the logistic function provides a meaningful way to interpret nonlinear change.



```{=tex}
\begin{figure}[H]
  \caption{Response Patterns Predicted by Polynomial (Equation \ref{eq:polynomial}) and Logistic (Equation \ref{eq:logistic1}) Functions}
  \label{fig:polynomial-vs-logistic}
  \includegraphics{Figures/polynomial_vs_nonlinear_plot} \hfill{}
  \caption*{Note. \textup{The top panel response pattern predicted by the polynomial function of Equation @ref(eq:polynomial) and the bottom panel shows the response pattern predicted by the logistic function of Equation @ref(eq:logistic1)}}
\end{figure}
```

## Overview of Simulation Experiments 

To investigate the effects of longitudinal design and analysis factors on modelling accuracy, I conducted three Monte Carlo experiments. Before summarizing the simulation experiments, one point needs to be mentioned regarding the maximum number of independent variables used in each experiment. No simulation experiment manipulated more than three variables because of the difficulty associated with interpreting interactions between four or more variables. Even among academics, the ability to correctly interpret interactions sharply declines when the number of independent variables increases from three to four [@halford2005]. Therefore, none of my simulation experiments manipulated more than three variables so that results could be readily interpreted. 

To summarize the three simulation experiments, the independent variables of each simulation experiment are listed below: 

* Experiment 1: number of measurements, spacing of measurements, and nature of change. 
* Experiment 2: number of measurements, spacing of measurements, and sample size. 
* Experiment 3: number of measurements, sample size, and time structuredness. 

\noindent The sections that follow will present each of the simulation experiments and their corresponding results. 

```{r logistic-interpretation-plot, eval=F, include=F}
#setup variables for logistic curve 
time <- seq(from = 1, to = 360, by = 1)

#df for points 
point_df <- data.frame('day' = c(1, beta, beta+gamma, 360), 
                       'curve_score' = c(theta, beta, gamma, alpha), 
                       'beta_brace' = factor(c('beta', 'beta', 'NA', 'NA')),
                       'beta_label' = rep('d[beta]', times = 4), 
                       
                       'gamma_brace' = factor(c('NA', 'gamma', 'gamma', 'NA')), 
                       'gamma_label' = rep('d[gamma]', times = 4),
                       
                       'total_brace' = factor(c('total', 'NA', 'NA', 'total')), 
                       'total_label' = rep('d[total]', times = 4))

font_size <- 8
title_font <- 30
axis_text_size <- 20
axis_title_size <- 24

theta_plot_1 <- ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(A:~Baseline~(theta)))) + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 440, y = 3.05, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) +

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


alpha_plot_1 <-ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(B:~Maximal~elevation~(alpha)))) + 
  #theta 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text',  x = 440, y = 3.27, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.20, label = 'alpha == 3.32', parse = T, size = font_size) + 

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


beta_plot_1 <- ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label =  expression(bold(C:~Days~to~halfway~elevation~(beta)))) + 
  #theta 
    geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = beta, xend = beta, y = 3.16, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 55, y = 3.14, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 55, y = 3.08, label = 'beta == 199~days', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

gamma_plot_1 <- ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(D:~`Halfway-triquarter`~'delta'~(gamma)))) +
  
  #gamma 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = beta + gamma, xend = beta + gamma, y = 3.233, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 100, y = 3.21, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.15, label = 'gamma == 21~days', parse = T, size = font_size) + 
  
  #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

combined_plot_1 <- ggarrange(theta_plot_1, alpha_plot_1, beta_plot_1, gamma_plot_1)
ggsave(plot = combined_plot_1, filename = 'Figures/combined_plot_1.pdf', width = 18, height = 12)


complete_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 1) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360))+
  annotate(geom = 'text', x = 50, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = 5) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 3) +

  #beta
  annotate(geom = 'text', x = 85, y = 3.15, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 90, y = 3.12, label = 'beta == 180~days', parse = T, size = font_size) +
  geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 180, xend = 180, y = 3.16, yend = 2.98, linetype = 2, size = 0.3) + #vertical dashed line 
  
  #gamma
  annotate(geom = 'text', x = 97, y = 3.233, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.21, label = 'gamma == 40~days', parse = T, size = font_size) + 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 220, xend = 220, y = 3.233, yend = 2.98, linetype = 2, size = 0.3)+  #vertical dashed line  
  
  coord_cartesian(clip = 'off') + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 3, size = 0.3) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 425, y = 3.03, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 425, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) + 

  #alpha 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 3, size = 0.3) + #vertical dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) +      #horizontal arrow
  annotate(geom = 'text', x = 430, y = 3.30, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 430, y = 3.26, label = 'alpha == 3.32', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold',size = title_font), 
        axis.title = element_text(size = 16), 
        axis.text = element_text(size = 13, colour = 'black'))
  
    ##brace information
  #stat_brace(data = point_df %>% filter(beta_brace == 'beta'), 
  #           mapping = aes(group = beta_brace, label = beta_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #stat_brace(data = point_df %>% filter(gamma_brace == 'gamma'), 
  #           mapping = aes(group = gamma_brace, label = gamma_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) +
  #
  #stat_brace(data = point_df %>% filter(total_brace == 'total'), 
  #           mapping = aes(group = total_brace, label = total_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #description box 
  #annotate(geom = 'rect', xmin = 235, xmax = 355, ymin = 3.02, ymax = 3.15, alpha = 0.1, color = 'black') + 
  #annotate(geom = 'text', x = 295, y = 3.13, label = 'd[total] == alpha~-~theta == 0.32', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.09, label = 'd[beta] == 0.5~(d[total]) == 0.16', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.06, label = 'd[gamma] == 0.23~(d[total]) == 0.07', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.03, label = 'd[beta]~+~d[gamma] == 0.73~(d[total]) == 0.23', parse = T, size = 4.5) + 

ggsave(plot = complete_plot, filename = 'Figures/complete_logistic_exp_plot.pdf', width = 9, height = 6)
```

```{=tex}
\begin{figure}[H]
  \caption{Description Each Parameters Logistic Function (Equation \ref{eq:logistic1})}
  \label{fig:combined_plot_1}
  \includegraphics{Figures/combined_plot} \hfill{}
  \caption*{Note. \textup{Panel A shows that the baseline parameter ($\uptheta$) sets the starting value of the of curve, which in the current example has a value of 3.00 ($\uptheta$ = 3.00). Panel B shows that the maximal elevation parameter ($\upalpha$) sets the ending value of the curve, which in the current example has a value of 3.32 ($\upalpha$ = 3.32). Panel C shows that the days-to-halfway elevation parameter ($\upbeta$) sets the number of days needed to reach 50\% of the difference between the baseline and maximal elevation. In the current example, the baseline-maximal elevation difference is 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway elevation parameter defines the number of days needed to reach a value of 3.16. Given that the days-to-halfway elevation parameter is set to 180 in the current example ($\upbeta = 180$), then 180 days are neededto go from a value of 3.00 to a value of 3.16. Panel D shows that the halfway-triquarter delta parameter ($\upgamma$) sets the number of days needed to go from halfway elevation to approximately 73\% of the baseline-maximal elevation difference of 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32). Given that 73\% of the baseline-maximal elevation difference is 0.23 and the halfway-triquarter delta is set to 40 days ($\upgamma = 40$), then 40 days are needed to go from the halfway point of 3.16 to the triquarter point of approximately 3.23).}}
\end{figure}
```


```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# Experiment 1

```{=tex}
\nointerlineskip
\vfill
\newpage
```

In Experiment 1, I investigated the conditions needed to achieve accurate modelling (i.e., low bias and high precision) under different measurement spacing schedules and measurement numbers when there was uncertainty in the nature of change. Before presenting the results of Experiment 1, I will present my design and and analysis goals. For each spacing schedule (equal, time-interval increasing, time-interval decreasing, middle-and-extreme), I used a 4(number of measurements: 5, 7, 9, 11) x 3(nature of change: population value for the fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] of 80, 180, or 280) design. For the analysis, I was primarily interested in determining whether, for each measurement spacing schedule, if the number of measurements could be increased so that accurate modelling (i.e., low bias and high precision) could be achieved across all nature of change values. As a secondary goal, I was also interested in determining whether any measurement spacing schedule provided optimal modelling accuracy (i.e., low bias and high precision) across all manipulated nature of change values. 


## Methods
### Variables Used in Simulation Experiment

#### Independent Variables

To build on current research, Experiment 1 (in addition to all the simulation experiments in my dissertation) used independent variable manipulations from a select number of studies. In looking at the summary of the simulation literature in Table \ref{tab:systematicReview}, the study by @coulombe2016 was the only one to investigate three longitudinal issues of interest to my dissertation, and so represented the most comprehensive investigation. Because my dissertation was also interested in investigating measurement spacing, manipulations were inspired from the only other simulation study to manipulate measurement spacing [the study by @timmons2015]. The sections that follow will discuss each of the variables manipulated in Experiment 1. 

##### Number of Measurements{#number-measurements}

(ref:loehlin2017) [(\textit{p}[{\textit{p} + 1}]/2\; see @loehlin2017]

The smallest measurement number value in @coulombe2016 could not be used in Experiment 1 (or any other simulation experiment that manipulated measurement number in my dissertation) because doing so would have created non-identified models. The model used in my simulations estimated 9 parameters (*p* = 9; 4 fixed-effects + 4 random-effects + 1
error) and so the minimum number of measurements (or observed variables) required for model identification (and to allow model comparison) was 4.\footnote{Degrees of freedom is calculated by multiplying the number of observed variables (\textit{p})
by \textit{p} + 1 and dividing it by 2 (ref:loehlin2017)}. Although a measurement number of three could not be used in my manipulation of measurement number, the next highest measurement number values in @coulombe2016 of 5, 7, and 9 were used. Importantly, a larger value of 11 was added to test for a possible effect of a high measurement number. Therefore, my simulation experiments used the following values in manipulating the number of
measurements: 5, 7, 9, and 11. 

##### Spacing of Measurements(#spacing-measurements)

The only simulation study identified by my systematic review that manipulated measurement spacing was
@timmons2015. Measurement spacing in @timmons2015 was manipulated in the
following four ways:

1)  **Equal spacing**: measurements were divided by intervals of
    equivalent lengths.
    
2)  **Time-interval increasing spacing**: intervals that divided measurements
    increased in length over time.

3)  **Time-interval decreasing spacing**: intervals that divided measurements
    decreased in length over time.

4)  **Middle-and-extreme spacing**: measurements were clustered near the
    beginning, middle, and end of the data collection period.

\noindent To maintain consistency with the established literature, my experiments manipulated measurement spacing in the same way as @timmons2015 presented above. Importantly, because @timmons2015 did not create their measurement spacing schedules with any systematicity, I developed a replicable procedure for generating measurement schedules for each of the four measurement spacing conditions that is described in [Appendix A][Appendix A: Procedure for generating measurement schedules in measurement spacing conditions]. 

Table \@ref(tab:measurementDays) lists the measurement days that were
used for all measurement spacing-measurement number cells. The first
column lists the type of measurement spacing (i.e., equal, time-interval
increasing, time-interval decreasing, or middle-and-extreme); the second
column lists the number of measurements (5, 7, 9, or 11); the third
column lists the measurement days that correspond to each measurement
number-measurement spacing condition; and the fourth column lists the
interval lengths that characterize each set of measurements. Note that
the interval lengths are equal for the equal spacing, increase over time
for the time-interval increasing spacing, and decrease over time for the
time-interval decreasing spacing, For cells with middle-and-extreme
spacing, the measurement days and and interval lengths corresponding to
the middle of the measurement window have been emboldened.

```{r measurementDays, echo=F}
time_period <- 360
num_measurements <- seq(from = 5, to = 11, by = 2)
smallest_int_length <- 30

#meausurement days 
equal_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'equal')$measurement_days
equal_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'equal')$measurement_days
equal_9 <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'equal')$measurement_days
equal_11 <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'equal')$measurement_days

time_inc_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days
time_inc_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days
time_inc_9 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days, 2)
time_inc_11 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days, 2)

time_dec_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days
time_dec_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days
time_dec_9 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days, 2)
time_dec_11 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days, 2)

mid_ext_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days
mid_ext_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days
mid_ext_9 <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days
mid_ext_11 <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days


#measurement intervals
equal_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'equal')$interval_lengths
equal_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'equal')$interval_lengths
equal_9_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'equal')$interval_lengths
equal_11_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'equal')$interval_lengths

time_inc_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths
time_inc_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths
time_inc_9_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths, 2)
time_inc_11_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths, 2)

time_dec_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths
time_dec_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths
time_dec_9_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths, 2)
time_dec_11_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths, 2)

mid_ext_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths
mid_ext_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths
mid_ext_9_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths
mid_ext_11_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths


measurement_days_df <- data.frame('Measurement spacing' = c('Equal', '', '', '', 
                                                            'Time-interval increasing', '', '', '', 
                                                            'Time-interval decreasing', '', '', '', 
                                                            'Middle-and-extreme', '', '', ''), 
                                  'Number of measurements' = rep(num_measurements, times = 4), 
                                  'Measurement days' = c(paste(equal_5, collapse = ', '), 
                                                         paste(equal_7, collapse = ', '), 
                                                         paste(equal_9, collapse = ', '), 
                                                         paste(equal_11, collapse = ', '), 
                                                         
                                                         paste(time_inc_5, collapse = ', '),
                                                         paste(time_inc_7, collapse = ', '),
                                                         paste(time_inc_9, collapse = ', '),
                                                         paste(time_inc_11, collapse = ', '),
                                                         
                                                         paste(time_dec_5, collapse = ', '),
                                                         paste(time_dec_7, collapse = ', '),
                                                         paste(time_dec_9, collapse = ', '),
                                                         paste(time_dec_11, collapse = ', '),
                                                         
                                                        '1, \\textbf{150, 180, 210}, 360',
                                                        '1, 30, \\textbf{150, 180, 210}, 330, 360',
                                                        '1, 30, 60, \\textbf{150, 180, 210}, 300, 330, 360',
                                                        '1, 30, 60, \\textbf{120, 150, 180, 210, 240,} 300, 330, 360'), 
                                  
                                  'Interval lengths' = c(paste(equal_5_int, collapse = ', '), 
                                                         paste(equal_7_int, collapse = ', '), 
                                                         paste(equal_9_int, collapse = ', '), 
                                                         paste(equal_11_int, collapse = ', '), 
                                                         
                                                         paste(time_inc_5_int, collapse = ', '),
                                                         paste(time_inc_7_int, collapse = ', '),
                                                         paste(time_inc_9_int, collapse = ', '),
                                                         paste(time_inc_11_int, collapse = ', '),
                                                         
                                                         paste(time_dec_5_int, collapse = ', '),
                                                         paste(time_dec_7_int, collapse = ', '),
                                                         paste(time_dec_9_int, collapse = ', '),
                                                         paste(time_dec_11_int, collapse = ', '),
                                                         
                                                        '150, \\textbf{30, 30}, 150',
                                                         '30, 120, \\textbf{30, 30}, 120, 30',
                                                         '30, 30, 90, \\textbf{30, 30}, 90, 30, 30',
                                                         '30, 30, 60, \\textbf{30, 30, 30, 30}, 60, 30, 30'),
                                  check.names = F)

kbl(x = measurement_days_df, booktabs = TRUE, format = 'latex', longtable = T, 
      align = c('l', 'l'), 
    linesep = c(rep('', times = 3),
            '\\cmidrule{1-4}\\addlinespace'), 
      caption = 'Measurement Days Used for All Measurement Number-Measurement Spacing Conditions ', 
    escape=F) %>%
   kable_styling(latex_options= c('hold_position'), font_size = 10, position = 'left') %>%  
    footnote(general =  "For conditions with middle-and-extreme spacing, the measurement days and and interval lengths corresponding to the middle of the measurement window have been emboldened.",  threeparttable = T,  escape = F, general_title = '\\\\textit{Note.}\\\\hspace{-1.25pc}')%>%
  landscape(margin = '1cm')

```


##### Population Values Set for The Fixed-Effect Days-to-Halfway Elevation Parameter $\upbeta_{fixed}$ (Nature of Change)

The nature of change was manipulated by setting the days-to-halfway elevation parameter ($\upbeta_{fixed}$) to a value of either 80, 180, or 280 days (see Figure \ref{fig:combined_plot}A). Note that no other study manipulated nature of change and so its manipulation in Experiment 1 was not informed by prior research. Nature of change was manipulated to simulation situations where uncertainty exists in the nature of change. 

#### Constants 

Because sample size was not manipulated in Experiment 1, I set it to have a constant value across all cells. I decided to set the sample size value to the average sample size used in organizational research [*n* = 225\; @bosco2015]. Another variable set to a constant value across the cells was time structuredness (data were assumed to be time structured). 

#### Dependent Variables

##### Convergence Success Rate

The proportion of iterations in a cell where models converged defined
the **convergence success rate**.\footnote{Specifically, convergence was obtained if the convergence code returned by OpenMx was 0.} Equation \@ref(eq:convergence) below shows the calculation used to compute the convergence success rate:

```{=tex}
\begin{align}
  \text{Convergence success rate} =  \frac{\text{Number of models that successfully converged in a cell}}{n},
  (\#eq:convergence) 
\end{align}
```
\noindent where *n* represents the total number of models run in a cell.


##### Bias

Bias was calculated to evaluate the accuracy with which each logistic
function parameter was estimated. As shown below in Equation
\@ref(eq:bias), *bias* was obtained by calculating the difference
between the population value set for a parameter and the average
estimated value in each cell.

\useshortskip
```{=tex}
\begin{align}
  \text{Bias} =  \text{Population value for parameter} - \text{Average estimated value}
  (\#eq:bias) 
\end{align}
```

\noindent Bias was calculated for the fixed- and random-effect parameters of the baseline ($\uptheta_{fixed}$, $\uptheta_{random}$), maximal elevation ($\upalpha_{fixed}$, $\upalpha_{random}$), days-to-halfway elevation ($\upbeta_{fixed}$, $\upbeta_{random}$), and the halfway-triquarter delta parameters ($\upgamma_{fixed}$, $\upgamma_{random}$). 

##### Precision

In addition to computing bias, precision was calculated to evaluate the confidence with which each parameter was estimated in a given cell. *Precision* was obtained by computing the range of values covered by the middle 95% of values estimated for a logistic parameter in each cell. By using the middle 95% of estimated values, a plausible range of population estimates was obtained.   

### Overview of Data Generation and Analysis of Dependent Variables(#data-generation-analysis)

#### Data Generation
##### Function Used to Generate Each Data Set

Data for each simulation experiment were generated using R [@rstudio].
To generate the data, the **multilevel logistic function** shown below
in Equation \@ref(eq:logFunction-generation) was used:

```{=tex}
\begin{align}
  y_{ij} = \uptheta_j + \frac{\upalpha_j - \uptheta_j}{{1 + e^\frac{\upbeta_j - time_i}{\upgamma_j}}} + \upepsilon_{ij}, 
(\#eq:logFunction-generation)
\end{align}
```

\noindent where $\uptheta$ represents the baseline parameter, $\upalpha$
represents the maximal elevation parameter, $\upbeta$ represents the
days-to-halfway elevation parameter, and $\upgamma$ represents
triquarter-halfway delta parameter. Note that, values for $\uptheta$,
$\upalpha$, $\upbeta$, and $\upgamma$ were generated for each *j* person
across all *i* time points, with an error value being randomly generated
at each *i* time point($\upepsilon_{ij}$). In other words, unique
response patterns were generated for each person in each of the 1000
data sets generated per cell.

Figure \ref{fig:combined_plot}A shows that the baseline parameter
($\uptheta$) sets the starting value of the curve, which in the current
example has a value of 3.00 ($\uptheta$ = 3.00). Figure
\ref{fig:combined_plot}B shows that the maximal elevation parameter
($\upalpha$) sets the ending value of the curve, which in the current
example has a value of 3.32 ($\upalpha$ = 3.32). Note that a difference
of 0.32 was selected to represent the average effect size in
organizational research [@bosco2015\; for more information, see section on [population values][Population Values Used for Logistic Function Parameters]]. Figure \ref{fig:combined_plot}C shows that the days-to-halfway elevation
parameter ($\upbeta$) sets the number of days needed to reach 50% of the
difference between the baseline and maximal elevation. In the current
example, the baseline-maximal elevation difference is 0.32
($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway
elevation parameter defines the number of days needed to reach a value
of 3.16. Given that the days-to-halfway elevation parameter is set to
180 in the current example ($\upbeta = 180$), then 180 days are needed
to go from a value of 3.00 to a value of 3.16. Figure
\ref{fig:combined_plot}D shows that the halfway-triquarter delta
parameter ($\upgamma$) sets the number of days needed to go from halfway
elevation to approximately 73% of the baseline-maximal elevation
difference of 0.32. Given that 73% of the baseline-maximal elevation
difference is 0.23 and the halfway-triquarter delta is set to 40 days
($\upgamma = 40$), then 40 days are needed to go from the halfway point
of 3.16 to the triquarter point of approximately 3.23.

```{r logistic-interpretation-plot1, eval=F, include=F}
#setup variables for logistic curve 
time <- seq(from = 1, to = 360, by = 1)
theta <- 3
alpha <- 3.32
beta <- 180
gamma <- 40

logistic_data <- data.frame('day' = time, 
                            'curve_score' = theta + (alpha - theta)/(1 + exp((beta - time)/gamma))) 

#make first and last values exactly equal to theta and alpha 
logistic_data$curve_score[c(1, 360)] <- c(theta, alpha)

baseline <- logistic_data$curve_score[logistic_data$day == 1]
halfway_value <- logistic_data$curve_score[logistic_data$day == beta]
triquarter_value <- logistic_data$curve_score[logistic_data$day == beta + gamma]
maximal_elevation <- logistic_data$curve_score[logistic_data$day == 360]

#df for points 
point_df <- data.frame('day' = c(1, beta, beta+gamma, 360), 
                       'curve_score' = c(baseline, halfway_value, triquarter_value, maximal_elevation), 
                       'beta_brace' = factor(c('beta', 'beta', 'NA', 'NA')),
                       'beta_label' = rep('d[beta]', times = 4), 
                       
                       'gamma_brace' = factor(c('NA', 'gamma', 'gamma', 'NA')), 
                       'gamma_label' = rep('d[gamma]', times = 4),
                       
                       'total_brace' = factor(c('total', 'NA', 'NA', 'total')), 
                       'total_label' = rep('d[total]', times = 4))

font_size <- 8
title_font <- 30
axis_text_size <- 20
axis_title_size <- 24

theta_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(A:~Baseline~(theta)))) + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 440, y = 3.05, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) + 

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


alpha_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(B:~Maximal~elevation~(alpha)))) + 
  #theta 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text',  x = 440, y = 3.27, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.20, label = 'alpha == 3.32', parse = T, size = font_size) + 

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


beta_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label =  expression(bold(C:~Days~to~halfway~elevation~(beta)))) + 
  #theta 
    geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 180, xend = 180, y = 3.16, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 55, y = 3.14, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 55, y = 3.08, label = 'beta == 180~days', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

gamma_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(D:~`Halfway-triquarter`~'delta'~(gamma)))) +
  
  #gamma 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 220, xend = 220, y = 3.233, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 100, y = 3.21, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.15, label = 'gamma == 40~days', parse = T, size = font_size) + 
  
  #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

combined_plot <- ggarrange(theta_plot, alpha_plot, beta_plot, gamma_plot)
ggsave(plot = combined_plot, filename = 'Figures/combined_plot.pdf', width = 18, height = 12)


complete_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 1) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360))+
  annotate(geom = 'text', x = 50, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = 5) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 3) +

  #beta
  annotate(geom = 'text', x = 85, y = 3.15, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 90, y = 3.12, label = 'beta == 180~days', parse = T, size = font_size) +
  geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 180, xend = 180, y = 3.16, yend = 2.98, linetype = 2, size = 0.3) + #vertical dashed line 
  
  #gamma
  annotate(geom = 'text', x = 97, y = 3.233, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.21, label = 'gamma == 40~days', parse = T, size = font_size) + 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 220, xend = 220, y = 3.233, yend = 2.98, linetype = 2, size = 0.3)+  #vertical dashed line  
  
  coord_cartesian(clip = 'off') + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 3, size = 0.3) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 425, y = 3.03, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 425, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) + 

  #alpha 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 3, size = 0.3) + #vertical dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) +      #horizontal arrow
  annotate(geom = 'text', x = 430, y = 3.30, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 430, y = 3.26, label = 'alpha == 3.32', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold',size = title_font), 
        axis.title = element_text(size = 16), 
        axis.text = element_text(size = 13, colour = 'black'))
  
    ##brace information
  #stat_brace(data = point_df %>% filter(beta_brace == 'beta'), 
  #           mapping = aes(group = beta_brace, label = beta_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #stat_brace(data = point_df %>% filter(gamma_brace == 'gamma'), 
  #           mapping = aes(group = gamma_brace, label = gamma_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) +
  #
  #stat_brace(data = point_df %>% filter(total_brace == 'total'), 
  #           mapping = aes(group = total_brace, label = total_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #description box 
  #annotate(geom = 'rect', xmin = 235, xmax = 355, ymin = 3.02, ymax = 3.15, alpha = 0.1, color = 'black') + 
  #annotate(geom = 'text', x = 295, y = 3.13, label = 'd[total] == alpha~-~theta == 0.32', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.09, label = 'd[beta] == 0.5~(d[total]) == 0.16', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.06, label = 'd[gamma] == 0.23~(d[total]) == 0.07', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.03, label = 'd[beta]~+~d[gamma] == 0.73~(d[total]) == 0.23', parse = T, size = 4.5) + 

ggsave(plot = complete_plot, filename = 'Figures/complete_logistic_exp_plot.pdf', width = 9, height = 6)
```

```{=tex}
\begin{figure}[H]
  \caption{Description of Each Parameter of Four-Parameter Logistic Function}
  \label{fig:combined_plot}
  \includegraphics{Figures/combined_plot} \hfill{}
  \caption*{Note. \textup{Panel A shows that the baseline parameter ($\uptheta$) sets the starting value of the of curve, which 
in the current example has a value of 3.00 ($\uptheta$ = 3.00). Panel B shows that the maximal elevation parameter ($\upalpha$) sets the ending value of the curve, which in the current example has a value of 3.32 ($\upalpha$ = 3.32). Panel C shows that the days-to-halfway elevation parameter ($\upbeta$) sets the number of days needed to reach 50\% of the difference between the baseline and maximal elevation. In the current example, the baseline-maximal elevation difference is 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway elevation parameter defines the number of days needed to reach a value of 3.16. Given that the days-to-halfway elevation parameter is set to 180 in the current example ($\upbeta = 180$), then 180 days are neededto go from a value of 3.00 to a value of 3.16. Panel D shows that the halfway-triquarter delta parameter ($\upgamma$) sets the number of days needed to go from halfway elevation to approximately 73\% of the baseline-maximal elevation difference of 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32). Given that 73\% of the baseline-maximal elevation difference is 0.23 and the halfway-triquarter delta is set to 40 days ($\upgamma = 40$), then 40 days are needed to go from the halfway point of 3.16 to the triquarter point of approximately 3.23).}}
\end{figure}
```

The logistic growth function (Equation \ref{eq:logFunction-generation}
was used because it is a common pattern of organizational change [or
institutionalization\; @lawrence2001]. Institutionalization curves follow
an s-shaped pattern of the logistic growth function, and so their rates
of change can be represented by the days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta$, $\upgamma$,
respectively), and the success of the change can be defined by the
magnitude of the difference between baseline and maximal elevation
parameters ($\upalpha$ - $\uptheta$, respectively).

##### Population Values Used for Function Parameters

(ref:bosco2015) [@bosco2015]

Table \@ref(tab:parameterValues) lists the parameter values that will be
used for the population parameters. Given that the decisions for setting the values for the baseline, maximal elevation, and residual variance parameters were informed by past research, the discussion that follows highlights how these decisions were made. The difference between the baseline and maximal elevation
parameters ($\uptheta$ and $\upalpha$, respectively) corresponded to the effect size most commonly observed in organizational research [i.e., the 50^th^ percentile effect size value\; @bosco2015]. Because the meta-analysis of @bosco2015 computed effect sizes as correlations, the the 50^th^ percentile effect size value of $r = .16$ was computed to a standardized effect size using the following conversion function shown in Equation \ref{eq:conversion-effect} [@borenstein2009, Chapter 7]:

\begin{align}
d = \frac{2r}{\sqrt{1 - r^2}}, 
(\#eq:conversion-effect)
\end{align}

\noindent where $r$ is the correlation effect size. Using Equation \ref{eq:conversion-effect}, a correlation value of $r = .16$ becomes a standardized effect size value of $d = 0.32$. For the value residual variance parameter, its value in @coulombe2016 was set to the value used for the value of the intercept variance parameter. In the current context, the intercept of the logistic function (Equation \ref{eq:logistic1}) is the baseline parameter.\footnote{The definition of an intercept parameter is the value of a curve when no time has elapsed, and this is precisely the definition of the baseline parameter ($\uptheta$). Therefore, the variance of the intercept parameter carries the same meaning as the variance of the baseline parameter ($\uptheta_{random}$).} Given that the value for the variability of the baseline parameter was 0.05 (albeit in standard deviation units), the value used for the residual variance parameter was 0.05 ($\upepsilon = 0.05$). Because justification for the other parameters could not be found in any of the simulation studies identified in my systematic review, values set for the other parameters was largely arbitrary. 

To facilitate interpretation of the results, data were generated to resemble the commonly used Likert (range of 1--5) by using a standard deviation of 1.00 and change was assumed to occur over a
period of 360 days. The decision to generate data in the context of a
360-day period was made because many organizational processes are often
governed by annual events (e.g., performance reviews, annual returns,
regulations, etc.). Importantly, because @coulombe2016 set covariances
between parameters to zero, all the simulation experiments used
zero-value covariances.

```{r parameterValues, echo=F}
#specify parameters for parameter table 
theta <- 3
alpha <- 3 + .32*1
beta <- 180
gamma <- 20

sd_alpha <- 0.05
sd_theta <- 0.05
sd_beta <- 10
sd_gamma <- 4

sd_error <- 0.05


#table of parameter values
parameterValues_df <- data.frame('Parameter' = c('Parameter means',
                                         'Baseline, $\\uptheta$',
                                         'Maximal elevation, $\\upalpha$', 
                                         'Days-to-halfway elevation, $\\upbeta$', 
                                         'Triquarter-halfway delta, $\\upgamma$', 
                                         
                         'Variability and covariability (in standard deviations)', 
                              'Baseline standard deviation, $\\uppsi_{\\uptheta}$',
                              'Maximal elevation standard deviation, $\\uppsi_{\\upalpha}$', 
                              'Days-to-halfway elevation standard deviation, $\\uppsi_{\\upbeta}$',
                              'Triquarter-halfway delta standard deviation, $\\uppsi_{\\upgamma}$',
                         
                              'Baseline-maximal elevation covariability, $\\uppsi_{\\uptheta\\upalpha}$',
                              'Baseline-days-to-halfway elevation covariability, $\\uppsi_{\\uptheta\\upbeta}$',
                              'Baseline-triquarter-halfway delta covariability, $\\uppsi_{\\uptheta\\upgamma}$',
                         
                              'Maximal elevation-days-to-halfway elevation covariability, $\\uppsi_{\\upalpha\\upbeta}$',
                              'Maximal elevation-triquarter-halfway delta covariability, $\\uppsi_{\\upalpha\\upgamma}$',
                         
                              'Days-to-halfway elevation-triquarter-halfway delta covariability, $\\uppsi_{\\upbeta\\upgamma}$',
                          
                              'Residual standard deviation, $\\uppsi_{\\upepsilon}$'), 
                         'Value' = c('', theta, alpha, beta, gamma, 
                                     '', sd_theta, sd_alpha, sd_beta, sd_gamma, 
                                     0, 0, 0, 0, 0,0,  sd_error), check.names = F)

#round numbers to that they print with two significant numbers
parameterValues_df$Value <- round(as.numeric(as.character(parameterValues_df$Value)), 3)
parameterValues_df$Value <- formatC(round(parameterValues_df$Value, 3), format='f', digits=2)

#replace '  NA' with empty string 
parameterValues_df$Value[parameterValues_df$Value ==" NA"] <- ''


kbl(parameterValues_df, booktabs = TRUE, format = 'latex', longtable = T, 
    linesep = c(rep('', times = 4), '\\addlinespace\\addlinespace', 
                rep('', times = 11))
    , 
    align = c('l', 'c'), 
    caption = "Values Used for Multilevel Logistic Function Parameters", 
    escape = F) %>%
   add_indent(positions = c(2:5, 7:16, 17), level_of_indent = 2) %>%
   kable_styling(latex_options= c('hold_position', 'repeat_header'), position = 'left', font_size = 10) %>%
  footnote(general =  "The difference between $\\\\alpha$ and $\\\\theta$ corresponds to the 50$\\\\mathrm{^{th}}$ percentile Cohen's $d$ value of 0.32 in organizational psychology (Bosco et al., 2015).",  threeparttable = T,  escape = F, general_title = '\\\\textit{Note.}\\\\hspace{-1pc}') %>%
   column_spec(column = 1, width = '12 cm')
```

##### Modelling of Each Generated Data Set

Each data set generated by the multilevel logistic function (Equation \ref{eq:logFunction-generation}) was analyzed using a modified latent growth curve model known as a structure latent growth curve model [@preacher2015]. To fit the logistic function (Equation \ref{eq:logistic1}) to a given data set, a linear approximation of the logistic function was needed so that it could fit within the structural equation modelling framework---a linear framework.\footnote{The logistic function (Equation \ref{eq:logistic1}) cannot be directly inserted into the structural equation modelling framework because it is a linear framework: It on allows matrix-matrix, matrix-vector, and vector-vector operations. Unfortunately, the algebraic operations permitted in a linear framework cannot directly reproduce logistic function operations and so a linear approximation of the logistic function must be constructed so that the logistic function can be inserted into the structural equation modelling framework.} To construct a linear approximation of the logistic function, a first-order Taylor series was constructed for the logistic function. For a detailed explanation of how the logistic function was fit into the structural equation modelling framework, see [Technical Appendix B](#structured-latent). 


##### Nonlinear Latent Growth Curve Model Used to Analyze Each Generated Data Set{#structured-latent}
#### Analysis of Dependent Variables 

To analyze the results, I determined whether bias and prevision values in the estimation of each logistic function parameter exceeded minimum effect sizes of interest. By identifying minimum effect sizes of interest, I could determine whether bias and precision values were meaningfully large [@murphy2014]. I also computed variance-accounted-for effect sizes for each experimental effect in each simulation experiment to understand what effects were practically significant.

##### Minimum Effect Sizes of Interest for Bias 
 
Minimum effect sizes of interest for bias were calculated as a percentage of the parameter's population value. In accordance with several simulation studies, an estimate with a bias value of less than 10% of the parameter's population value was deemed acceptable [@muthen1997]. 

##### Minimum Effect Sizes of Interest for Precision

Because precision was defined as the range of values covered by the middle 95% of estimated values for a given parameter (see [precision][Precision]), precision provided a range of plausible population estimates. Precision was deemed acceptable if the plausible range of population estimates did not allow for any estimates with a bias value greater than the 10% minimum effect size of interest. 

##### Computation of Variance-Acccounted-For Effect Sizes 

Among the several effect size metrics---at a broad level, effect size
metrics can represent standardized differences or variance-accounted-for
measures that are corrected or uncorrected for sampling error---the corrected
variance-accounted-for effect size metric of partial $\upomega^2$ was
chosen because of two desirable properties. First, partial
$\upomega^2$ provides a less biased estimate of effect size than other
variance-accounted-for measures [@okada2013]. Second, partial $\upomega^2$ is more
robust to assumption violations of normality and homogeneity of variance
[@yigit2018]. Given that parameter estimates were often non-normally distributed across cells, effect size values computed with using partial $\upomega^2$ should be relatively less biased than
other variance-accounted-for effect size metrics (e.g., $\eta^2$). To
compute partial $\upomega^2$ value for each experimental effect,
Equation \ref{eq:partial-omega} shown below was used:

```{=tex}
\begin{align}
\text{partial} \upomega^2 = \frac{\sigma^2_{effect}}{\sigma^2_{effect} + MSE} 
(\#eq:partial-omega)
\end{align}
```

\noindent where $\sigma^2_{effect}$ represents the variance attributable
to an effect and $MSE$ is the mean squared error.

To compute partial $\upomega^2$ values for effects, a Brown-Forsythe test was computed and the appropriate
sum-of-squares terms were used to compute partial $\upomega^2$ values. To compute the Brown-Forsythe test, median absolute values were computed from the median value of each estimated value in each cell as shown in
Equation \ref{eq:brown-forsythe} shown below:

\begin{align}
\text{Median absolute deviation} = \lvert \text{Parameter estimate} - \text{Median parameter estimate}_{cell} \rvert.
(\#eq:brown-forsythe)
\end{align}

\noindent An ANOVA was then computed on the median absolute deviation
values (using the independent variables of the experiment as predictors), with the terms in Equation \ref{eq:partial-omega} extracted from
the ANOVA output to compute partial $\upomega^2$ values. Note that
deviations from the median value in each cell were used because using
the median protects against the biasing effects of skewed distributions
that were observed in the parameter estimate distributions current simulation experiments [@brown1974]. An effect was deemed as meaningful if it was larger than .01; this value has been used in previous simulation research [@krull1999; @murphy2011] and is the cutoff value for a small effect [@olejnik2000]. 

##### Analysis of Convergence Success Rate

For the analysis of convergence success rate, the mean convergence
success rate was computed for each cell in each experiment (see section on
[convergence success rate][Convergence Success Rate]). Because convergence rates exhibited little
variability across cells due to the nearly unanimous high rates (almost all cells across all experiments had convrergence success rates above 90%), examining the effects of any independent variable on these rates using
partial $\upomega^2$ values would have provided little information, and so
only the average convergence success rate for each cell was reported (see [Appendix B](#appendix-a-convergence-rates)). 




## Overview of Data Processing and Modelling Procedure

Due to the considerable number of analyses conducted in each
experiment, I have written the following sections to equip the reader with a framework to efficiently navigate the results sections of each experiment. In the sections that follow , I will present overviews of the following topics: a) Data pre-processing and model convergence, b) meaning of logistic function parameters, c) presentation of variability in parameter estimation, d) interpretation of parameter estimation plots, and e) the model estimation procedure. 

### Pre-Processing of Data and Model Convergence

After collecting the output from the simulations, non-converged models
(and their corresponding parameter estimates) were removed from
subsequent analyses. Tables \ref{tab:conv-exp-1}--\ref{tab:conv-exp-3}
in [Appendix B](#appendix-a-convergence-rates) provide the convergence
success rates for each cell in each of the three simulation experiments.
Model convergence was almost always above 90% and convergence rates
rates below 90% only occurred in the following frequencies in each
experiment:

-   Experiment 1: two instances of sub-90% model convergence rates (see
    Table \ref{tab:conv-exp-1})
-   Experiment 2: two instances of sub-90% model convergence rates (see
    Table \ref{tab:conv-exp-2})
-   Experiment 3: two instances of sub-90% model convergence rates (see
    Table \ref{tab:conv-exp-3})

\noindent Note that all instances of sub-90% model convergence occurred
with five measurements.

### Review of Logistic Function Parameters Used to Generate Data

Data in each experimental condition were generated using the the
logistic function shown below in Equation
\ref{eq:logFunction-generation2}:

```{=tex}
\begin{align}
y_{pi} = \uptheta_p + \frac{\upalpha_p - \uptheta_p}{{1 + e^\frac{\upbeta_p - time_i}{\upgamma_p}}} + \upepsilon_{pi}.
(\#eq:logFunction-generation2)
\end{align}
```

\noindent where $\uptheta$ represents the baseline parameter, $\upalpha$
represents the maximal elevation parameter, $\upbeta$ represents the
days-to-halfway elevation parameter, and $\upgamma$ represents
triquarter-halfway delta parameter. Note that, values for $\uptheta$,
$\upalpha$, $\upbeta$, and $\upgamma$ were generated for each *p* person
across all *i* time points, with an error value being randomly generated
at each *i* time point ($\upepsilon_{ij}$). In other words, unique
response patterns were generated for each person in each of the 1000
data sets generated per cell. For a review of the logistic function, see
the section on [data generation](#generation)..

### Presentation of Precision

Given that sampling error causes any population parameter to be
estimated with some degree of variability and that two of the three
simulation experiments manipulated sample size, variability was expected
to occur in the estimation of the logistic function parameters. Even in
situations where sample size was not manipulated (as in Experiment 1) and
where it may have seemed unusual for other independent variables to affect the
variability with which a parameter was estimated, variables outside of
sample size have been shown to affect the precision of parameter
estimation on occasion [e.g., @coulombe2016]. Given that the precision
of parameter estimation may be affected in each simulation experiment,
the paragraphs that follow explain how precision was
visualized.

As a reminder, I defined precision as the range covered
by the middle 95% of estimated values for a given logistic function parameter (see [precision][Precision]). A depiction of precision is shown in Figure \ref{fig:beta-histogram} for the days-to-halfway elevation parameter ($\upbeta$). In Figure \ref{fig:beta-histogram}, a density distribution is shown for the values
estimated for the days-to-halfway elevation parameter ($\upbeta$) in a cell.The
region of the density distribution shaded in gray represents the middle
95% of values estimated for the days-to-halfway elevation parameter
($\upbeta$) and the upper and lower values of this region set the upper
and lower values of the error bar that lies above the density distribution. Importantly, I used error bars to represent precision in parameter estimation because using density distributions for each of the nine parameters in each experimental cell would have been impractical given the large number of cells in each experiment.

To visualize precision across multiple experimental conditions for one
logistic function parameter, parameter estimation plots were
constructed. Figure \ref{fig:beta-density-to-param-plot} shows the
procedure that was followed to create a parameter estimation plot for the days-to-halfway elevation
parameter ($\upbeta$). For each cell (all measurement number-sample size combinations in the current example), a density distribution was computed for the values estimated for the days-to-halfway elevation
parameter ($\upbeta$), and the range covered by the middle 95% of values
in the density distribution was used to set the length of each error bar.
The plot at the bottom of Figure \ref{fig:beta-density-to-param-plot} is
a *parameter estimation plot* for the days-to-halfway elevation
parameter (specifically, the fixed-effect days-to-halfway elevation
parameter [$\upbeta_{fixed}$]), with the error bars showing the
precision with which $\upbeta_{fixed}$ was modelled in each sample
size-measurement number combination. This style of error bar was used
to represent precision for each parameter in each experimental
cell.

```{r variability-histograms, eval=F, include=F}
exp_2 <- read_csv(file = 'data/exp_2_data.csv')

#function does not work anymore 
generate_param_density_plot(raw_exp_data = exp_2, param_summary_data = param_summary_exp_2, spacing = 'Equal', num_measurements = 5, sample_size = 30)

```

```{=tex}
\begin{figure}
  \caption{Density Distribution of Values Estimated for the Days-to-Halfway Elevation Parameter ($\upbeta$)}
  \label{fig:beta-histogram}
  \includegraphics[height = 8cm, width = 20cm]{Figures/beta_fixed_Equal_5_30} \hfill{}
  \caption*{Note. \textup{Area shaded in gray represents the middle 95\% of estimated values. The upper and lower limits of the shaded areay define the upper and lower limits of the error bar on top of the density distribution.}}
\end{figure}
```

```{r beta-density-to-param-plot, eval=F, include=F}
num_measurements_levels <- as.numeric(levels(param_summary_exp_2$number_measurements))
sample_size_levels <- range(as.numeric(levels(param_summary_exp_2$sample_size)))
exp_2 <- read_csv(file = 'data/exp_2.csv')

for (num_measurements in num_measurements_levels) {
  
  for(sample_size in sample_size_levels){
    
    generate_param_density_plot(raw_exp_data = exp_2, param_summary_data = param_summary_exp_2, 
                                spacing = 'Equal', num_measurements = num_measurements,
                                sample_size = sample_size)
  }
}
```

```{=tex}
\begin{figure}
  \caption{Depiction of Modelling Procedure for Generating Error Bars on Parameter Estimation Plots}
  \label{fig:beta-density-to-param-plot}
  \includegraphics[height = 25cm, width = 10cm]{Figures/density_to_param_plot} \hfill{}
  \caption*{Note. \textup{Density distributions are generated for each experimental cell and the range covered by the middle 95\% of values in each density distribution is used to create an error bar. The plot at the bottom is a \textit{parameter estimation plot} for the days-to-halfway elevation parameter ($\upbeta$) and it shows the accuracy with which $\upbeta$ is estimated across all sample size-measurement number combinations when measurement spacing is equal.}}
\end{figure}
```

### Interpreting a Parameter Estimation Plot

Given that several parameter estimation plots were presented in the
results sections of each experiment, I will provide an overview of how
to interpret these plots. Parameter estimation plots show two indicators
of estimation accuracy: bias and precision. In the sections that
follow, I will explain how these two accuracy indicators are depicted by parameter estimation
plots.

#### Bias

Figure \ref{fig:param-estimation-ex} shows a parameter estimation plot for the fixed-effect halfway-triquarter parameter ($\upgamma_{fixed}$) across all measurement number-sample size combinations. The dots (squares, circles, triangles, diamonds) indicate the average estimated value and the error bars show the range of values
covered by the middle 95% of the estimated values (see [presentation of precision][Presentation of Precision]). The horizontal blue line indicates the population value ($\upgamma_{fixed}$ = 4.00) and the gray band indicates the acceptable margin of error (i.e., $\pm$ 10% of the population value). Dots that lie outside of the margin of error are empty and error bars that exceed 20% of the parameter's population value are coloured in light blue.

*Bias* describes the extent to which an estimate either over- or
underestimates the population value. Looking at the parameter estimation
plot in Figure \ref{fig:param-estimation-ex}, bias is
represented by whether a dot lies outside of the gray band (i.e., the 10% margin of error). In the current example, the average value estimated for the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$) only has an empty dot (i.e., has bias above the tolerable 10%) with a sample size of 50. Therefore, bias is acceptable in almost all conditions.

#### Precision

*Precision* is described by the length of the error bars in the parameter estimation plot. In the current example of Figure \ref{fig:param-estimation-ex}, precision decreases monotonically as sample size and measurement number increase. Importantly, the error bars are often light blue (i.e., above the 20% cutoff) and are only black (i.e., below the 20% cutoff) with sample sizes of at least 200 and seven measurements. 

```{=tex}
\begin{figure}
  \caption{Parameter Estimation Plot for Fixed-Effect Days-to-Halfway Elevation Parameter ($\upgamma_{fixed}$)}
  \label{fig:param-estimation-ex}
  \includegraphics[height = 27cm, width = 11cm]{Figures/param_estimation_ex} \hfill{}
  \caption*{Note. \textup{The dots (squares, circles, triangles, diamonds) indicate the average estimated value and the error bars show the range of values covered by the middle 95\% of the estimated values (see section \ref{Presentation of Precision}). The horizontal blue line indicates the population value ($\upgamma_{fixed}$ = 4.00) and the gray band indicates the acceptable margin of error (i.e., $\pm$10\% of the population value). Dots that lie outside of the margin of error are empty and error bars whose lengths exceed 20\% of the parameter's population value are coloured in light blue. Parameter estimation plots show two markers of estimation accuracy: bias and precision \textit{Bias} describes the extent to which an estimate either over- or underestimates the population value. In the parameter estimation plot,  bias is represented by whether a dot lies outside of the the gray band (i.e., the 10\% margin of error). \textit{Precision} is described by the length of the error bars in the parameter estimation plot.}}
\end{figure}
```


### Model Estimation Procedure

Because a considerable number of parameters were estimated in each cell,
I will review the modelling procedure as a whole. Figure
\ref{fig:results-plot-primer} shows that each parameter of the logistic
function (for each parameter, see Figure \ref{fig:combined_plot}) was
modelled as a fixed and random effect. Values predicted for fixed-effect
parameters are constant across all individuals, whereas values predicted
for random-effect parameters represent the variability with which a
parameter is estimated.\footnote{Estimating a random-effect for a parameter allows person-specific values to be computed for the parameter.} In addition to the random- and fixed-effects estimated for each logistic
function parameter, an error term ($\upepsilon$) was also estimated. 

One important point to mention is that the results of Experiments 1--3
focused on the effects of experimental variables on day-unit parameters
(days-to-halfway elevation [$\upbeta$; Figure
\ref{fig:combined_plot}C] and halfway-triquarter delta
parameters[$\upgamma$; Figure \ref{fig:combined_plot}D]. Across all
three experiments, experimental variables had little effect on the
estimation of Likert-unit parameters (baseline [$\uptheta$; Figure
\ref{fig:combined_plot}A] and maximal elevation [$\upalpha$]; Figure
\ref{fig:combined_plot}B), and so including their estimation plots would
have added unnecessary length and complexity to the results sections of
each experiment. Note that the parameter estimation plots for
Likert-unit parameters were been included in [Appendix
C](#appendix-c-parameter-estimation-plots-for-likert-unit-parameters).

```{=tex}
\begin{figure}[H]
  \caption{Set of Parameters Estimated in Each Simulation Experiment}
  \label{fig:results-plot-primer}
  \includegraphics[height = 17cm, width = 15cm]{Figures/logistic_results_plot} \hfill{}
  \caption*{Note. \textup{Each parameter of the logistic function (for a review, see Figure \ref{fig:combined_plot}) is modelled as a fixed and random effect. Values predicted for fixed-effect parameters are constant across all individuals, whereas values predicted for random-effect parameters are unique across individuals. In addition the random- and fixed-effects estimated for each logistic function parameter, an error term ($\upepsilon$) is also estimated. For each experimental condition, parameter estimation plots will be created for each logistic function parameter that show the accuracy with which each parameter is modelled.}}
\end{figure}
```



## Results and Discussion

In the sections that follow, I will organize the results by presenting them for each measurement spacing schedule (equal, time-interval increasing, time-interval decreasing, middle-and-extreme). Importantly, only the results for the day-unit parameters will be presented (i.e., fixed- and random-effect days-to-halfway elevation and halfway-triquarter delta parameters [$\upbeta_{fixed}$, $\upbeta_{random}$, $\upgamma_{fixed}$, $\upgamma_{random}$, respectively]). The results for the likert-unit parameters (i.e., fixed- and random-effect baseline and maximal elevation parameters [$\uptheta_{fixed}$, $\uptheta_{random}$, $\upalpha_{fixed}$, $\upalpha_{random}$, respectively]) were largely trivial and so are presented in [Appendix C](#appendix-c)). 

For each measurement spacing schedule, I first provide a concise summary of the results and then provide a detailed report of the estimation accuracy of each day-unit parameter of the logistic function. Because the length of the detailed reports is considerable, I provide concise summaries before the detailed reports to provide a framework with which to understand the detailed reports. The detailed report of each measurement spacing schedule will summarize the results of each day-unit's parameter estimation plot, report partial $\upomega^2$ values for each day-unit parameter of the 4(number of measurements: 5, 7, 9, 11) x 3(nature of change: population value for the fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] of 80, 180, or 280) design, and then provide a qualitative summary.

### Equal Spacing

#### Concise Summary 

For equal spacing, Table \ref{tab:summary-table-equal-spacing-exp1} provides a concise summary of the results for the day-unit parameters (see Figure \ref{fig:exp1_plot_equal} for the corresponding parameter estimation plots). Text within the bias and precision columns indicates the number of measurements needed to achieve either low bias or high precision across all manipulated nature of change values. Bias is low in all cells for each day-unit parameter except the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$), where bias is low across all manipulated nature of change values with 11 measurements. Therefore, with equal spacing, at least 11 measurements are needed to estimate all day-unit parameters with low bias across all manipulated nature of change values. 

With respect to precision with equal spacing, a slightly different pattern occurs than with bias. For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), precision is high across all cell values. For the fixed-effect triquarter-halfway delta prameter ($\upgamma_{fixed}$), precision is high when seven or more measurements are used.  For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] and the random-effect triquarter-halfway elevation parameter [$\upgamma_{random}$]), precision is low in all cells. Therefore, with equal spacing, although nine measurements allows the fixed-effect day-unit parameters to be modelled with high precision across all manipulated nature of change values, no measurement number allows the random-effect day-unit parameters to be modelled with high precision.  

In summarizing the results with equal spacing, low bias is achieved in the estimation of all day-unit parameters across all manipulated nature of change values with 11 measurements, but high precision is never achieved in the estimation of all day-unit parameters with any manipulated measurement number. Although it may be discouraging that 11 measurements separated over time with time-interval decreasing cannot achieve high precision in the estimation of all day-unit parameters, the improvements in bias and precision in the estimation of each parameter diminish after the number of measurements increases beyond a certain value. With equal spacing, the improvements in bias and precision across all day-unit parameters diminish after seven measurements.

#### Detailed Report

For equal spacing, Figure \ref{fig:exp1_plot_equal} shows the parameter estimation plots for the day-unit parameters. Error bars represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter (with population values of $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, and $\upgamma_{random}$ = 4.00), and gray bands specify the acceptable margins of error (i.e., 10% of the parameter's population value). I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. Note that Table \ref{tab:omega-exp1-equal} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing. 

```{r summary-table-equal-spacing-exp1, echo=F}

summary_table <- data.frame('Parameter' = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_equal}A)',
                                                  '$\\gamma_{fixed}$ (Figure \\ref{fig:exp1_plot_equal}B)', 
                                                  '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_equal}C)', 
                                                  '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_equal}D)'), 
                            'Low Bias' = c('All cells', 
                                           'All cells', 
                                           'All cells', 
                                       '11+ measurements'), 
                            'High Precision' = c('All cells', 
                                            '9+ measurements', 
                                            'No cells',
                                            'No cells'), 
                            'Qualitative Summary' = c('Diminishing improvements in precision with 7+ measurements', 
                                                         'Diminishing improvements in precision with 7+ measurements', 
                                                         'Diminishing improvements in precision with 7+ measurements', 
                                                         'Diminishing improvements in bias and precision with 7+ measurements'), 
                            
                            check.names = F)

kbl(x = summary_table, format = 'latex',
       longtable = T, booktabs = T, centering = T, escape = F,
       align = c('l', rep('l', times = ncol(summary_table) - 1)), 
    caption = 'Concise Summary of Results for Equal Spacing in Experiment 1') %>%
   #header
  column_spec(column = 2, width = '5cm') %>%
  column_spec(column = 3, width = '5cm') %>%
  column_spec(column = 4, width = '9.5cm') %>%
  #column_spec(column = 3, width = '4cm') %>%
  #footnotes
  footnote(escape = F, threeparttable = T, general_title = '\\\\textit{Note.}\\\\hspace{-1.25pc}',
           general = "\\\\phantom{fi} $\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter, 
           $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, 
           $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter.") %>% 
    kable_styling(position = 'left') %>%
  landscape(margin = '1cm')

```

```{r plots-equal-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Equal spacing',
                                x_axis_name = expression("Population Value Set for"~beta[fixed]), 
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -30, beta_upper = 25, beta_ticks = 5)

```

```{r text-values-equal-exp1, echo=F}
gamma_rand_equal_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 7, midpoint == 180, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci),
         estimate = as.integer(estimate)) 

gamma_fixed_equal_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, midpoint == 80, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

gamma_fixed_equal_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, midpoint == 180, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

gamma_fixed_equal_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, midpoint == 280, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

gamma_fixed_equal_7_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 7, midpoint == 180, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

##### Bias 

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp1_plot_equal}), bias only occurred in the estimation of the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D) is difficult to see because 10% of the parameter's population value is a small value relative to the range of the y-axis (i.e., 10% of 4.00 is 0.40, which is small relative to the y-axis range of 55). 

For the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D), bias occurred on the five-measurements line (squares; i.e., dots fell outside the gray 10% margin of error band) at population values of 80, 180, and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the seven-measurements line (circles), dots fell outside the gray 10% margin of error band at population values of 180 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the nine-measurements line (triangles), dots fell outside the gray 10% margin of error band at a population value of 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the eleven-measurements line (diamonds), no dot falls outside the gray 10% margin of error. Therefore, using equal spacing, low bias can be achieved in the estimation of the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$)---and, consequently, all the day-unit parameters---by using eleven measurements.

##### Precision

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp1_plot_equal}), precision was low for all parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp1_plot_equal}A). I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision are coloured in light blue.

For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_equal}C), error bar

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Equal Spacing in Experiment 1}
  \label{fig:exp1_plot_equal}
  \includegraphics{Figures/exp1_plot_days_equal spacing} \hfill{}
\end{figure}
\begin{figure}[H]
\caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

 
\noindent lengths were longer than the 20% tolerated amount with five measurements at population values of 80, 180, and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). With seven measurements, error bar lengths were longer than the 20% acceptable amount at a population value of 180 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, using equal spacing, nine or more measurements are needed to estimate the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$) with high precision across all manipulated nature of change values.

For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] in Figure \ref{fig:exp1_plot_equal}B and the random-effect halfway-triquarter delta parameter [$\upgamma_{random}$] in Figure \ref{fig:exp1_plot_equal}D), error bar lengths are longer than the 20% acceptable amount in all the cells. Therefore, with equal spacing, precision remains low for the estimation of the random-effect day-unit parameters across all measurement number values. 

In summary, with equal spacing, high precision is achieved in the estimation of the fixed-effect day-unit parameters across all manipulated nature of change values with nine measurements, but no manipulated measurement number achieves high precision in the estimation of the random-effect day-unit parameters. 

##### Partial $\upomega^2$ Effect Sizes

Table \ref{tab:omega-exp1-equal} lists the partial $\upomega^2$ effect size values for each effect with each parameter when equal spacing was used. The number of measurements accounts for the greatest amount of variability in the parameter estimation accuracy of each day-unit parameter. 

```{r omega-exp1-equal, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'equal', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'M', 'NM x M'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Equal Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), M = population value set for $\\\\upbeta_{fixed}$ (80, 180, 280), NM x M = interaction between number of measurements and population value set for $\\\\upbeta_{fixed}$. \\\\phantom{ indicate conditions where}, $\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter, 
           $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, and 
           $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_equal}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_equal}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_equal}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_equal}D)'))
```




##### Qualitative Summary  

In looking at the results for equal spacing in Figure \ref{fig:exp1_plot_equal}, general patterns of results exist in bias and precision. For  bias, it decreases the most across all the manipulated nature of change values in the estimation of the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$) when the number of measurements goes from five to seven. With respect to precision, a pattern similar to that with bias occurs in the estimation of all day-unit parameters: The largest improvements in precision across all day-unit parameters and across all manipulated nature of change values occur when the number of measurements increases from five to seven. Therefore, for equal spacing, even though error bar lengths remain long (indicative of low precision) for the random-effect day-unit parameters across all manipulated measurement numbers, the largest improvements in bias and precision across all manipulated nature of change values for all day-unit parameters are achieved with seven measurements.

### Time-Interval Increasing Spacing

```{r plots-time-increasing-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Time-interval increasing',
                                x_axis_name = expression("Population Value Set for"~beta[fixed]), 
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -60, beta_upper = 60, beta_ticks = 10)
```

```{r density-plot-functions, include=F, eval=F}
compute_ind_param_error_bar_range <- function(param_search_name, param_data) {
  
  #extract estimates from single parameter
  ind_param_data <-  param_data %>%
    filter(str_detect(string = param_data$parameter, pattern = param_search_name)) %>% 
    pull(estimate)
  
  param_range <- as.numeric(compute_middle_95_estimate(param_data = ind_param_data))
  
  return(param_range)
}

compute_80_ind_param_error_bar_range <- function(param_search_name, param_data) {
  
  #extract estimates from single parameter
  ind_param_data <-  param_data %>%
    filter(str_detect(string = param_data$parameter, pattern = param_search_name)) %>% 
    pull(estimate)
  
  param_range <- as.numeric(compute_middle_80_estimate(param_data = ind_param_data))
  
  return(param_range)
}

compute_90_ind_param_error_bar_range <- function(param_search_name, param_data) {
  
  #extract estimates from single parameter
  ind_param_data <-  param_data %>%
    filter(str_detect(string = param_data$parameter, pattern = param_search_name)) %>% 
    pull(estimate)
  
  param_range <- as.numeric(compute_middle_90_estimate(param_data = ind_param_data))
  
  return(param_range)
}

compute_density <- function(error_bar_range, group, param_name, density_data) { 
  
  density_data <- density_data[density_data$group == group , ]
  
  density_lower_x <- min(which(density_data$x >= error_bar_range[1]))
  density_upper_x <- max(which(density_data$x <= error_bar_range[2]))
  
  day_values <- density_data$x[density_lower_x:density_upper_x]
  
  
  density_df <- data.frame('parameter' = param_name,
                           'day_value' = day_values,
                           'probability' = density_data$y[density_lower_x:density_upper_x], 
                           'max_density_value' = max(density_data$y),
                           'lower_ci' = error_bar_range[1], 
                           'upper_ci' = error_bar_range[2])

    return(density_df)
}
```

```{r exp1-density-plot-time-inc, include=F, eval=F}
##generate figure showing density distributions if gamma_fizxd at N = 500 and N = 1000 with middle 95% area shaded. Emphasize
##that longer error bars in  N = 1000 are likely a statistical artifact of the random number generating procedure. 
exp_1 <- read_csv('data/exp_1_data.csv') %>% filter(code == 0) #load data 
exp_1 <- convert_raw_var_to_sd(raw_data = exp_1) #var to sd conversion 

parameter_names <- c(bquote(expr = bold(A:~gamma[random]~(beta[fixed]~" = 280, 5 measurements"))),
                     bquote(expr =bold(B:~gamma[random]~(beta[fixed]~" = 280, 7 measurements"))))
                     
#1) Extract estimates for four parameter gamma_fixed and beta_rand when N =500 or 1000, num_measurements == 5, spacing == equal. 
##Convert to long format, with parameter name going into unique column and estimate becoming new column 
param_data <- exp_1 %>%
  filter(number_measurements <= 7, measurement_spacing == 'time_inc', midpoint == 280) %>%
  select(locate_ivs(exp_1),'gamma_rand') %>%
  pivot_longer(cols = c(gamma_rand), values_to = 'estimate') %>%
  unite(col = 'parameter', c(number_measurements,name))

#2) Replace parameter values with tag labels. 
param_data$parameter <- factor(param_data$parameter, 
                               levels = c("5_gamma_rand", "7_gamma_rand"), 
                               labels = parameter_names)

base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

param_search_names <- c("A\\:", "B\\:")

param_error_bar_ranges <- lapply(X = param_search_names, compute_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


example_plot <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = example_plot, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_inc_exp1.pdf')

```

```{r exp1-density-plot-time-inc-fixed, include=F, eval=F}
param_error_bar_ranges <- lapply(X = param_search_names, compute_80_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


time_inc_fixed_density <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = time_inc_fixed_density, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_inc_exp1_fixed.pdf')

```

```{r text-values-time-increasing-exp1, echo=F}
beta_time_inc_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_inc_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_inc_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_inc_7_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 7, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

#### Concise Summary  

For time-interval increasing spacing, Table \ref{tab:summary-table-time-inc-exp1} provides a concise summary of the results for the day-unit parameters (see Figure \ref{fig:exp1_plot_time_inc} for the corresponding parameter estimation plots). Text within the bias and precision columns indicates the number of measurements needed to achieve either low bias or high precision across all manipulated nature of change values. Bias is low for the fixed-effect day-unit parameters (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] and fixed-effect triquarter-halfway delta parameter [$\upgamma_{fixed}$]) in all cells. For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), bias is low across all manipulated nature of change values when seven or more measurements are used. For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$), bias is low across all manipulated nature of change values when seven or more measurements are used. For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$), bias is low across all manipulated nature of change values  when nine or more measurements are used. Therefore, with time-interval increasing spacing, at least 11 measurements are needed to estimate all day unit parameters with low bias across all manipulated nature of change values. 

With respect to precision with time-interval increasing spacing, a slightly different pattern occurs than with bias. For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), precision is high across all nature of change values with seven or more measurements. For fixed-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$),   precision is high across all nature of change values with  nine or more measurements. For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] and random-effect triquarter-halfway delta parameter [$\upgamma_{random}$]), precision is low in all cells. Therefore, with time-interval increasing spacing, although nine measurements allows the fixed-effect day-unit parameters to be modelled with high precision across all manipulated nature of change values, no measurement number allows the random-effect day-unit parameters to be modelled with high precision.  

In summarizing the results with time-interval increasing spacing, low bias is achieved in the estimation of all day-unit parameters across all manipulated nature of change values with 11 measurements, but high precision is never achieved in the estimation of all day-unit parameters with any manipulated measurement number. Although it may be discouraging that 11 measurements separated over time with time-interval increasing cannot achieve high precision in the estimation of all day-unit parameters, the improvements in bias and precision in the estimation of each parameter diminish after the number of measurements increases beyond a certain value. With time-interval increasing spacing, the improvements in bias and precision across all day-unit parameters diminish after nine measurements. 

```{r summary-table-time-inc-exp1, echo=F}

summary_table <- data.frame('Parameter' = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_inc}A)',
                                                  '$\\gamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_inc}B)', 
                                                  '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_inc}C)', 
                                                  '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_inc}D)'), 
                            'Low Bias' = c('All cells', 
                                       'All cells', 
                                       '7+ measurements', 
                                       '9+ measurements'), 
                            'High Precision' = c('7+ measurements', 
                                            '9+ measurements', 
                                            'No cells',
                                            'No cells'), 
                            'Qualitative Summary' = c('Diminishing improvement in precision with 9+ measurements', 
                                                         'Diminishing improvement in precision with 9+ measurements', 
                                                         'Diminishing improvement in precision with 9+ measurements', 
                                                         'Diminishing improvements in bias and precision with 9+ measurements'), 
                            
                            check.names = F)

kbl(x = summary_table, format = 'latex',
       longtable = T, booktabs = T, centering = T, escape = F,
       align = c('l', rep('l', times = ncol(summary_table) - 1)), 
    caption = 'Concise Summary of Results for Time-Interval Increasing Spacing in Experiment 1') %>%
   #header
   column_spec(column = 2, width = '5cm') %>%
  column_spec(column = 3, width = '5cm') %>%
  column_spec(column = 4, width = '9.5cm') %>%

  #column_spec(column = 3, width = '4cm') %>%
  #footnotes
  footnote(escape = F, threeparttable = T, general_title = '\\\\textit{Note.}\\\\hspace{-1.25pc}',
           general = '$\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter, 
           $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, and 
           $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter.') %>% 
    kable_styling(position = 'left') %>%
  landscape(margin = '1cm')

```


#### Detailed Report 

For time-interval increasing spacing, Figure \ref{fig:exp1_plot_time_inc} shows the parameter estimation plots for the day-unit parameters. Error bars represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter (with population values of $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, and $\upgamma_{random}$ = 4.00), and gray bands specify the acceptable margins of error (i.e., 10% of the parameter's population value). I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. Note that Table \ref{tab:omega-exp1-equal} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under time-interval increasing spacing.  

##### Bias 

For all simulations pertaining to time-interval increasing spacing (see Figure \ref{fig:exp1_plot_time_inc}), bias only occurred in the estimation of the random-effect parameters. The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error bands for the random-effect parameters in Figures \ref{fig:exp1_plot_equal}C--D were difficult to see because 10% of the parameters' population values yielded small values relative to the range of the y-axis (i.e., 10% of 4.00 is 0.40 and 10% of 10 is 1.00, which are both small relative to the y-axis range of 55). 

For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:exp1_plot_time_inc}C), dots fell outside the gray 10% margin of error band at a population value of 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis) on the five-measurements line. Therefore, using time-interval increasing spacing, low bias can be achieved in the estimation of the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$) by using seven measurements. Therefore, using time-interval increasing spacing, seven or more measurements are needed to estimate the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$) with low bias across all manipulated nature of change values.

For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_inc}D), dots fell outside the gray 10% margin of error band at population value of 80, 180, and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis) on the five-measurements line (square dots). For the seven-measurements line (circular dots), dots fell outside the gray 10% margin of error band at a population values of 180 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, using time-interval increasing spacing, nine or more measurements are needed to model the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$) with low bias across all manipulated nature of change values. 

In summary, with equal spacing, low bias is achieved in the estimation of all day-unit parameters across all manipulated nature of change values with nine measurements. 

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Increasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_inc}
  \includegraphics{Figures/exp1_plot_days_time-interval increasing_mod} \hfill{}
\end{figure}
\begin{figure}[H]
\caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```


##### Precision

For all simulations pertaining to time-interval increasing spacing (see Figure \ref{fig:exp1_plot_time_inc}), instances of low precision occurred in the estimation of all parameters. I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision are coloured in light blue. 

For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp1_plot_time_inc}), error bar lengths are longer than the 20% acceptable amount with five measurements and a population value of 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, with time-interval increasing spacing, seven or more measurements are needed to estimate the  fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$) with high precision across all manipulated nature of change values.

For the fixed-effect triquarter-halfway elevation parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_time_inc}B), error bar lengths are longer than the 20% tolerated amount with five and seven measurements at population values of 80, 180, and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, with time-interval increasing spacing, nine or more measurements are needed to estimate the fixed-effect triquarter-halfway elevation parameter ($\upgamma_{fixed}$) with high precision across all manipulated nature of change values.

For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] in Figure \ref{fig:exp1_plot_time_inc}C and the random-effect triquarter-halfway parameter [$\upgamma_{random}$] in Figure \ref{fig:exp1_plot_time_inc}D), error bar lengths were longer than the acceptable 20% amount in all cells. Therefore, with time-interval increasing spacing, precision remains low for the estimation of the random-effect day-unit parameters across all measurement number values. 

In summary, with time-interval increasing spacing, high precision is achieved in the estimation of the fixed-effect day-unit parameters across all manipulated nature of change values with nine measurements, but no manipulated measurement number achieves high precision in the estimation of the random-effect day-unit parameters. 

One additional minor point should be mentioned. In Figure \ref{fig:exp1_plot_time_inc}D when the population value set for the days-to-halfway elevation parameter ($\upbeta_{fixed}$) is 80 (within the dashed rectangle), error bar lengths counterintuitively increase in the estimation of the the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$) as the number of measurements increases from five to seven. Inspection of the underlying density plots for the random-effect triquarter-halfway delta parameter in Figure \ref{fig:exp1_density_time_increasing}C shows that the increase in error bar length is not due to outliers and, rather, occurs because of a slight increase in the variability of estimated values estimates (see the shaded portion of the density distributions).

```{=tex}
\begin{figure} [H]
  \caption{Density Plots of the Random-Effect Halfway-Triquarter Delta ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_inc}D) With Time-Interval Increasing Spacing in Experiment 1 (95\% Error Bars)}
  \label{fig:exp1_density_time_increasing}
  \includegraphics[height = 15cm, width = 25cm]{Figures/density_plots_time_inc_exp1} \hfill{}
  \caption*{Note. \textup{Regions shaded in in gray represent the the middle 95\% of estimated values and the width of the shaded regions is indicated by the length of the horizontal error bars. The error bar length increases as the number of measurements increases from five to seven.}}
\end{figure}
```

##### Partial $\upomega^2$ Effect Sizes

Table \ref{tab:omega-exp1-time-inc} lists the partial $\upomega^2$ effect size values for each effect with each day-unit parameter when time-interval increasing spacing is used. Each effect accounts for more variability in parameter estimation accuracy for the fixed-effect day-unit parameters (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$]) and the fixed-effect triquarter-halfway delta parameter [$\upgamma_{fixed}$]) than the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$]) and the random-effect triquarter-halfway delta parameter [$\upgamma_{random}$]). 

```{r omega-exp1-time-inc, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'time_inc', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'NC', 'NM x NC'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Decreasing Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), NC = nature of change (population value set for $\\\\upbeta_{fixed}$ [i.e.,80, 180, 280]), NM x NC = interaction between number of measurements and nature of change, $\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter, 
           $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, and 
           $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_inc}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_inc}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_inc}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_inc}D)'))
```


##### Qualitative Summary 

In looking at the results for time-interval increasing spacing in Figure \ref{fig:exp1_plot_time_inc}, bias decreases the most across all the manipulated nature of change values in the estimation of the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$) when the number of measurements goes from five to seven. For the random-effect random-effect triquarter-halfway delta parameter ($\upgamma_{random}$), bias decreases the most across all measurement number values when the number of measurements goes from five to nine. With respect to precision with time-interval increasing spacing,a pattern similar to that with bias occurs in the estimation of all the day-unit parameters: The largest improvements in precision across all manipulated nature of change values occur when the number of measurements increases from five to nine. In summary, for time-interval increasing spacing, even though no manipulated measurement number achieves high precision for the random-effect day-unit parameters, the largest improvements in precision---and bias---across all manipulated nature of change values for all day-unit parameters are achieved with nine measurements.


### Time-Interval Decreasing Spacing
#### Concise Summary  
 
For time-interval decreasing spacing, Table \ref{tab:summary-table-time-dec-exp1} provides a concise summary of the results for the day-unit parameters (see Figure \ref{fig:exp1_plot_time_decreasing} for the corresponding parameter estimation plots). Text within the bias and precision columns indicates the number of measurements needed to achieve either low bias or high precision across all manipulated nature of change values. Bias is low for the fixed-effect day-unit parameters (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] and fixed-effect triquarter-halfway delta parameter [$\upgamma_{fixed}$]) in all cells. For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), bias is low across all nature of change values when seven or more measurements are used. For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), bias is low across across all nature of change values when eleven measurements are used. Therefore, with time-interval decreasing spacing, at least 11 measurements are needed to estimate all day unit parameters with low bias across all manipulated nature of change values. 

With respect to precision with time-interval decreasing spacing, a slightly different pattern occurs than with bias. For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), precision is high across all manipulated nature of change values when seven or more measurements are used and high for the fixed-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$) when nine or more measurements are used. For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] and random-effect triquarter-halfway delta parameter [$\upgamma_{random}$]), precision is low in all cells. Therefore, although nine measurements allows the fixed-effect day-unit parameters to be modelled with high precision across all manipulated nature of change values, no measurement number allows the random-effect day-unit parameters to be modelled with high precision. 

In summarizing the results with time-interval decreasing spacing, low bias is achieved in the estimation of all day-unit parameters across all manipulated nature of change values with 11 measurements, but high precision is never achieved in the estimation of all day-unit parameters with any manipulated measurement number. Although it may be discouraging that 11 measurements separated over time with time-interval decreasing cannot achieve high precision in the estimation of all day-unit parameters, the improvements in bias and precision in the estimation of each parameter diminish after the number of measurements increase beyond a certain value. With time-interval decreasing spacing, the improvements in bias and precision across all day-unit parameters diminish after nine measurements. 


```{r summary-table-time-dec-exp1, echo=F}

summary_table <- data.frame('Parameter' = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_decreasing}A)',
                                                  '$\\gamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_decreasing}B)', 
                                                  '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_decreasing}C)', 
                                                  '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_decreasing}D)'), 
                            'Low Bias' = c('All cells', 
                                           'All cells', 
                                           '7+ measurements', 
                                           '9+ measurements'), 
                            'High Precision' = c('7+ measurements', 
                                            '9+ measurements', 
                                            'No cells',
                                            'No cells'), 
                            'Qualitative Summary' = c('Diminishing improvement in precision with 7+ measurements', 
                                                         'Diminishing improvement in precision with 9+ measurements', 
                                                         'Diminishing improvements in bias and precision with 7+ measurements', 
                                                         'Diminishing improvements in bias and precision with 9+ measurements'), 
                            
                            check.names = F)

kbl(x = summary_table, format = 'latex',
       longtable = T, booktabs = T, centering = T, escape = F,
       align = c('l', rep('l', times = ncol(summary_table) - 1)), 
    caption = 'Concise Summary of Results for Time-Interval Decreasing Spacing in Experiment 1') %>%
   #header
  column_spec(column = 2, width = '5cm') %>%
  column_spec(column = 3, width = '5cm') %>%
  column_spec(column = 4, width = '9.5cm') %>%

  #column_spec(column = 3, width = '4cm') %>%
  #footnotes
  footnote(escape = F, threeparttable = T, general_title = '\\\\textit{Note.}\\\\hspace{-1.25pc}',
           general = '$\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter, 
           $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, and 
           $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter.') %>% 
    kable_styling(position = 'left') %>%
  landscape(margin = '1cm')

```

#### Detailed Report

```{r plots-time-decreasing-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Time-interval decreasing',
                                x_axis_name = expression("Population Value Set for"~beta[fixed]),
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -60, beta_upper = 60,  beta_ticks = 10)
```

```{r exp1-density-plot-time-dec, include=F, eval=F}
##generate figure showing density distributions if gamma_fizxd at N = 500 and N = 1000 with middle 95% area shaded. Emphasize
##that longer error bars in  N = 1000 are likely a statistical artifact of the random number generating procedure. 
exp_1 <- read_csv('data/exp_1_data.csv') %>% filter(code == 0) #load data 
exp_1 <- convert_raw_var_to_sd(raw_data = exp_1) #var to sd conversion 

parameter_names <- c(bquote(expr = bold(A:~gamma[random]~(beta[fixed]~" = 80, 5 measurements"))),
                     bquote(expr =bold(B:~gamma[random]~(beta[fixed]~" = 80, 7 measurements"))))
                     
#1) Extract estimates for four parameter gamma_fixed and beta_rand when N =500 or 1000, num_measurements == 5, spacing == equal. 
##Convert to long format, with parameter name going into unique column and estimate becoming new column 
param_data <- exp_1 %>%
  filter(number_measurements <= 7, measurement_spacing == 'time_dec', midpoint == 80) %>%
  select(locate_ivs(exp_1),'gamma_rand') %>%
  pivot_longer(cols = c(gamma_rand), values_to = 'estimate') %>%
  unite(col = 'parameter', c(number_measurements,name))

#2) Replace parameter values with tag labels. 
param_data$parameter <- factor(param_data$parameter, 
                               levels = c("5_gamma_rand", "7_gamma_rand"), 
                               labels = parameter_names)

base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3, alpha = 0) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

param_search_names <- c("A\\:", "B\\:")

param_error_bar_ranges <- lapply(X = param_search_names, compute_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


density_time_dec_exp_1 <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = density_time_dec_exp_1, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_dec_exp1.pdf')

```

```{r exp1-density-plot-time-dec-fixed, include=F, eval=F}
param_error_bar_ranges <- lapply(X = param_search_names, compute_80_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


time_dec_fixed_density <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = time_dec_fixed_density, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_dec_exp1_fixed.pdf')

```

```{r text-values-time-decreasing-exp1, echo=F}
beta_time_dec_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_dec_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_dec_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_dec_7_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 7, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

Figure \ref{fig:exp1_plot_time_decreasing} shows the parameter estimation plots
for the day-unit parameters when time-interval decreasing was used. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the tolerable amount of bias and precision was based on a population value of 180. Note that Table \ref{tab:omega-exp1-time-dec} provide the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing. 

##### Bias 

For all simulations pertaining to time-interval decreasing spacing (see Figure \ref{fig:exp1_plot_time_decreasing}), bias only occurred in the estimation of the random-effect parameters. The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effects parameters in Figures \ref{fig:exp1_plot_time_decreasing}C--D were difficult to see because 10% of the parameters' population values yielded small values relativeto the scale of the y-axis (i.e., 10% of 4.00 is 0.40 and 10% of 10 is 1.00, which are small relative to the y-axis range of 55). For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:exp1_plot_time_decreasing}C), dots fell outside the gray 10% margin of error band at a population value of 80 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis) on the five-measurements line (squares). For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_decreasing}D), dots fell outside the gray 10% margin of error band at population value of 80, 180, and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis) on the five-measurements line (squares). For the seven-measurements line (circles), dots fell outside the gray 10% margin of error band at a population values of 80 and 180 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the nine-measurements line (triangles), dots fell outside the gray 10% margin of error band at a population values of 180 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the eleven-measurements line (diamonds), all dots fell within the 10% margin of error band. Therefore, eleven measurements are needed to achieve low-bias estimation for all day-unit parameters. 

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Decreasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_decreasing}
  \includegraphics{Figures/exp1_plot_days_time-interval decreasing_mod} \hfill{}
\end{figure}
\begin{figure}[H]
\caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

##### Precision

For all simulations pertaining to time-interval decreasing spacing (see Figure \ref{fig:exp1_plot_time_decreasing}), instances of low precision occurred in the estimation of all parameters. I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp1_plot_time_decreasing}A), error bar lengths were longer than the 20% tolerated amount with five measurements and a population value of 80 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the fixed-effect triquarter-halfway elevation parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_time_decreasing}B), error bar lengths were longer than the 20% tolerated amount with five and seven measurements at population values of 80, 180, and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). For the random-effects day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] in Figure \ref{fig:exp1_plot_time_decreasing}C, random-effect halfway-triquarter delta parameter [$\upbeta_{random}$] in Figure \ref{fig:exp1_plot_time_decreasing}D), error bar lengths were longer than the tolerated 20% amount in all cells. Therefore, nine or more measurements are needed to achieve high precision for the fixed-effect day-unit parameters and precision remains low for the random-effect day-unit parameters for all measurement number values. 

One additional minor point should be mentioned. In Figure
\ref{fig:exp1_density_time_decreasing}D when the population value set for the days-to-halfway elevation parameter ($\upbeta_{fixed}$) is 80 (within the dashed rectangle), error bar lengths counterintuitively increase in the estimation of the the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$) as the number of measurements increases from five to seven. Inspection of the underlying density plots for the random-effect triquarter-halfway delta parameter in Figure \ref{fig:exp1_density_time_decreasing}C shows that the increase in error bar length is not due to outliers and, rather, occurs because of a slight increase in the variability of estimated values estimates (see the shaded portion of the density distributions).

```{=tex}
\begin{figure} [H]
  \caption{Density Plots of the Random-Effect Halfway-Triquarter Delta ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_decreasing}D) With Time-Interval Decreasing Spacing in Experiment 1 (95\% Error Bars)}
  \label{fig:exp1_density_time_decreasing}
  \includegraphics[height = 15cm, width = 30cm]{Figures/density_plots_time_dec_exp1} \hfill{}
  \caption*{Note. \textup{Regions shaded in in gray represent the the middle 95\% of estimated values and the width of the shaded regions is indicated by the length of the horizontal error bars. The error bar length increases as the number of measurements increases from five to seven.}}
\end{figure}
```


##### Partial $\upomega^2$ Effect Sizes

Table \ref{tab:omega-exp1-time-dec} lists the partial $\upomega^2$ effect size values for each effect with each day-unit parameter when time-interval decreasing spacing is used. Each effect accounts for more variability in parameter estimation accuracy for the fixed-effect day-unit parameters (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$]) and the fixed-effect triquarter-halfway delta parameter [$\upgamma_{fixed}$]) than the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$]) and the random-effect triquarter-halfway delta parameter [$\upgamma_{random}$]). 

```{r omega-exp1-time-dec, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'time_dec', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'NC', 'NM x NC'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Decreasing Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), NC = nature of change (population value set for $\\\\upbeta_{fixed}$ [i.e.,80, 180, 280]), NM x NC = interaction between number of measurements and nature of change. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_inc}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_inc}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_inc}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_inc}D)'))
```
##### Qualitative Assessment

In looking at the results for time-interval decreasing spacing in Figure \ref{fig:exp1_plot_time_decreasing}, bias decreases the most across all the manipulated nature of change values in the estimation of the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$) when the number of measurements goes from five to seven. For the random-effect random-effect triquarter-halfway delta parameter ($\upgamma_{random}$), bias decreases the most across all measurement number values when the number of measurements goes from five to nine. With respect to precision with time-interval decreasing spacing, a pattern similar to that with bias occurs in the estimation of all the day-unit parameters: The largest improvements in precision across all manipulated nature of change values occur when the number of measurements increases from five to nine. In summary, for time-interval decreasing spacing, even though no manipulated measurement number achieves high precision for the random-effect day-unit parameters, the largest improvements in precision---and bias---across all manipulated nature of change values for all day-unit parameters are achieved with nine measurements.



### Middle-and-Extreme Spacing

#### Concise Summary 

For middle-and-extreme spacing, Table \ref{tab:summary-table-equal-spacing-exp1} provides a concise summary of the results for the day-unit parameters (see Figure \ref{fig:exp1_plot_time_decreasing} for the corresponding parameter estimation plots). Text within the bias and precision columns indicates the number of measurements needed to achieve either low bias or high precision across all manipulated nature of change values. Bias is low for the fixed-effect day-unit parameters (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] and fixed-effect triquarter-halfway delta parameter [$\upgamma_{fixed}$]) in all cells. For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), bias is low across all nature of change values when seven or more measurements are used. For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), bias is low across across all nature of change values when eleven measurements are used. Therefore, with middle-and-extreme spacing, at least 11 measurements are needed to estimate all day unit parameters with low bias across all manipulated nature of change values. 

With respect to precision with middle-and-extreme spacing, a slightly different pattern occurs than with bias. For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), precision is high across all manipulated nature of change values with seven or more measurements. For the fixed-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$),  precision is high across all manipulated nature of change values with nine or more measurements. For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{random}$] and random-effect triquarter-halfway delta parameter [$\upgamma_{random}$]), precision is low in all cells. Therefore, with middle-and-extreme spacing, although nine measurements allows the fixed-effect day-unit parameters to be modelled with high precision across all manipulated nature of change values, no measurement number allows the random-effect day-unit parameters to be modelled with high precision. 

In summarizing the results with middle-and-extreme spacing, low bias is achieved in the estimation of all day-unit parameters across all manipulated nature of change values with at least 11 measurements, but high precision is never achieved in the estimation of all day-unit parameters with any manipulated measurement number. Although it may be discouraging that 11 measurements separated over time with middle-and-extreme spacing cannot achieve high precision in the estimation of all day-unit parameters, the improvements in bias and precision in the estimation of each parameter diminish after the number of measurements increase beyond a certain value. With middle-and-extreme spacing, the improvements in bias and precision across all day-unit parameters diminish after nine measurements. 


```{r summary-table-mid-ext, echo=F}

summary_table <- data.frame('Parameter' = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}A)',
                                                  '$\\gamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}B)', 
                                                  '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}C)', 
                                                  '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}D)'), 
                            'Bias' = c('All cells', 
                                       '7+ measurements',
                                       '9+ measurements', 
                                       '11+ measurements'), 
                            'Precision' = c('7+ measurements', 
                                            '11+ measurements', 
                                            'No cells',
                                            'No cells'), 
                            'Qualitative Summary' = c('Diminishing improvement in precision with 7+ measurements', 
                                                      'Diminishing improvements in bias and precision with 7+ measurements', 
                                                      'Diminishing improvements in bias and precision with 9+ measurements', 
                                                      'Diminishing improvements in bias and precision with 7+ measurements'), 
                            
                            check.names = F)

kbl(x = summary_table, format = 'latex',
       longtable = T, booktabs = T, centering = T, escape = F,
       align = c('l', rep('l', times = ncol(summary_table) - 1)), 
    caption = 'Concise Summary of Results for Middle-and-Extreme Spacing in Experiment 1') %>%
   #header
    column_spec(column = 2, width = '5cm') %>%
  column_spec(column = 3, width = '5cm') %>%
  column_spec(column = 4, width = '9.5cm') %>%

  #column_spec(column = 3, width = '4cm') %>%
  #footnotes
  footnote(escape = F, threeparttable = T, general_title = '\\\\textit{Note.}\\\\hspace{-1.25pc}',
           general = '$\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter, 
           $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, and 
           $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter.') %>% 
    kable_styling(position = 'left') %>%
  landscape(margin = '1cm')

```

```{r plots-mid-ext-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Middle-and-extreme spacing',
                                x_axis_name = expression("Population Value Set for"~beta[fixed]), 
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -80, beta_upper = 80, beta_ticks = 20)
```

```{r text-values-mid-ext-exp1, echo=F}
beta_mid_ext_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))

beta_mid_ext_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_mid_ext_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))


beta_mid_ext_7_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 7, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))


beta__rand_mid_ext_7_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 7, midpoint == 280, grepl('beta\\[random\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta__rand_mid_ext_7_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 7, midpoint == 80, grepl('beta\\[random\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

#### Detailed Report

Figure \ref{fig:exp1_plot_time_mid_ext} shows the parameter estimation plots
for the day-unit parameters when middle-and-extreme spacing was used. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the tolerable amount of bias and precision was based on a population value of 180. Note that Table \ref{tab:omega-exp1-mid-ext} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing.


##### Bias 

For all simulations pertaining to middle-and-extreme spacing (see Figure \ref{fig:exp1_plot_time_mid_ext}), bias  occurred in the estimation of all day-unit parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp1_plot_time_mid_ext}A). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effects parameters in Figures \ref{fig:exp1_plot_time_decreasing}C--D were difficult to see because 10% of the parameters' population values yielded small values relative to the range of the y-axis (e.g., 10% of 4.00 is 0.40 and 10% of 10 is 1.00, which are small values relative to the y-axis range of 55). In the paragraphs that follow...

For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_time_mid_ext}B), bias occurred on the five-measurements line (squares) at population values of 80 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, when using middle-and-extreme spacing, seven or more measurement are needed to to estimate the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$) with low bias across the three nature of change values I manipulated.

For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:exp1_plot_time_mid_ext}C), bias occurred on the five- and seven-measurements lines (squares and circles, respectively) at population values of 80 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, when using middle-and-extreme spacing, nine or more measurements are needed to estimate the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$) with low bias across the three nature of change values I manipulated.

For the random-effect halfway-triquarter parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_mid_ext}D), bias occurred on the five-, seven-, and nine-measurements lines (squares, circles, and triangles, respectively) at population values of 80 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, to estimate the andom-effect halfway-triquarter parameter ($\upgamma_{random}$) with low bias across all nature of change values, eleven or more measurements are needed. 

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Middle-and-Extreme Spacing in Experiment 1}
  \label{fig:exp1_plot_time_mid_ext}
  \includegraphics{Figures/exp1_plot_days_middle-and-extreme spacing} \hfill{}
\end{figure}
\begin{figure}[H]
 \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```


##### Precision

For all simulations pertaining to middle-and-extreme spacing (see Figure \ref{fig:exp1_plot_time_mid_ext}), instances of low precision occurred in the estimation of all parameters. I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the fixed-effect days-to-halfway elevation parameter ($\upbeta{fixed}$; Figure \ref{fig:exp1_plot_time_mid_ext}A), error bar lengths were longer than the 20% tolerated amount with five measurements at population values of 80 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, seven or more measurements are needed to estimate the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$) with high precision across all nature of change values. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_time_mid_ext}B), error bar lengths were longer than the 20% tolerated amount with five, seven, and nine measurements at population values of 80 and 280 for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; shown on x-axis). Therefore, 11 measurements are needed to estimate the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$) with high precision. For the random-effect day-unit parameters (random-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] in Figure \ref{fig:exp1_plot_time_mid_ext}C and the random-effect triquarter-halfway parameter [$\upgamma_{fixed}$] in Figure \ref{fig:exp1_plot_time_mid_ext}D), error bar lengths were longer than the 20% tolerated amount in all the cells. Therefore, the random-effect day-unit parameters cannot be estimated with high precision with any measurement number value of 11 and below. 

##### Partial $\upomega^2$ Effect Sizes


```{r omega-exp1-mid-ext, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'mid_ext', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'NC', 'NM x NC'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Middle-and-Extreme Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), NC = nature of change (population value set for $\\\\upbeta_{fixed}$ [i.e.,80, 180, 280]), NM x NC = interaction between number of measurements and nature of change. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}D)'))
```


##### Qualitative Assessment 

In looking at the results for middle-and-extreme spacing in Figure \ref{fig:exp1_plot_time_mid_ext}, bias decreases the most for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively) across all the manipulated nature of change values when the number of measurements increases from five to nine. For the fixed- and random-effect triquarter-halfway delta parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively), bias decreases the most across all the manipulated nature of change values when the number of measurements increases from five to seven. With respect to precision, a pattern similar to that with bias occurs. For all day-unit parameters except the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), the largest improvements in precision across all manipulated nature of change values occur when the number of measurements increases from five to seven. For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$), the largest improvement in precision across all manipulated nature of change values occurs when the number of measurements increases from five to nine. In summary, for middle-and-extreme spacing, even though no manipulated measurement number achieves high precision for the random-effect day-unit parameters, the largest improvements in precision---and bias---across all manipulated nature of change values for all day-unit parameters are achieved with nine measurements.


### Comparison of Measurement Spacing Schedules 

```{r plot-summary-exp1, include=F, eval=F}
cond_summary_exp_1 <- compute_condition_summary(param_summary_data = combined_analytical_exp_1, facet_var = 'measurement_spacing', 
                          ind_vars = c('number_measurements', 'measurement_spacing', 'midpoint'))


#bias plot
generate_summary_facet_plot(condition_data = cond_summary_exp_1, lower_y_limit = 0, upper_y_limit = 70, ticks = 5, exp_num = 'Figures/exp1_bias_',
                            y_axis_var = 'mean_perc_error', 
                            x_axis_var = 'midpoint', x_axis_name = expression("Population Value Set for"~beta[fixed]))

#variability plot 
generate_summary_facet_plot(condition_data = cond_summary_exp_1, lower_y_limit = 0, upper_y_limit = 25, ticks = 5, exp_num = 'Figures/exp1_sd_',
                            y_axis_var = 'mean_sd', y_axis_name = 'Average Parameter Standard Deviation',
                            x_axis_var = 'midpoint', x_axis_name = expression("Population Value Set for"~beta[fixed]))
```






```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# Experiment 2

```{=tex}
\nointerlineskip
\vfill
\newpage
```

Experiment 2 investigated how the modelling accuracy of a nonlinear pattern was affected under conditions characterized by different measurement spacing schedules, measurement numbers, and sample sizes (see Table \ref{tab:experimentOverview}). Convergence success rate was computed for each cell and percent bias was computed for each parameter in each cell. Variables held constant were the nature of change (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] = 180), the distribution of errors over time (independent and identically distributed), and absence of missing data. 

## Methods
### Variables Used in Simulation Experiment 

#### Independent Variables
##### Number of Measurements

I used the following values for the number of measurements: 5, 7, 9, and 11 (see [number of measurements](#number-measurements)} for more discussion)).  


##### Spacing of Measurements

I used the following values for measurement spacing: equal, time-interval increasing, time-interval decreasing, and middle-and-extreme spacing (see [spacing of measurements](#spacing-measurements) for more discussion). 

##### Sample Size (#sample-size)

Sample size values were borrowed from @coulombe2016 with one difference.
Because my experiments investigated the effects of measurement timing
factors on the ability to model nonlinear patterns, which are inherently
more complex than linear patterns of change, a sample size value of *N*
= 1000 was added as the largest sample size. Therefore, the following
values were used for my sample size manipulation: 30, 50, 100, 200, 500,
and 1000 (see Table \ref{tab:myValues}). Importantly, in experiments where
sample size was not manipulated (i.e., Experiment 1), the sample size
value used corresponded to the average sample size used in
organizational research [*n* = 225\; @bosco2015].

#### Constants 

Because the nature of change not manipulated in Experiment 2, I set it to have a constant value across all cells. To keep the nature of change constant across all cells, I set the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$) to have a value of 180. Another variable set to a constant value across the cells was time structuredness (data were assumed to be time structured). 

#### Dependent Variables

##### Convergence Success Rate

The proportion of iterations in a cell where models converged defined
the **convergence success rate**.\footnote{Specifically, convergence was obtained if the convergence code returned by OpenMx was 0.} Equation \@ref(eq:convergence) below shows the calculation used to compute the convergence success rate:

```{=tex}
\begin{align}
  \text{Convergence success rate} =  \frac{\text{Number of models that successfully converged}}{n},
  (\#eq:convergence) 
\end{align}
```
\noindent where *n* represents the total number of models run in a cell.


##### Bias

Bias was calculated to evaluate the accuracy with which each logistic function parameter was estimated. As shown below in Equation \@ref(eq:bias), *bias* was obtained by calculating the difference
between the population value set for a parameter and the average
estimated value in each cell.

\useshortskip
```{=tex}
\begin{align}
  \text{Bias} =  \text{Population value for parameter} - \text{Average estimated value}
  (\#eq:bias) 
\end{align}
```

##### Precision

In addition to computing bias, precision was calculated to evaluate the confidence with which each parameter was estimated in a given cell. *Precision* was obtained by identifying the range of values covered by the middle 95% of values estimated for a logistic parameter in each cell. By using the middle 95% of estimated values, a plausible range of population estimates was obtained.   

## Overview of Data Generation and Analysis of Dependent Variables 

Data generation and analysis of the dependent variables was the same as in Experiment 1 (see [data generation and analysis](#data-generation-analysis) for discussion).  

## Results

In the section that follow, I will describe the results of the simulation experiment by each spacing condition (equal, time-interval increasing, time-interval decreasing, middle-and-extreme). Each section will contain a set of parameter estimation plots (one for each logistic function parameter) and a table of partial $\upomega^2$ values. After describing the results for each spacing condition, I will provide a summary of the results. 

### Equal Spacing

```{r plots-equal-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Equal spacing',
                               x_axis_name = expression("Sample Size ("*italic(N)*")"), 
                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 160, beta_upper = 210, beta_ticks = 5)
                                
```

```{r exp2-density-plot-equal, include=F, eval=F}
##generate figure showing density distributions if gamma_fizxd at N = 500 and N = 1000 with middle 95% area shaded. Emphasize
##that longer error bars in  N = 1000 are likely a statistical artifact of the random number generating procedure. 
exp_2 <- read_csv('data/exp_2.csv') %>% filter(code == 0) #load data 
exp_2 <- convert_raw_var_to_sd(raw_data = exp_2) #var to sd conversion 

parameter_names <- c(bquote(expr = bold(A:~gamma[fixed]~"(N=500, 5 measurements)")),
                     bquote(expr = bold(B:~beta[random]~"(N=500, 5 measurements)")),
                     bquote(expr = bold(C:~gamma[fixed]~"(N=1000, 5 measurements)")),
                     bquote(expr = bold(D:~beta[random]~"(N=1000, 5 measurements)")))
                     
#1) Extract estimates for four parameter gamma_fixed and beta_rand when N =500 or 1000, num_measurements == 5, spacing == equal. 
##Convert to long format, with parameter name going into unique column and estimate becoming new column 
param_data_equal_exp_2 <- exp_2 %>%
  filter(number_measurements == 5, measurement_spacing == 'equal', sample_size %in% c(500, 1000)) %>%
  select(locate_ivs(exp_2),'gamma_fixed', 'beta_rand') %>%
  pivot_longer(cols = c(gamma_fixed, beta_rand), values_to = 'estimate') %>%
  unite(col = 'parameter', sample_size:name) 

#2) Replace parameter values with tag labels. 
param_data_equal_exp_2$parameter <- factor(param_data_equal_exp_2$parameter, 
                               levels = c("500_gamma_fixed", "500_beta_rand",
                                          "1000_gamma_fixed", "1000_beta_rand"), 
                               labels = parameter_names)

base_density_plot <- ggplot(data = param_data_equal_exp_2, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

param_search_names <- c("A\\:", "B\\:", "C\\:", "D\\:")

param_error_bar_ranges <- lapply(X = param_search_names, compute_ind_param_error_bar_range, param_data = param_data_equal_exp_2)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data_equal_exp_2, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


density_plot_equal_exp2 <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 2, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(3,scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(4, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.90, by = 0.10),
                                                                                                   limits = c(0, .90))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 60, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = density_plot_equal_exp2, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plot_fixed_equal_exp2.pdf')

```

```{r exp2-density-plot-equal-fixed, include=F, eval=F}
param_error_bar_ranges <- lapply(X = param_search_names, compute_90_ind_param_error_bar_range, param_data = param_data_equal_exp_2)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data_equal_exp_2, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


density_equal_fixed_exp2 <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
     facet_wrap_custom( ~ parameter, scales = "free", ncol = 2, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(3,scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(4, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.90, by = 0.10),
                                                                                                   limits = c(0, .90))))) + 
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = density_equal_fixed_exp2, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_equal_fixed_exp2.pdf')

```

```{r plots-equal-exp2-fixed, include=F, eval=F}
exp_2_analytical$days$upper_ci <- exp_2_analytical$days$upper_ci_90
exp_2_analytical$days$lower_ci <- exp_2_analytical$days$lower_ci_90

generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Equal spacing',
                               x_axis_name = 'Sample size (*N*)', 
                               x_axis_var = 'sample_size', exp_num = 'exp2_fixed_', beta_lower = 165, beta_upper = 205,
                               ticks = 5)
```

```{r text-values-equal-exp2, echo=F}
#fixed-effect halfway-triquarter delta (5, 100)
gamma_fixed_equal_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_equal_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter(7, 100)
gamma_fixed_equal_7_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 7, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect halfway-triquarter delta (5, 200)
gamma_rand_equal_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))
```


Figure \ref{fig:exp2_plot_equal} shows the parameter estimation plots
for the day-unit parameters when equal spacing was used. Error bars
represent the middle 95% of estimated values, blue horizontal lines
indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp2-equal} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing. 

#### Bias

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp2_plot_equal}), bias only occurred in the estimation of the fixed- and random-effect triquarter-halfway delta parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$; Figure \ref{fig:exp2_plot_equal}B and Figure \ref{fig:exp2_plot_equal}D, respectively). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D) is difficult to see because 10% of the parameter's population value is a small value (i.e., 10% of 4.00 is 0.40). For the fixed-effect halfway-triquarter delta parameter (Figure \ref{fig:fig:exp2_plot_equal}B), bias occurred with a sample size of 50 (shown on x-axis) on the five-measurements line (square dots). For the random-effect halfway-triquarter delta parameter (Figure \ref{fig:fig:exp2_plot_equal}D), bias on the five-measurements line (square dots) with a sample sizes of 30 to 1000 (shown on x-axis), the seven-measurements line (circular dots) with a sample sizes of 30 to 500, the nine-measurements line (triangle dots) with sample sizes of 30 and 50, and the eleven-measurements line (diamond shape) with sample sizes of 30 and 50. For all other cells, bias did not occur.

#### Precision

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp2_plot_equal}), instances of low precision occurred for precision was low for all parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp2_plot_equal}A). I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the random-effects days-to-halfway elevation ($\upbeta_{random}$; Figure \ref{fig:exp2_plot_equal}B) and halfway-triquarter delta parameters ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_equal}D), error bar lengths were longer than the 20% tolerated amount in all the cells. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp2_plot_equal}C), error bar lengths were longer than the 20% tolerated amount with five measurements at sample sizes of 30 to 1000 (shown on x-axis). With nine measurements, error bar lengths were longer than the 20% tolerated amount at sample sizes of 30 to 100 (shown on x-axis). With nine measurements, error bar lengths were longer than the 20% tolerated amount at sample sizes of 30 to 100 (shown on x-axis).  For all other cells in Figure \ref{fig:exp1_plot_equal}C, precision was high. 

```{r density-ranges, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 500)
beta_fixed_equal_5_500 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 500, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 500)
beta_fixed_equal_5_1000 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 1000, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Equal Spacing in Experiment 2}
  \label{fig:exp2_plot_equal}
  \includegraphics{Figures/exp2_plot_days_equal spacing} \hfill{}
\caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp2-equal, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'equal', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Equal Spacing in Experiment 2',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_equal}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_equal}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_equal}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_equal}D)'))
```

### Time-Interval Increasing Spacing

```{r plots-time-increasing-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Time-interval increasing',
                                                             x_axis_name = expression("Sample Size ("*italic(N)*")"), 

                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 160, beta_upper = 210,  beta_ticks = 5)
```


```{r text-values-time-increasing-exp2, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_time_inc_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (7, 100)
beta_fixed_time_inc_7_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 7, sample_size == 100, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp2_plot_time_increasing} shows the parameter estimation plots
for the day-unit parameters when time-interval increasing spacing was used. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp2-time-inc} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing. 

#### Bias

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp2_plot_time_increasing}), bias only occurred in the estimation of the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{exp2_plot_time_increasing}C) and the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_equal}D). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{exp2_plot_time_increasing}C) and the random-effect halfway-triquarter delta parameters were difficult to see because 10% of either parameter's population value was small (i.e., 10% of 4.00 is 0.40). For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:fig:exp2_plot_time_increasing}B), bias occurred on the five-measurements line (square dots) at sample sizes of 30 to 1000 (shown on x-axis). For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:fig:exp2_plot_time_increasing}D), bias occurred on the five-measurements line (square dots) at sample sizes of 30 to 1000 (shown on x-axis), on the seven-measurements line (circle dots) at sample sizes of 30 to 1000 (shown on x-axis), on the nine-measurements line (triangle dots) at sample sizes of 30, 50, 100, and 1000, and on the eleven-measurements line at sample size values of 30 and 50 (shown on x-axis). For all other cells, bias did not occur. 

#### Precision

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp2_plot_time_increasing}), instances of low precision occurred for all parameters. I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the fixed-effect days-to-halfway-elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp2_plot_time_increasing}A), precision was low with five measurements (square dots) at sample size values of 30 and 50. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp2_plot_time_increasing}B), precision was low with five measurements (square dots) at sample size values of 30 to 1000 (shown on x-axis), seven measurements (circle dots) at sample size values of 30 to 200 (shown on x-axis), nine measurements (triangle dots) at sample size values of 30 to 100 (shown on x-axis), and eleven measurements (diamond dots) at sample size values of 30 to 100 (shown on x-axis). For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:exp2_plot_time_increasing}C), precision was low for all cells. For the random-effect halfway-triquarter elevation parameter ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_time_increasing}D), precision was low for all cells.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Interval Increasing Spacing in Experiment 2}
  \label{fig:exp2_plot_days_time-interval increasing}
 \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-time-inc} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp2-time-inc, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'time_inc', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Increasing Spacing in Experiment 2',
footnote = 'NM = number of measurements, S = sample size, NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_time_increasing}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_time_increasing}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_time_increasing}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_time_increasing}D)'))
```

### Time-Interval Decreasing Spacing

```{r plots-time-decreasing-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Time-interval decreasing',
                                                                                             x_axis_name = expression("Sample Size ("*italic(N)*")"), 


                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 155, beta_upper = 210,
                                beta_ticks = 5)
```

```{r text-values-time-decreasing-exp2, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_time_dec_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (7, 100)
beta_fixed_time_dec_7_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 7, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp2_plot_time_decreasing} shows the parameter estimation plots
for the day-unit parameters when time-interval decreasing spacing was used. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp2-time-dec} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under time-interval decreasing spacing 

#### Bias

For all simulations pertaining to time-interval decreasing spacing (see Figure \ref{fig:exp2_plot_time_decreasing}), bias only occurred in the estimation of the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{exp2_plot_time_decreasing}C) and the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_time_decreasing}D). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{exp2_plot_time_decreasing}C) and the random-effect halfway-triquarter delta parameters were difficult to see because 10% of either parameter's population value was small (i.e., 10% of 4.00 is 0.40). For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:fig:exp2_plot_time_decreasing}B), bias occurred on the five-measurements line (square dots) at a sample size of 30 (shown on x-axis). For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:fig:exp2_plot_time_decreasing}D), bias occurred on the five-measurements line (square dots) at sample sizes of 30 to 1000 (shown on x-axis), on the seven-measurements line (circle dots) at sample sizes of 30 to 1000 (shown on x-axis), on the nine-measurements line (triangle dots) at sample sizes of 30, 50, and 100, and on the eleven-measurements line at sample size values of 30 and 50 (shown on x-axis). For all other cells, bias did not occur. 

#### Precision

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp2_plot_time_decreasing }), instances of low precision occurred for all parameters. I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the fixed-effect days-to-halfway-elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp2_plot_time_increasing}A), precision was low with five measurements (square dots) at sample size values of 30 and 50. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp2_plot_time_increasing}B), precision was low with five measurements (square dots) at sample size values of 30 to 1000 (shown on x-axis), seven measurements (circle dots) at sample size values of 30 to 200 (shown on x-axis), nine measurements (triangle dots) at sample size values of 30 to 100 (shown on x-axis), and eleven measurements (diamond dots) at sample size values of 30 to 100 (shown on x-axis). For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:exp2_plot_time_increasing}C), precision was low for all cells. For the random-effect halfway-triquarter elevation parameter ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_time_increasing}D), precision was low for all cells.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Interval Decreasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_decreasing}
  \includegraphics{Figures/exp2_plot_days_time-interval decreasing} \hfill{}
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-time-dec} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp2-time-dec, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'time_dec', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Decreasing Spacing in Experiment 2',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_time_decreasing}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_time_decreasing}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_time_decreasing}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_time_decreasing}D)'))
```

### Middle-and-Extreme Spacing

```{r plots-mid-ext-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Middle-and-extreme spacing',
                                                             x_axis_name = expression("Sample Size ("*italic(N)*")"), 

                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 160, beta_upper = 210,
                                beta_ticks = 5)
```

```{r text-values-mid-ext-exp2, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 30)
beta_fixed_mid_ext_5_30 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, sample_size == 30, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```


Figure \ref{fig:exp2_plot_time_mid_ext} shows the parameter estimation plots
for the day-unit parameters when middle-and-extreme spacing was used. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp2-mid-ext} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under middle-and-extreme spacing 

#### Bias

For all simulations pertaining to middle-and-extreme spacing (see Figure \ref{fig:exp2_plot_time_mid_ext}), bias only occurred in the estimation of the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{exp2_plot_time_decreasing}C). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect halfway-triquarter delta parameter was difficult to see because 10% of its population value was small value relative to the range of the y-axis (i.e., 10% of 4.00 is 0.40, which is small compared to the y-axis range of 55). For the five-measurements line (square dots), bias occurred at a sample size values of 30, 50, and 100 (shown on x-axis). For the seven-measurements line (circle dots), bias occurred at a sample size values of 30 and 50(shown on x-axis). For the nine-measurements line (triangle dots), bias occurred at a sample size values of 30, 50, and 100 (shown on x-axis). For all other cells, bias did not occur. 

#### Precision

For all simulations pertaining to middle-and-extreme spacing spacing (see Figure \ref{fig:exp2_plot_time_mid_ext}), instances of low precision occurred for all parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp2_plot_time_mid_ext}A). I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the random-effect days-to-halfway elevation ($upbeta_{random}$; Figure \ref{fig:exp2_plot_time_mid_ext}C) and the random-effect halfway-triquarter delta parameters ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_time_increasing}D), precision was low for all the cells. For the fixed-effect halfway-triquarter elevation parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp2_plot_time_mid_ext}B), precision was low across all measurement number values at sample sizes of 30, 50, and 100 (shown on x-axis). Precision was high for all other cells. 

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Middle-and-Extreme Spacing in Experiment 2}
  \label{fig:exp2_plot_time_mid_ext}
  \includegraphics{Figures/exp2_plot_days_middle-and-extreme spacing} \hfill{}
 \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-mid-ext} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp2-mid-ext, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'mid_ext', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Middle-and-Extreme Spacing in Experiment 2',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}D)'))
```

```{r plot-summary-exp2, include=F, eval=F}

generate_summary_facet_plot(condition_data = cond_summary_exp_2, lower_y_limit = -6, upper_y_limit = 20, ticks = 2, exp_num = 'Figures/exp2_', 
                            y_axis_var = 'mean_perc_error', y_axis_name = 'Parameter bias (percentage error)', 
                            x_axis_var = 'sample_size', x_axis_name = 'Sample size (*N*)')

```


## Discussion of Experiment 2 

```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```
# Experiment 3

```{=tex}
\nointerlineskip
\vfill
\newpage
```

Experiment 3 investigated how the modelling accuracy of a nonlinear pattern was different under combinations of measurement number and sample size was affected by different levels of time structuredness (see Table \ref{tab:experimentOverview}). Convergence success rate was computed for each cell and percent bias was computed for each parameter in each cell. Variables held constant were the nature of change (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] = 180), measurement spacing (equal), the distribution of errors over time (independent and identically distributed), and absence of missing data. 


## Methods
### Variables Used in Simulation Experiment 

#### Independent Variables

##### Number of Measurements

I used the following values for the number of measurements: 5, 7, 9, and 11 (see [number of measurements](#number-measurements} for more discussion)).  

##### Sample Size

I used the following values for sample size: 30, 50, 100, 200, 500, and 100 (see [sample size](#sample-size) for more discussion). 

##### Time Structuredness and the Simulation Procedure

To simulate time-unstructured data, response rates at each collection
point followed an exponential pattern described by either a fast or slow
response rate (for a review, see section on [time
structuredness](#sec:time-structuredness)). Importantly, data generated
for each person at each time point had to be sampled according to a
probability density function defined by either the fast or slow response
rate cumulative distribution function. In the current context, a
**probability density function** describes the probability of sampling
any given time delay value $x$ where the range of time delay values is
0--36 ($\{x : 0 \le x \le  36 \}$). To obtain the probability density functions
for fast and slow response rates, the response rate function shown in
Equation \@ref(eq:exponential) was differentiated with respect to $x$ to
obtain the function shown below in Equation \ref{eq:pdf-function}\footnote{Euler's notation for differentiation is used to represent derivatives. In words, $\frac{\partial f(x)}{\partial x}$ means that the derivative of the function $f(x)$ is taken with respect to $x$.}:

```{=tex}
\begin{align}
f^\prime = \frac{\partial f(x)}{\partial x} &= \frac{\partial}{\partial x}M(1 - e^{-ax}). \nonumber \\
&= M (e^{-ax}a)
(\#eq:pdf-function)
\end {align}
```

\noindent To compute the probability density function for the fast
response rate cumulative distribution function, the growth rate
parameter $a$ was set to 0.37 in Equation \ref{eq:pdf-function} to
obtain the following function in Equation \ref{eq:fast-pdf-function}:

```{=tex}
\begin{align}
f^\prime_{fast}(x) = M (e^{-a_{fast}x}a_{fast}) = M (e^{-0.37x}0.37). 
(\#eq:fast-pdf-function)
\end {align}
```
\noindent To compute the probability density function for the slow
response rate cumulative distribution function, the growth rate
parameter $a$ was set to 0.15 in Equation \ref{eq:pdf-function} to
obtain the following function in Equation \ref{eq:slow-pdf-function}:

```{=tex}
\begin{align}
f^\prime_{slow}(x) = M (e^{-0.15}a_{slow}) = M (e^{-0.15}0.15). 
(\#eq:slow-pdf-function)
\end {align}
```
Figure \ref{fig:cdf-pdf-plots} shows the fast and slow response
cumulative distribution functions (CDF) and their corresponding
probability density functions (PDF). Panel A shows the cumulative
distribution function for the fast response rate (with a growth
parameter value $a$ set to 0.37; see Equation \ref{eq:fast-cdf}) and
Panel B shows the probability density function that results from
computing the derivative of the fast response rate cumulative
distribution function with respect to $x$ (see Equation
\ref{eq:fast-pdf-function}). Panel C shows the cumulative distribution
function for the slow response rate (with a growth parameter value $a$
set to 0.15; see Equation \ref{eq:slow-cdf})) and Panel D shows the
probability density function that results from computing the derivative
of the slow response rate cumulative distribution function with respect
to $x$ (see Equation \ref{eq:slow-pdf-function} and section on [time
structuredness](#sec:time-structuredness) for more discussion). For the
fast response rate functions, an 80% response rate is obtained after
4.32 days or, equivalently, 80% of the area underneath the probability
density function is obtained at 4.32 days
($\int^{4.32}_{0} f_{fast}^\prime (x) = 0.80$; the integral from 0 to 4.32 of the probability density function for a fast response rate $f^\prime(x)_{fast}$ is 0.80). For the slow response
rate functions, an 80% response rate is obtained after 10.80 days or,
equivalently, 80% of the area underneath the probability density
function is obtained at 10.80 days
($\int^{10.80}_{0} f_{slow}^\prime (x) = 0.80$; the integral from 0 to 10.80 of the probability density function for a slow response rate $f^\prime(x)_{slow}$ is 0.80).

```{r pdf-time-structuredness, eval=F, include=F}
#1) Generate CDFs
day <- seq(from = 0, to = 36, by = 0.01)
M <- 1
satiation_value <- 0.8
satiation_point_fast <- 4.32
satiation_point_slow <- 10.80

a_fast <- log(1 - satiation_value)/-satiation_point_fast
a_slow <- log(1 - satiation_value)/-satiation_point_slow

#y data
y_fast <- M*(1 - exp(-a_fast*day))
y_slow <- M*(1 - exp(-a_slow*day))

#probability values
cdf_fast <- expression(M*(1 - exp(-a_fast*day)))
cdf_slow <- expression(M*(1 - exp(-a_slow*day)))

pdf_fast <- D(expr = cdf_fast, 'day')
pdf_slow <- D(expr = cdf_slow, 'day')

probability_values_fast<- eval(pdf_fast)
probability_values_slow <- eval(pdf_slow)


cdf_pdf_data <- data.frame('response_rate' = factor(c(rep('fast', times = length(y_fast)), 
                                           rep('slow', times = length(y_slow)))), 
                       'day' = rep(day, times = 2), 
                       'CDF' = c(y_fast, y_slow), 
                       'PDF' = c(probability_values_fast, probability_values_slow))

cdf_pdf_data_long <- cdf_pdf_data %>%
  pivot_longer(cols = c(CDF, PDF), names_to = 'prob_dist',
  names_ptypes = factor(levels = c('CDF', 'PDF'))) %>% 
  unite('dist_type', c('response_rate', 'prob_dist')) %>%
  mutate(dist_type = factor(dist_type, levels = c('fast_CDF', 'slow_CDF','fast_PDF', 
                                                   'slow_PDF')))

cdf_pdf_data_long$dist_type <- recode_factor(cdf_pdf_data_long$dist_type,   
                                             fast_CDF = 'bold(A:~CDF~(Fast~response~rate))', 
                                             slow_CDF = 'bold(C:~CDF~(Slow~response~rate))', 
                                             fast_PDF = 'bold(B:~PDF~(Fast~response~rate))', 
                                             slow_PDF = 'bold(D:~PDF~(Slow~response~rate))')
                
#lines showing 80% mark    
v_line_data <- data.frame(
  dist_type =c("bold(A:~CDF~(Fast~response~rate))", "bold(C:~CDF~(Slow~response~rate))"), 
  x = c(4.32, 10.80), 
  x_end = c(4.32, 10.80), 
  y = c(0.8, 0.8), 
  y_end = c(0, 0))

h_line_data <- data.frame(
  dist_type = c("bold(A:~CDF~(Fast~response~rate))", "bold(C:~CDF~(Slow~response~rate))"), 
  x = c(0, 0), 
  x_end = c(4.32, 10.80), 
  y = c(0.8, 0.8), 
  y_end = c(0.8, 0.8))

#needed for shading
pdf_shading_data <- cdf_pdf_data_long %>% 
  filter(str_detect(dist_type, pattern = 'bold\\(B') & day <= 4.32 | 
         str_detect(dist_type, pattern = 'bold\\(D') & day <= 10.80)

#needed for points 
point_data <- data.frame(
  dist_type =c(rep("bold(A:~CDF~(Fast~response~rate))", times = 1), 
               rep("bold(C:~CDF~(Slow~response~rate))", times = 1)),
  x = c(4.32, 10.80), 
  y = c(0.8, 0.8))

#arrows 
arrow_data <- data.frame(
  dist_type =c(rep("bold(A:~CDF~(Fast~response~rate))", times = 1), 
               rep("bold(C:~CDF~(Slow~response~rate))", times = 1)),
  xmin = c(4.32, 10.80), 
  xmax = c(10, 17), 
  ymin = c(0.8, 0.8), 
  ymax = c(0.25, 0.25))


#equations 
equation_data <- data.frame(
  dist_type =c(rep("bold(A:~CDF~(Fast~response~rate))", times = 4), 
               rep("bold(C:~CDF~(Slow~response~rate))", times = 4), 
               rep("bold(B:~PDF~(Fast~response~rate))", times = 3), 
               rep("bold(D:~PDF~(Slow~response~rate))", times = 3)),
  
  label = c("f[fast](x) == M(1 - e^{-a[fast]~x})", "a[fast] == 0.37", "M ==1", "0.80 == 1(1-e^{-0.37(4.32)})", 
            "f[slow](x) == M(1 - e^{-a[slow]~x})", "a[slow] == 0.15", "M == 1", "0.80 == 1(1-e^{-0.15(10.80)})",
            
            "f[fast]^{phantom() * minute }(x) * {phantom() == phantom()} * frac(partialdiff *f[fast](x),  partialdiff *x) * {phantom() == phantom()} * M(e^{a[fast]*x} * a[fast])",
            "integral(f[fast]^{phantom() * minute}, 0, 4.32)*(x) == f[fast](4.32) - f[fast](0)", "phantom() == 0.80", 
            
            "f[slow]^{phantom() * minute }(x) * {phantom() == phantom()} * frac(partialdiff *f[slow](x),  partialdiff *x) * {phantom() == phantom()} * M(e^{a[slow]*x} * a[slow])",
            "integral(f[slow]^{phantom() * minute}, 0, 10.80)*(x) == f[slow](10.80) - f[slow](0)", "phantom() == 0.80"), 
  x = c(18, 17, 18, 18, 
        24, 25, 24, 24, 
        22, 22, 20.5, 
        22, 22, 20.5),
  y = c(rep(c(0.8, 0.65, 0.50, 0.2), times = 2), 
        0.35, 0.15, 0.10, 
        0.35, 0.15, 0.10))


           
cdf_pdf_plot <- ggplot(cdf_pdf_data_long, aes(x = day, y = value)) + 
  geom_line(size = 1.5) + 
  scale_y_continuous(breaks = c(0, 0.25, 0.50, 0.80, 1.00)) +
  theme_classic(base_family = 'Helvetica') + 
  
  geom_area(data = pdf_shading_data, mapping = aes(x = day, y = value), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 1.5) +
  geom_text(data = equation_data, inherit.aes = F, mapping = aes(x = x, y = y, label = label), parse = T, size = 9) + 
  geom_point(data = point_data, inherit.aes = F, mapping = aes(x = x , y = y), size = 4) + 
  
  #arrows
  geom_segment(data = arrow_data, inherit.aes = F, mapping = aes(x = xmin, xend = xmax, y = ymin, yend = ymax), 
               arrow = arrow(length = unit(0.3, 'cm')), size = 1)  + 
  
    #vertical lines 
  geom_segment(data = v_line_data, mapping = aes(x = x, y = y, xend = x_end, yend = y_end), linetype = 2, size = 1) + 
  geom_segment(data = h_line_data, mapping = aes(x = x, y = y, xend = x_end, yend = y_end), linetype = 2, size = 1)  + 



  facet_wrap_custom( ~ dist_type, scales = "free", ncol = 2, nrow = 2 , dir = 'v',
                     labeller = label_parsed,  

                            scale_overrides = list(
                            scale_override(1,
                              scale_x_continuous(
                                breaks = c(0, 4.32, 18, 24, 36),
                                limits = c(0, 36))), 
                          
                            scale_override(which = 2,
                              scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4),
                                limits = c(0, 0.4))),
                             scale_override(which = 2,
                              scale_x_continuous(breaks = c(0, 4.32, 18, 24, 36),
                                limits = c(0, 36))),
                            
                            scale_override(which = 3,
                               scale_x_continuous(
                                breaks = c(0, 10.80, 18, 24, 36),
                                limits = c(0, 36))), 
                         
                            scale_override(4,
                              scale_x_continuous(
                                breaks = c(0, 10.80, 18, 24, 36),
                                limits = c(0, 36))), 
                            scale_override(which = 4,
                                scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4),
                                limits = c(0, 0.4))))) +  


  labs( x = 'Response window day') + 
  
  theme(strip.text.x = element_text(face = 'bold', hjust = 0, size = 30, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),
        strip.background = element_rect(fill = "white", color = "white"), 

        #axis details
        axis.text = element_text(size = 22, color = 'black'),
        axis.title = element_text(size = 28),
        axis.line = element_line(size = 1),
        axis.ticks.length.x = unit(x = 0.5, units = 'cm'), 
        axis.title.x = element_text(margin = unit(c(1, 0, 0, 0), "cm")),
        axis.ticks = element_line(size = 1, colour = 'black'),

      panel.spacing.y = unit(x = 2, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
       

g <- ggplotGrob(cdf_pdf_plot)

#customize y-axis label 
g$grobs[[28]]$children$GRID.text.24299$label <- paste("Density (probability, f'(x))", str_pad('', width = 13), "Response percentage (f(x))")
g$grobs[[28]]$children$GRID.text.24299$y <- grid::unit(0.52,"npc")
g$grobs[[28]]$children$GRID.text.24299$x <- grid::unit(-0.2,"npc")

plot_converted <- as_ggplot(g)

#create PDF of faceted plot
set_panel_size(p = plot_converted, height = unit(x = 32, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/cdf_pdf_plots.pdf')
```

```{=tex}
\begin{figure}[H]
  \caption{Cumulative Distribution Functions (CDF) and Probability Density Functions (PDF) for Fast and Slow Response Rates}
  \label{fig:cdf-pdf-plots}
  \includegraphics{Figures/cdf_pdf_plots} \hfill{}
  \caption*{Note. \textup{Panel A shows the cumulative distribution function for the fast response rate (with a growth parameter value $a$ set to 0.37; see Equation \ref{eq:fast-cdf}) and Panel B shows the probability density function that results from computing the derivative of the fast response rate cumulative distribution function with respect to $x$ (see Equation \ref{eq:fast-pdf-function}). Panel C shows the cumulative distribution function for the slow response rate (with a growth parameter value $a$ set to 0.15; see Equation \ref{eq:slow-cdf}) and Panel D shows the probability density function that results from computing the derivative of the slow response rate cumulative distribution function with respect to $x$ (see Equation \ref{eq:slow-pdf-function} and section \ref{sec:time-structuredness} for more discussion on time structuredness). For the fast response rate functions, an 80\% response rate is obtained after 4.32 days or, equivalently, 80\% of the area underneath the probability density function is obtained at 4.32 days ($\int^{4.32}_{0} f_{fast}^\prime (x) = 0.80$). For the slow response rate functions, an 80\% response rate is obtained after 10.80 days or, equivalently, 80\% of the area underneath the probability density function is obtained at 10.80 days ($\int^{10.80}_{0} f_{slow}^\prime (x) = 0.80$).}}
\end{figure}
```
Having computed probability density functions for fast and slow response
rates, time delays could be generated to create time-unstructured data. To generate time-unstructured data for a
person at a given time point, a time delay wasfirst
generated by sampling values according to the probability density function defined by either a fast or slow response rate (Equations \ref{eq:fast-pdf-function}--\ref{eq:slow-pdf-function}). The sampled time delay was then added to the value of the current measurement day, with the combined measurement day then being plugged into the logistic function (Equation \ref{eq:logFunction-generation}) along with a set of person-specific parameter values to generate an observed score at a given time point for a given person. 


#### Constants 

Because the nature of change not manipulated in Experiment 2, I set it to have a constant value across all cells. To keep the nature of change constant across all cells, I set the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$) to have a value of 180. Another variable set to a constant value across the cells was measurement spacing (equal spacing was used). 

#### Dependent Variables

##### Convergence Success Rate

The proportion of iterations in a cell where models converged defined
the **convergence success rate**.\footnote{Specifically, convergence was obtained if the convergence code returned by OpenMx was 0.} Equation \@ref(eq:convergence) below shows the calculation used to compute the convergence success rate:

```{=tex}
\begin{align}
  \text{Convergence success rate} =  \frac{\text{Number of models that successfully converged}}{n},
  (\#eq:convergence) 
\end{align}
```
\noindent where *n* represents the total number of models run in a cell.


##### Bias

Bias was calculated to evaluate the accuracy with which each logistic function parameter was estimated. As shown below in Equation \@ref(eq:bias), *bias* was obtained by calculating the difference
between the population value set for a parameter and the average
estimated value in each cell.

\useshortskip
```{=tex}
\begin{align}
  \text{Bias} =  \text{Population value for parameter} - \text{Average estimated value}
  (\#eq:bias) 
\end{align}
```

##### Precision

In addition to computing bias, precision was calculated to evaluate the confidence with which each parameter was estimated in a given cell. *Precision* was obtained by identifying the range of values covered by the middle 95% of values estimated for a logistic parameter in each cell. By using the middle 95% of estimated values, a plausible range of population estimates was obtained.   

## Overview of Data Generation and Analysis of Dependent Variables 

Data generation and analysis of the dependent variables was the same as in Experiment 1 (see [data generation and analysis](#data-generation-analysis) for discussion).  



## Results

### Time-Structured Data

```{r plots-time-structured-exp3, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_3_analytical, 
                               target_col = 'time_structuredness', target_value = 'Time structured',
                                                              x_axis_name = expression("Sample Size ("*italic(N)*")"), 

                               x_axis_var = 'sample_size', exp_num = 'exp3_', beta_lower = 160, beta_upper = 210,
                                beta_ticks = 5)
```

```{r text-values-time-structured-exp3, echo=F}
#fixed-effect halfway-triquarter delta (5, 100)
gamma_fixed_equal_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 5, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_equal_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter(7, 100)
gamma_fixed_equal_7_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 7, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))  

#fixed-effect halfway-triquarter delta (5, 200)
gamma_rand_equal_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci),
         estimate = as.integer(estimate))

```


Figure \ref{fig:exp3_plot_days_time_structured} shows the parameter estimation plots
for the day-unit parameters with time-structured data. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp3-time-structured} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters with time-structured data. 

#### Bias

For all simulations pertaining to time-structured data (see Figure \ref{fig:exp3_plot_days_time_structured}), bias only occurred in the estimation of the random-effect triquarter-halfway delta parameter($\upgamma_{random}$; Figure \ref{fig:exp2_plot_equal}D). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D) is difficult to see because 10% of the parameter's population value is a small relative to the range of the y-axis (i.e., 10% of 4.00 is 0.40, which is small relative to the y-axis range of 55). For the random-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:fig:exp3_plot_days_time_structured}D), bias on the five-measurements line (square dots) at sample sizes of 30 to 1000 (shown on x-axis), the seven-measurements line (circular dots) at sample sizes of 30 to 1000, the nine-measurements line (triangle dots) at sample sizes of 30 and 50, and the eleven-measurements line (diamond shape) at sample sizes of 30 and 50. For all other cells, bias did not occur.

#### Precision

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp3_plot_days_time_structured}), instances of low precision occurred for all parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp3_plot_days_time_structured}A). I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the random-effects days-to-halfway elevation ($\upbeta_{random}$; Figure \ref{fig:exp3_plot_days_time_structured}B) and halfway-triquarter delta parameters ($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_time_structured}D), error bar lengths were longer than the 20% tolerated amount in all the cells. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp3_plot_days_time_structured}C), error bar lengths were longer than the 20% tolerated amount with five measurements at sample sizes of 30 to 1000 (shown on x-axis), with seven measurements at sample sizes of 30 to 500 (shown on x-axis), with nine measurements at sample sizes of 30 and 50 (shown on x-axis), and with eleven measurements (diamond dots) at sample sizes of 30 and 50 (shown on x-axis). For all other cells in Figure \ref{fig:exp1_plot_equal}C, precision was high. 

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Structured Data in Experiment 3}
  \label{fig:exp3_plot_days_time_structured}
  \includegraphics{Figures/exp3_plot_days_time structured} \hfill{}
 \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-time-structured} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp3-time-structured, echo=F}
print_bias_var_omega_table(exp_data = exp_3_raw, target_col = 'time_structuredness', target_value = 'time_structured', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Structured Data in Experiment 3',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 100), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp3_plot_days_time_structured}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp3_plot_days_time_structured}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp3_plot_days_time_structured}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp3_plot_days_time_structured}D)'))

```


### Time-Unstructured Data (Fast Response)

```{r plots-fast-response-exp3, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_3_analytical, 
                               target_col = 'time_structuredness', target_value = 'Time unstructured (fast response)',
                              x_axis_name = expression("Sample Size ("*italic(N)*")"), 
                               x_axis_var = 'sample_size', exp_num = 'exp3_', beta_lower = 160, beta_upper = 210,
                                 beta_ticks = 5)
```

```{r text-values-fast-response-exp3, echo=F}
#fixed-effect triquarter-halfway delta parameter (5, 1000)
gamma_fixed_fast_5_1000 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 5, sample_size == 1000, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect triquarter-halfway delta parameter (5, 200)
gamma_random_fast_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))

#random-effect triquarter-halfway delta parameter (5, 200)
gamma_random_fast_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect triquarter-halfway delta parameter (7, 200)
gamma_random_fast_7_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 7, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp3_plot_days_time_structured} shows the parameter estimation plots
for the day-unit parameters with time-unstructured data characterized by fast response rates. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp3-time-structured} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters with time-structured data. 

#### Bias

For all simulations pertaining to time-untructured data characterized by fast response rates (see Figure \ref{fig:exp3_plot_days_fast_response}), bias only occurred in the estimation of the random-effect triquarter-halfway delta parameter($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_fast_response}D). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error band for the random-effect triquarter-halfway delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_fast_response}D) is difficult to see because 10% of the parameter's population value is a small value relative to the range of the y-axis (i.e., 10% of 4.00 is 0.40, which is small relative to the y-axis range of 55). For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:fig:exp3_plot_days_time_structured}D), bias occured on the five-measurements line (square dots) at sample sizes of 30 to 1000 (shown on x-axis), the seven-measurements line (circular dots) at sample sizes of 30 to 500, the nine-measurements line (triangle dots) at sample sizes of 30 and 100, and the eleven-measurements line (diamond shape) at sample sizes of 30 and 50. For all other cells, bias did not occur.

#### Precision

For all simulations pertaining to equal spacing (see Figure \ref{fig:exp3_plot_days_fast_response}), instances of low precision occurred for all parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp3_plot_days_fast_response}A). I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the random-effects days-to-halfway elevation ($\upbeta_{random}$; Figure \ref{fig:exp3_plot_days_fast_response}B) and halfway-triquarter delta parameters ($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_fast_response}D), error bar lengths were longer than the 20% tolerated amount in all the cells. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp3_plot_days_fast_response}C), error bar lengths were longer than the 20% tolerated amount with five measurements (square dots) at sample sizes of 30 to 1000 (shown on x-axis), with seven measurements (circle dots) at sample sizes of 30 to 200 (shown on x-axis), with nine measurements (triangle dots) at sample sizes of 30 and 50 (shown on x-axis), and with eleven measurements (diamond dots) at sample sizes of 30 and 50 (shown on x-axis). For all other cells in Figure \ref{fig:exp1_plot_equal}C, precision was high. 

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Unstructured Data (Fast Response Rate) in Experiment 3}
  \label{fig:exp3_plot_days_fast_response}
  \includegraphics{Figures/exp3_plot_days_time unstructured (fast response)} \hfill{}
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. See Table \ref{tab:exp3-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-fast-response} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp3-fast-response, echo=F}
print_bias_var_omega_table(exp_data = exp_3_raw, target_col = 'time_structuredness', target_value = 'fast_response', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Unstructured Data (Fast Response Rate) in Experiment 3',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp3_plot_days_fast_response}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp3_plot_days_fast_response}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp3_plot_days_fast_response}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp3_plot_days_fast_response}D)'))

```

### Time-Unstructured Data (Slow Response)

```{r plots-slow-response-exp3, include=F, echo=F}
generate_day_likert_facet_plot(analytical_data = exp_3_analytical, 
                               target_col = 'time_structuredness', target_value = 'Time unstructured (slow response)',
                               x_axis_name = expression("Sample Size ("*italic(N)*")"), 
                               x_axis_var = 'sample_size', exp_num = 'exp3_', beta_lower = 160, beta_upper = 210,
                                beta_ticks = 5)
```
```{r text-values-slow-response-exp3, echoF}
#fixed-effect triquarter-halfway delta parameter (5, 1000)
gamma_fixed_slow_5_1000 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 1000, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect triquarter-halfway delta parameter (5, 200)
gamma_random_slow_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_slow_5_100 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_fixed_slow_5_30 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 30, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

beta_fixed_slow_11_1000 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 11, sample_size == 1000, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect triquarter-halfway delta parameter (5, 100)
gamma_random_slow_5_100 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 100, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect triquarter-halfway delta parameter (7, 100)
gamma_random_slow_7_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 7, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp3_plot_days_slow_response} shows the parameter estimation plots
for the day-unit parameters with time-unstructured data characterized by slow response rates. Error bars
represent the middle 95% of estimated values, blue horizontal lines indicate the population values for each parameter, and gray bands specify acceptable margins of error. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20% of the parameter's population value as indicative of low-precision estimation. Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Note that Table \ref{tab:omega-exp3-time-structured} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters with time-unstructured data characterized by slow response rates.

#### Bias

For all simulations pertaining to time-unstructured data characterized by fast response rates (see Figure \ref{fig:exp3_plot_days_slow_response}), bias occurred in the estimation of all the parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp3_plot_days_slow_response}A). The gray bands indicate the $\pm 10\%$ margin of error. I classified dots that fell outside the gray band as biased and these dots remained unfilled. Note that the gray 10% margin of error bands for the random-effect days-to-halfway elevation and triquarter-halfway delta parameters ($\upbeta_{random}$ [Figure \ref{fig:exp3_plot_days_slow_response}C] and $\upgamma_{random}$ [\ref{fig:exp3_plot_days_slow_response}D], respectively) were difficult to see because 10% of each parameter's population value was small value relative to the range of the y-axis (i.e., 10% of 4.00 is 0.40, which is small relative to the y-axis range of 55). For the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$; Figure \ref{fig:fig:exp3_plot_days_slow_response}D), bias occured in all the cells except on the five-measurements line (square dots) at a sample size of 50 (shown on x-axis) and on the eleven-measurements line at sample sizes of 30 and 50 (shown on x-axis). For the random-effect halfway-triquarter delta parameter ($\upgamma_{random}$; Figure \ref{fig:fig:exp3_plot_days_slow_response}D), bias occured in all cells. For the fixed-effect days-to-halfway elevation parameter ($\upbeta_{beta}$; Figure \ref{fig:fig:exp3_plot_days_slow_response}B), bias only occured on the five-measurements line (square dots) at a sample size of 50 (shown on x-axis).

#### Precision

For all simulations pertaining to time-unstructured data characterized by fast response rates (see Figure \ref{fig:exp3_plot_days_slow_response}), instances of low precision occurred for all parameters except the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure \ref{fig:exp3_plot_days_slow_response}A). I considered precision to be low if the length of an error bar covered a range of values larger than 20% of the parameter's population value. Error bars indicating low precision were coloured in light blue. For the random-effects days-to-halfway elevation ($\upbeta_{random}$; Figure \ref{fig:exp3_plot_days_slow_response}B) and halfway-triquarter delta parameters ($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_slow_response}D), error bar lengths were longer than the 20% tolerated amount in all the cells. For the fixed-effect halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp3_plot_days_slow_response}B), error bar lengths were longer than the 20% tolerated amount with five measurements (square dots) at sample sizes of 30 to 1000 (shown on x-axis), with seven measurements (circle dots) at sample sizes of 30 to 200 (shown on x-axis), with nine measurements (triangle dots) at sample sizes of 30, 50, and 100 (shown on x-axis), and with eleven measurements (diamond dots) at sample sizes of 30, 50, and 100 (shown on x-axis). For all other cells in Figure \ref{fig:exp1_plot_equal}C, precision was high.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Unstructured Data (Slow Response Rate) in Experiment 3}
  \label{fig:exp3_plot_days_slow_response}
  \includegraphics{Figures/exp3_plot_days_time unstructured (slow response)} \hfill{}
   \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. See Table \ref{tab:exp3-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-slow-response} for $\upomega^2$ effect size values.}}
\end{figure}
```

#### $\upomega^2$ Effect Sizes

```{r omega-exp3-slow-response, echo=F}
print_bias_var_omega_table(exp_data = exp_3_raw, target_col = 'time_structuredness', target_value = 'slow_response', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Untructured Data (Slow Response Rate) in Experiment 3',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp3_plot_days_slow_response}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp3_plot_days_slow_response}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp3_plot_days_slow_response}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp3_plot_days_slow_response}D)'))

```



## Discussion of Experiment 3 



# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

```{=tex}
\endgroup
```


# Supplementary Appendix
## Appendix A: Procedure for Generating Measurement Schedules in Measurement Spacing Cells {.unnumbered}

Given that no procedure existed (to my knowledge) for creating
measurement schedules, I devised a method for generating measurement
schedules for the four spacing conditions (equal, time-interval
increasing, time-interval decreasing, and middle-and-extreme spacing).
For each measurement spacing conditions across all measurement number
levels, a two-step procedure was employed to generate measurement
schedules in Experiments 2--3. At a broad level, the first step involved
computing setup variables and the second step computed the interval
lengths.

### Appendix A1: Procedure for calculating measurement schedules with equal spacing {.unnumbered}

Figure \@ref{fig:equal_spacing_diagram} shows how the two-step procedure
was implemented to construct a measurement schedule with equal spacing
and five measurements. In the first step, the number of intervals ($NI$)
was computed by subtracting one from the number of measurements ($NM$),
giving five measurements ($NM = 5$) and four intervals ($NI = 4$). In
the second step, interval lengths were calculated by dividing the length
of the measurement period ($MP$) by the number of intervals ($NI$),
yielding an interval length of 90 days
($\frac{MP}{NI} = \frac{360}{4} = 90$).

\blandscape

```{=tex}
\begin{figure}[H]
  \caption{Procedure for Generating Equal Measurement Spacing Schedules}
  \label{fig:equal_spacing_diagram}
  \includegraphics{Figures/equal_spacing_diagram} \hfill{}
  \caption*{Note. \textup{}}
\end{figure}
```
\elandscape

### Appendix A2: Procedure for calculating measurement schedules with time-interval increasing spacing

Figure \@ref{fig:time_inc_diagram} shows how the two-step procedure was
used to calculate the interval lengths for measurement schedules defined
by time-interval increasing spacing with five measurements. In the first
step, the number of intervals was determined by subtracting one from the
number of measurements, yielding a value of four for the number of
intervals ($NI = NM - 1 = 5 - 1 = 4$). Importantly, the length of each
interval increased by a constant value $c$ over time as shown below in
Equation \ref{eq:time-inc-length}:

```{=tex}
\begin{align}
\text{Interval length} = x + \#IN(c)
(\#eq:time-inc-length)
\end{align}
```
\noindent where $\#IN$ represents the interval number in increasing
order such that
$\#IN \in \{0, ..., \text{number of intervals (NI)} - 1\}$. In the
second step, the constant value by which interval lengths increased
($c$) was computed by first subtracting the smallest interval length
from each interval (i.e., $x = 36$ days) from the measurement period
($MP$), yielding 216 remaining days
($N_{remain} = MP - NIx = 360 - 4(36) = 216$). The number of remaining
days then had to be divided across the constant interval lengths.
Because each interval increased by some constant value ($c$ after each
measurement point, the total number of constant-value interval lengths
was obtained by computing the following sum in Equation
\ref{eq:time-sum-constants}:

```{=tex}
\begin{align}
\text{Number constant intervals} = \sum^{\#IN}_{i = 0} i.
(\#eq:time-sum-constants)
\end{align}
```
\noindent With $\#IN = 3$, the number of constant intervals was 6, and
so the constant value was obtained by using Equation
\ref{eq:constant-length} below:

```{=tex}
\begin{align}
\text{Number constant intervals} = \frac{N_{remain}}{\sum^{\#IN}_{i = 0} i},
(\#eq:constant-length)
\end{align}
```
\noindent giving a length of 36 days for the constant value
($c = \frac{216}{6} = 36$ days). Having computed the value for $c$, the
following interval lengths were obtained:

-   $i_{1} = x + \#IN(c) = 36 + 0(36)$ = 36 days
-   $i_{2} = x + \#IN(c) = 36 + 1(36)$ = 72 days
-   $i_{3} = x + \#IN(c) = 36 + 2(36)$ = 108 days
-   $i_{4} = x + \#IN(c) = 36 + 3(36)$ = 144 days

\blandscape

```{=tex}
\begin{figure}[H]
  \caption{Procedure for Generating Time-Interval Increasing Measurement Spacing Schedules}
  \label{fig:time_inc_diagram}
  \includegraphics{Figures/time_inc_diagram} \hfill{}
  \caption*{Note. \textup{}}
\end{figure}
```
\elandscape

### Appendix A3: Procedure for calculating measurement schedules with time-interval decreasing spacing {.unnumbered}

Figure \@ref{fig:time_dec_diagram} shows how the two-step procedure was
used to calculate the interval lengths for measurement schedules defined
by time-interval decreasing spacing with five measurements. In the first
step, the number of intervals was determined by subtracting one from the
number of measurements, yielding a value of four for the number of
intervals ($NI = NM - 1 = 5 - 1 = 4$). Importantly, the length of each
interval decreased by a constant value $c$ over time as shown below in
Equation \ref{eq:time-dec-length}:

```{=tex}
\begin{align}
\text{Interval length} = x + \#INT(c)
(\#eq:time-dec-length)
\end{align}
```
\noindent where $\#IN$ represents the interval number in decreasing
order such that
$\#IN \in \{\text{number of intervals (NI)} - 1, ... , 0\}$. In the
second step, the constant value by which interval lengths decreased
($c$) was computed by first subtracting the smallest interval length
from each interval (i.e., $x = 36$ days) from the measurement period
($MP$), yielding 216 remaining days
($N_{remain} = MP - NIx = 360 - 4(36) = 216$). The number of remaining
days then had to be divided across the constant interval lengths.
Because each interval decreased by some constant value ($c$ after each
measurement point, the total number of constant-value interval lengths
was obtained by computing the following sum in Equation
\ref{eq:dec-sum-constants}:

```{=tex}
\begin{align}
\text{Number constant intervals} = \sum_{\#IN}^{i = 0} i.
(\#eq:dec-sum-constants)
\end{align}
```
\noindent With $\#IN = 3$, the number of constant intervals was 6, and
so the constant value was obtained by using Equation
\ref{eq:constant-length-2} below:

```{=tex}
\begin{align}
\text{Number constant intervals} = \frac{N_{remain}}{\sum^{\#IN}_{i = 0} i},
(\#eq:constant-length-2)
\end{align}
```
\noindent giving a length of 36 days for the constant value
($c = \frac{216}{6} = 36$ days). Having computed the value for $c$, the
following interval lengths were obtained:

-   $i_{1} = x + \#IN(c) = 36 + 3(36)$ = 144 days
-   $i_{4} = x + \#IN(c) = 36 + 0(36)$ = 36 days
-   $i_{3} = x + \#IN(c) = 36 + 1(36)$ = 72 days
-   $i_{2} = x + \#IN(c) = 36 + 2(36)$ = 108 days

\blandscape

```{=tex}
\begin{figure}[H]
  \caption{Procedure for Generating Time-Interval Decreasing Measurement Spacing Schedules}
  \label{fig:time_dec_diagram}
  \includegraphics{Figures/time_dec_diagram} \hfill{}
  \caption*{Note. \textup{}}
\end{figure}
```
\elandscape

### Appendix A4: Procedure for calculating measurement schedules with middle-and-extreme spacing {.unnumbered}

Figure \@ref{fig:mid_ext_diagram} shows how the two-step procedure was
used to calculate the interval lengths for measurement schedules defined
by middle-and-extreme spacing with five measurements. In the first step,
the number of intervals was determined by subtracting one from the
number of measurements, yielding a value of four for the number of
intervals ($NI = NM - 1 = 5 - 1 = 4$)... 

\blandscape

```{=tex}
\begin{figure}[H]
  \caption{Procedure for Generating Time-Interval Decreasing Measurement Spacing Schedules}
  \label{fig:mid_ext_diagram}
  \includegraphics{Figures/mid_ext_diagram} \hfill{}
  \caption*{Note. \textup{}}
\end{figure}
```
\elandscape

## Appendix B: Convergence rates {#appendix-a-convergence-rates}

\newpage

```{r convergence-rates, echo=F, warning=F}
exp_1_conv <- generate_conv_success_data(condition_data = cond_summary_exp_1, first_col = 'measurement_spacing', second_col = 'number_measurements', wide_var = 'midpoint', exp_num = 1, recode_var = 'measurement_spacing')

exp_2_conv <- generate_conv_success_data(condition_data = cond_summary_exp_2, first_col = 'measurement_spacing', second_col = 'number_measurements', wide_var = 'sample_size', exp_num = 2, recode_var = 'measurement_spacing')

exp_3_conv <- generate_conv_success_data(condition_data = cond_summary_exp_3, first_col = 'time_structuredness', second_col = 'number_measurements', wide_var = 'sample_size', exp_num = 3, recode_var = 'time_structuredness')
```

\newpage

```{r conv-exp-1, echo=F}
print_conv_table(table_ready_data = exp_1_conv, 
                 caption_name = 'Convergence Success in Experiment 1', 
                 col_header_name = c('Days to halfway elevation'), 
                 IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
                 column_names = c('80', '180', '280'))
```

\newpage

```{r conv-exp-2, echo=F}
print_conv_table(table_ready_data = exp_2_conv, 
                 caption_name = 'Convergence Success in Experiment 2', 
                 col_header_name = c('Sample size (\\\\textit{N})'), 
                 IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
                 column_names = c('30', '50', '100', '200', '500', '1000'))
```

\newpage

```{r conv-exp-3, echo=F}
print_conv_table(table_ready_data = exp_3_conv, 
                 caption_name = 'Convergence Success in Experiment 3', 
                 col_header_name = c('Sample size (\\\\textit{N})'), 
                 IV_names =  c('Time Structuredness', 'Number of Measurements'), 
                 column_names = c('30', '50', '100', '200', '500', '1000'))
```

## Appendix C: Complete Versions of Parameter Estimation Plots (Day- and Likert-Unit Parameters) {#appendix-c}

### Experiment 1 
### Equal Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Equal Spacing in Experiment 1}
  \label{fig:exp1_plot_equal_app}
  \includegraphics{Figures/exp1_plot_days_equal spacing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Equal Spacing in Experiment 1}
  \label{fig:exp1_plot_equal_app}
  \includegraphics{Figures/exp1_plot_likert_equal spacing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Time-Interval Increasing Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Increasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_inc_app}
  \includegraphics{Figures/exp1_plot_days_time-interval increasing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Interval Increasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_inc_app}
  \includegraphics{Figures/exp1_plot_likert_time-interval increasing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Time-Interval Decreasing Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Decreasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_dec_app}
  \includegraphics{Figures/exp1_plot_days_time-interval decreasing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Interval Decreasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_dec_app}
  \includegraphics{Figures/exp1_plot_likert_time-interval decreasing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Middle-and-Extreme Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Middle-and-Extreme Spacing in Experiment 1}
  \label{fig:exp1_plot_time_mid_ext_app}
  \includegraphics{Figures/exp1_plot_days_middle-and-extreme spacing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Middle-and-Extreme Spacing in Experiment 1}
  \label{fig:exp1_plot_time_mid_ext_app}
  \includegraphics{Figures/exp1_plot_likert_middle-and-extreme spacing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Experiment 2 
### Equal Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Equal Spacing in Experiment 2}
  \label{fig:exp1_plot_equal_app}
  \includegraphics{Figures/exp2_plot_days_equal spacing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Equal Spacing in Experiment 2}
  \label{fig:exp1_plot_equal_app}
  \includegraphics{Figures/exp2_plot_likert_equal spacing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```


### Time-Interval Increasing Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Increasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_inc_app}
  \includegraphics{Figures/exp2_plot_days_time-interval increasing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Interval Increasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_inc_app}
  \includegraphics{Figures/exp2_plot_likert_time-interval increasing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-time-inc} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Time-Interval Decreasing Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Decreasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_dec_app}
  \includegraphics{Figures/exp2_plot_days_time-interval decreasing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Interval Decreasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_dec_app}
  \includegraphics{Figures/exp2_plot_likert_time-interval decreasing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-time-inc} for $\upomega^2$ effect size values.}}
\end{figure}
```
### Middle-and-Extreme Spacing

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Middle-and-Extreme Spacing in Experiment 2}
  \label{fig:exp2_plot_time_mid_ext_app}
  \includegraphics{Figures/exp2_plot_days_middle-and-extreme spacing} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Middle-and-Extreme Spacing in Experiment 2}
  \label{fig:exp2_plot_time_mid_ext_app}
  \includegraphics{Figures/exp2_plot_likert_middle-and-extreme spacing} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:exp2-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-mid-ext} for $\upomega^2$ effect size values.}}
\end{figure}
```
### Experiment 3
### Time-Structured Data 

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Structured Data in Experiment 3}
  \label{fig:exp3_plot_time_struc_app}
  \includegraphics{Figures/exp3_plot_days_time structured} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Structured Data in Experiment 3}
  \label{fig:exp3_plot_time_struc_app}
  \includegraphics{Figures/exp3_plot_likert_time structured} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-time-structured} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Time-Untructured Data (Fast Response Rate)
```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Unstructured Data (Fast Response Rate) in Experiment 3}
  \label{fig:exp3_plot_fast_resp_app}
  \includegraphics{Figures/exp3_plot_days_time unstructured (fast response)} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Unstructured Data (Fast Response Rate) in Experiment 3}
  \label{fig:exp3_plot_fast_resp_app}
  \includegraphics{Figures/exp3_plot_likert_time unstructured (fast response)} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-fast-response} for $\upomega^2$ effect size values.}}
\end{figure}
```

### Time-Untructured Data (Slow Response Rate)

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Unstructured Data (Slow Response Rate) in Experiment 3}
  \label{fig:exp3_plot_slow_resp_app}
  \includegraphics{Figures/exp3_plot_days_time unstructured (slow response)} \hfill{}
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Likert-Unit Parameters With Time-Unstructured Data (Slow Response Rate) in Experiment 3}
  \label{fig:exp3_plot_slow_resp_app}
  \includegraphics{Figures/exp3_plot_likert_time unstructured (slow response)} 
\end{figure}
\addtocounter{figure}{-1}
\begin{figure}[H]
  \caption*{Note. \textup{Panels A--B show the parameter estimation plots for the fixed- and random-effect days-to-halfway elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively). Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively). Panels E--F show the parameter estimation plots for the fixed- and random-effect baseline parameters ($\uptheta_{fixed}$ and $\uptheta_{random}$, respectively). Panels G--H show the parameter estimation plots for the fixed- and random-effect maximal elevation parameters ($\upalpha_{fixed}$ and $\upalpha_{random}$, respectively). Panel I shows the parameter estimation plot for the error parameter ($\upepsilon$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\uptheta_{fixed}$ = 3.00, $\uptheta_{random}$ = 0.05, $\upalpha_{fixed}$ = 3.32, $\upalpha_{random}$ = 0.05, $\upepsilon$ = 0.05. Gray bands indicate the acceptable $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating cells with variability greater than 20\% of the parameter's population value. I considered dots that fell outside the gray bands as biased and error bar lengths that exceeded 20\% of the parameter's population value as indicative of low-precision estimation. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-slow-response} for $\upomega^2$ effect size values.}}
\end{figure}
```



## Appendix D: Parameter estimate tables

### Experiment 1

```{r exp-table-generation, echo=F}
exp_1_tables <- create_table_data_sets(param_summary_data = param_summary_exp_1, 
                                       wide_var = 'midpoint', first_col = 'measurement_spacing', second_col = 'number_measurements')
  
exp_2_tables <- create_table_data_sets(param_summary_data = param_summary_exp_2, 
                                       wide_var = 'sample_size', first_col = 'measurement_spacing', second_col = 'number_measurements')

exp_3_tables <- create_table_data_sets(param_summary_data = param_summary_exp_3, 
                                       wide_var = 'sample_size', first_col = 'time_structuredness', second_col = 'number_measurements') 

#remove measurement spacing columns 
col_to_remove <- which(str_detect(string = names(exp_3_tables$estimate_table), pattern = 'measurement_spacing'))

exp_3_tables$estimate_table <- exp_3_tables$estimate_table[, -col_to_remove]
exp_3_tables$removed_value_table <- exp_3_tables$removed_value_table[ , -col_to_remove]
```

\renewcommand{\arraystretch}{0.8}{0.9}

```{r exp1-alpha-theta-param-est, echo=F}
usepackage_latex('makecell')

print_param_table(table_ready_data = exp_1_tables$estimate_table, parameter_name = 'theta|alpha', 
            caption_name = 'Parameter Values Estimated in Experiment 1', 
            col_header_name = c('\\\\thead{$\\\\uptheta_{fixed}$ (Baseline)  \\\\\\\\ Pop value = 3.00}', 
                                '\\\\thead{$\\\\uptheta_{random}$ (Baseline) \\\\\\\\ Pop value = 0.05}',
                                '\\\\thead{$\\\\upalpha_{fixed}$ (Maximal \\\\\\\\ elevation)  \\\\\\\\ Pop value = 3.32}', 
                                '\\\\thead{$\\\\upalpha_{random}$ (Maximal \\\\\\\\ elevation) \\\\\\\\ Pop value = 0.05}'), 
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c('80', '180', '280'))
```

\addtocounter{table}{-1}

```{r exp1-beta-gamma-param-est, echo=F}
print_param_table(table_ready_data = exp_1_tables$estimate_table, parameter_name = 'beta|gamma', 
            caption_name = 'Parameter Values Estimated in Experiment 1 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upbeta_{fixed}$ (Days to \\\\\\\\ halfway elevation)}', 
                                '\\\\thead{$\\\\upbeta_{random}$ (Days to \\\\\\\\ halfway elevation) \\\\\\\\ Pop value = 10.00}', 
                                '\\\\thead{$\\\\upgamma_{fixed}$ (Triquarter- \\\\\\\\ halfway delta) \\\\\\\\ Pop value = 20.00}', 
                                '\\\\thead{$\\\\upgamma_{random}$ (Triquarter- \\\\\\\\ halfway delta) \\\\\\\\ Pop value = 4.00}'),
             IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c(80, 180, 280))
```

\addtocounter{table}{-1}

```{r exp1-epsilon-param-est, echo=F}
print_param_table(table_ready_data = exp_1_tables$estimate_table, parameter_name = 'epsilon', 
            caption_name = 'Parameter Values Estimated in Experiment 1 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upepsilon$(error) \\\\\\\\ Pop value = 0.03}'),
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c(80, 180, 280))
```

### Experiment 2

```{r exp2-theta-param-est, echo=F}
print_param_table(table_ready_data = exp_2_tables$estimate_table, parameter_name = 'theta', 
            caption_name = 'Parameter Values Estimated in Experiment 2', 
           col_header_name = c('\\\\thead{$\\\\uptheta_{fixed}$ (Baseline) \\\\\\\\ Pop value = 3.00}', 
                               '\\\\thead{$\\\\uptheta_{random}$ (Baseline) \\\\\\\\ Pop value = 0.05}'),
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

\addtocounter{table}{-1}

```{r exp2-alpha-param-est, echo=F}
print_param_table(table_ready_data = exp_2_tables$estimate_table, parameter_name = 'alpha', 
            caption_name = 'Parameter Values Estimated in Experiment 2 (continued)', 
           col_header_name = c('\\\\thead{$\\\\upalpha_{fixed}$ (Maximal elevation) \\\\\\\\ Pop value = 3.32}', 
                               '\\\\thead{$\\\\upalpha_{random}$ (Maximal elevation) \\\\\\\\ Pop value = 0.05}'),
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

\addtocounter{table}{-1}

```{r exp2-beta-param-est, echo=F}
print_param_table(table_ready_data = exp_2_tables$estimate_table, parameter_name = 'beta', 
            caption_name = 'Parameter Values Estimated in Experiment 2 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upbeta_{fixed}$ (Days to halfway elevation) \\\\\\\\ Pop value = 180.00}', 
                               '\\\\thead{$\\\\upbeta_{random}$ (Days to halfway elevation) \\\\\\\\ Pop value = 10.00}'), 
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

\addtocounter{table}{-1}

```{r exp2-gamma-param-est, echo=F}
print_param_table(table_ready_data = exp_2_tables$estimate_table, parameter_name = 'gamma', 
            caption_name = 'Parameter Values Estimated in Experiment 2 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upgamma_{fixed}$ (Triquarter-halfway delta) \\\\\\\\ Pop value = 20.00}', 
                                '\\\\thead{$\\\\upgamma_{random}$ (Triquarter-halfway delta) \\\\\\\\ Pop value = 4.00}'),
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

\addtocounter{table}{-1}

```{r exp2-epsilon-param-est, echo=F}
print_param_table(table_ready_data = exp_2_tables$estimate_table, parameter_name = 'epsilon', 
            caption_name = 'Parameter Values Estimated in Experiment 2 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upepsilon$(error) \\\\\\\\ Pop value = 0.03}'), 
            IV_names =  c('Measurement Spacing', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

### Experiment 3

```{r exp3-theta-param-est, echo=F}
print_param_table(table_ready_data = exp_3_tables$estimate_table, parameter_name = 'theta', 
            caption_name = 'Parameter Values Estimated in Experiment 3', 
           col_header_name = c('\\\\thead{$\\\\uptheta_{fixed}$ (Baseline) \\\\\\\\ Pop value = 3.00}', 
                               '\\\\thead{$\\\\uptheta_{random}$ (Baseline) \\\\\\\\ Pop value = 0.05}'),
           IV_names =  c( 'Time Structuredness', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

```{r exp3-alpha-param-est, echo=F}
print_param_table(table_ready_data = exp_3_tables$estimate_table, parameter_name = 'alpha', 
            caption_name = 'Parameter Values Estimated in Experiment 3 (continued)', 
           col_header_name = c('\\\\thead{$\\\\upalpha_{fixed}$ (Maximal elevation) \\\\\\\\ Pop value = 3.32}', 
                               '\\\\thead{$\\\\upalpha_{random}$ (Maximal elevation) \\\\\\\\ Pop value = 0.05}'),
            IV_names =  c( 'Time Structuredness', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

```{r exp3-beta-param-est, echo=F}
print_param_table(table_ready_data = exp_3_tables$estimate_table, parameter_name = 'beta', 
            caption_name = 'Parameter Values Estimated in Experiment 3 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upbeta_{fixed}$ (Days to halfway elevation) \\\\\\\\ Pop value = 180.00}', 
                               '\\\\thead{$\\\\upbeta_{random}$ (Days to halfway elevation) \\\\\\\\ Pop value = 10.00}'), 
            IV_names =  c('Time Structuredness', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

```{r exp3-gamma-param-est, echo=F}
print_param_table(table_ready_data = exp_3_tables$estimate_table, parameter_name = 'gamma', 
            caption_name = 'Parameter Values Estimated in Experiment 3 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upbeta_{fixed}$ (Days to halfway elevation) \\\\\\\\ Pop value = 180.00}', 
                               '\\\\thead{$\\\\upbeta_{random}$ (Days to halfway elevation) \\\\\\\\ Pop value = 10.00}'), 
            IV_names =  c('Time Structuredness', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```

```{r exp3-epsilon-param-est, echo=F}
print_param_table(table_ready_data = exp_3_tables$estimate_table, parameter_name = 'epsilon', 
            caption_name = 'Parameter Values Estimated in Experiment 3 (continued)', 
            col_header_name = c('\\\\thead{$\\\\upepsilon$(error) \\\\\\\\ Pop value = 0.03}'), 
            IV_names =  c('Time Structuredness', 'Number of Measurements'), 
            column_names = c("30", "50", "100", "200", "500", "1000"))
```


# Technical Appendix
## Technical Appendix A: Ergodicity and the Need to Conduct Longitudinal Research

(ref:petersen1983) [for an introduction, see @petersen1983]
(ref:birkhoff1931) @birkhoff1931 [for a review, see @choe2005, Chapter 3]

To understand why cross-sectional results are unlikely to agree with longitudinal results for any given analysis, a discussion of data structure is apropos. Consider an example where a researcher obtains data from 50 people measured over 100 time points such that each row contains a $p$ person's data over the 100 time points and each column contains data from 50 people at a $t$ time point. For didactic purposes, all data are assumed to be sampled from a normal distribution. To understand whether findings in any given cross-sectional data set yield the same findings in any given longitudinal data set, the researcher randomly samples one cross-sectional and one longitudinal data set and computes the mean and variance in each set. To conduct a cross-sectional analysis, the researcher randomly samples the data across the 50 people at a given time point and computes a mean of the scores at the sampled time point ($\bar{X}_t$) using Equation \ref{eq:cross-mean} shown below:

\begin{align}
\bar{X}_t = \frac{1}{P}\sum^P_{p = 1} x_p,
(\#eq:cross-mean)
\end{align}

\noindent where the scores of all $P$ people are summed ($x_p$) and then divided by the number of people ($P$). To compute the variance of the scores at the sampled time point ($S^2_t$), the researcher uses Equation \ref{eq:cross-variance} shown below:

\begin{align}
\S^2_t = \frac{1}{P}\sum^P_{p = 1} (x_p - \bar{X}_t)^2,
(\#eq:cross-variance)
\end{align}

\noindent where the sum of squared differences between each person's score ($x_p$) and the average value at the given $t$ time point ($\bar{X}_t$) is computed and then divided by the number of people ($P$).  To conduct a longitudinal analysis, the researcher randomly samples the data across the 100 time points for a given person and also computes a mean and variance of the scores. To compute the mean across  the $t$ time points of the longitudinal data set ($\bar{X}_p$), the researcher uses Equation \ref{eq:long-mean} shown below:

\begin{align}
\bar{X}_p = \frac{1}{T}\sum^T_{t = 1} x_t,
(\#eq:long-mean)
\end{align}

\noindent where the scores at each $t$ time point are summed ($x_t$) and then divided by the number of time points ($T$). The researcher also computes a variance of the sampled person's scores across all time points ($S^2_p$) using Equation \ref{eq:long-variance} shown below:

\begin{align}
\S^2_p = \frac{1}{T}\sum^T_{t = 1} (x_t - \bar{X}_p)^2,
(\#eq:long-variance)
\end{align}

\noindent where the sum of squared differences between the score at each time point ($x_t$) and the average value of the $p$ person's scores ($\bar{X}_p$) is computed and then divided by the number of time points ($T$). 

If the researcher wants treat the mean and variance values computed from the cross-sectional and longitudinal data sets as interchangeable, then two conditions outlined by ergodic theory must be satisfied [@molenaar2004; @molenaar2009].\footnote{Note that ergodic theory is an entire mathematical discipline (ref:petersen1983). In the current context, the most important ergodic theorems are those proven by (ref:birkhoff1931).} First, a given cross-sectional mean and variance can only closely estimate the mean and variance of any given person's data (i.e., a longitudinal data set) to the extent that each person's data are generated from a normal distribution with the same mean and variance. If each person's data were generated from a different normal distribution, the computing the mean and variance at a given time point would, at best, describe the values of one person. When each person's data are generated from the same normal distribution, the condition of **homogeneity** is met. Importantly, satisfying the condition of homogeneity does not guarantee that the mean and variance obtained from another cross-sectional data set will closely estimate the mean and variance of any given person (i.e., any given longitudinal data set). The mean and variance values computed from any given cross-sectional data set can only closely estimate the values of any given person to the extent that the cross-sectional mean and variance remain constant over time. If the mean and variance of observations remain constant over time, then the the second condition of **stationarity** is satisfied. Therefore, the researcher can only treat means and variances from cross-sectional and longitudinal data sets as interchangeable if each person's data is generated from the same normal distribution (homogeneity) and if the mean and variance remain constant over time (stationarity). When the conditions of homogeneity and stationarity are satisfied, a process is said to be **ergodic**: Analyses of cross-sectional data sets return the same values as analyses on longitudinal data sets.

Given that psychological studies almost never collect data from only one person, one potential reservation may be that the conditions required for ergodicity only hold when a longitudinal data set contains the data of one person. That is, if the researcher used the full data set containing the data of 100 people sampled over 100 time points and computed 100 cross-sectional means and variances (Equation \ref{eq:cross-mean} and Equation \ref{eq:cross-variance}, respectively) and 100 longitudinal means and variances (Equation \ref{eq:long-mean} and Equation \ref{eq:long-variance}, respectively), wouldn't the average of the cross-sectional means and variances be the same as the average of the longitudinal means and variances? Although the averaging the cross-sectional mean returns the same value as averaging the longitudinal means, the average longitudinal variance remains different from the average cross-sectional variance [for several empirical examples, see @fisher2018]. Therefore, the conditions of ergodicity apply even with larger longitudinal and cross-sectional sample sizes. 

(ref:voelkle2014spector2019) [@voelkle2014; for similar discussion, see @spector2019]
(ref:adolf2019medaglia2019) [@adolf2019; @medaglia2019]

The guaranteed differences in cross-sectional and longitudinal variance values that result from non-ergodic processes have far-reaching implications. Almost every analysis employed in organizational research---whether it be correlation, regression, factor analysis, mediation analysis, etc.---analyzes variability, and so, when a process is non-ergodic, cross-sectional variability will differ from longitudinal variability, and the results obtained from applying any given analysis on each of the variabilities will differ as a consequence. Because variability is central to so many analyses, the non-equivalence of longitudinal and cross-sectional variances that results from a non-ergodic process explains why discussions of ergodicity often comment that "for non-ergodic processes, an analysis of the structure of IEV [interindividual variability] will yield results that differ from results obtained in an analogous analysis of IAV [intraindividual variability]"[@molenaar2004, p. 202].\footnote{It is important to note that a violation of one or both ergodic conditions (homogeneity and stationarity) does not mean that an analysis of cross-sectional variability yields results that have no relation to the results gained from applying the analysis on longitudinal variability (i.e., the causes of cross-sectional variability are independent from the causes of longitudinal variability). An analysis of cross-sectional variability can still give insight into temporal dynamics if the causes of non-ergodicity can be identified (ref:voelkle2014spector2019). Thus, conceptualizing ergodicity on a continuum with non-erdogicity and ergodicity on opposite ends provides a more balanced perspective for understanding ergodicity (ref:adolf2019medaglia2019).}

With an understanding of the conditions required for ergodicity, a brief consideration of organizational phenomena finds that these conditions are regularly violated. Focusing only on homogeneity (each person's data are generated from the same distribution), several instances in organizational research violate this condition. As examples of homogeneity violations, employees show different patterns of absenteeism over five years [@magee2016], leadership development over the course of a seminar [@day2011], career stress over the course of 10 years [@igic2017], and job performance in response to organizational restructuring [@miraglia2015]. With respect to stationarity (constant values for statistical parameters across people over time), several examples can be generated by realizing how calendar events affect psychological processes and behaviours throughout the year. As examples of stationarity violations, consider how salespeople, on average, undoubtedly sell more products during holidays, how employees, on average, take more sick days during the winter months, and how accountants, on average, experience more stress during tax season. With ergodic condition violations commonly occurring in organizational psychology, it becomes fitting to echo the commonly held sentiment that few, if any, psychological processes are ergodic [@molenaar2004; @molenaar2008; @molenaar2009; @fisher2018; @curran2011; @wang2015; @hamaker2012]. 


## Technical Appendix B: Using Nonlinear Function in the Structural Equation Modelling Framework 
### Nonlinear Latent Growth Curve Model Used to Analyze Each Generated Data Set{#structured-latent}

The sections that follow will first review the framework used to build
latent growth curve models and then explain how nonlinear functions can
be modified to fit into this framework.

#### Brief Review of the Latent Growth Curve Model Framework

(ref:meredith1990browne1993) [@meredith1990; @browne1993]

(ref:blozis2004) [@blozis2004]

The latent growth curve model proposed by @meredith1990
is briefly reviewed here [for a review, see @preacher2008]. Consider an example where data are collected at five time
points ($T = 5$) to yield five observations for each $p$ person
($\mathbf{y_p} = [y_1, y_2, y_3, y_4, y_5$). A simple model to fit is
one where change over is defined by a straight line and each person's
pattern of change is some variation of this straight line. In modelling
parlance, an intercept-slope model is fit where both the intercept and
slope are random effects whose values are allowed to vary for each
person. Intercept and slope parameters can be algebraically represented
by a two-column matrix that represents the effect of each parameter on
the outcome variable $y$ at each $i$ time point. Because the effect of
the intercept parameter is constant over time, a column of 1s is used to
represent its effect. For the slope parameter, a pattern of linear
growth can be specified filling the second column with a series of
monotonically increasing numbers such as
0--4.\footnote{The set of numbers specified for the slope starts at zero because there is presumably no effect of any variable at the first time point.}The matrix $\mathbf{\Uplambda}$ below shows a two-column matrix that
specifies the effects for an intercept and slope parameter:

$$ 
\mathbf{\Uplambda} = 
\begin{bmatrix}
1 & 0 \\ 
1 & 1 \\ 
1 & 2 \\ 
1 & 3 \\
1 & 4 \\
\end{bmatrix}
$$

\noindent To create a model that allows different linear patterns to be
fit to each person's data, a weight can be applied to each column of
$\mathbf{\Uplambda}$ and each weight can vary across individuals.\footnote{The columns of $\mathbf{\Uplambda}$ are called basis curves (ref:blozis2004) or basis functions (ref:meredith1990browne1993) because each column specifies a particular component of change.}That is, each $p$ person's pattern of change is predicted with a unique set of weights in $\mathbf{\upiota_p}$ that determines the extent to
which each basis column of $\mathbf{\Uplambda}$ contributes to that
person's change over time. Discrepancies between the values predicted by
$\mathbf{\Uplambda\upiota_p}$ and a person's observed scores across all
five time points are stored in an error vector
$\mathbf{\mathcal{E}_p}$. Thus, a person's observed data ($\mathbf{y_p}$) is
constructed using the expression shown below in Equation \ref{eq:sem-framework}:

```{=tex}
\begin{align}
 y_p = \mathbf{\Uplambda\upiota_p} + \mathbf{\mathcal{E}_p}.
 (\#eq:sem-framework)
\end{align}
```

\noindent Note that Equation \ref{eq:sem-framework} defines the general structural equation modelling framework. 

#### Fitting a Nonlinear Function in the Structural Equation Modelling Framework

Unfortunately, the logistic function of Equation
\ref{eq:logFunction-generation}---where each parameter was estimated as a
fixed- and random-effect---could not be directly used in a latent growth curve model because it would have violated the linear nature of the structural equation modelling framework (Equation \ref{eq:sem-framework}). Structural equation models only permit linear combinations---specifically, the
products of matrix-vector and/or matrix-matrix multiplication---and so
directly fitting a nonlinear function such as the logistic function in
Equation \ref{eq:logFunction-generation} would not have been possible.

One solution to fitting the logistic function within the structural equation modelling framework was to implement the structured latent curve modelling approach
[@browne1991; @browne1993; for an excellent review, see @preacher2015]. Briefly, the structured latent curve modelling approach constructs a Taylor series approximation of a nonlinear function so that the nonlinear function can be fit into the structural equation modelling framework (Equation \ref{eq:sem-framework}). The sections that follow will present the structured latent curve modelling approach in four parts such that 1) Taylor series approximations will first be reviewed, 2) a Taylor series approximation will then be constructed for the logistic function, 3) the logistic Taylor series approximation will be modified and fit into the structural equation modelling framework, and 4) the process of parameter estimation will be reviewed. 

##### Taylor Series Approximations

A Taylor series uses derivative information of a nonlinear function to
construct a linear approximation.\footnote{Linear functions are
defined as functions where no parameter exists within its own partial
derivative. For example, none of the parameters in the polynomial
equation of $y = a + bt + ct^2 + dt^3$ exist within their own partial
derivative: $\frac{\partial y}{\partial a} = 1$,
$\frac{\partial y}{\partial b} = t$,
$\frac{\partial y}{\partial c} = t^2$, and
$\frac{\partial y}{\partial d} = t^3$. Conversely, the logistic function
is nonlinear because $\upbeta$ and $\upgamma$ exist in their own
partial derivatives. For example, the derivative of the logistic function  $y = \uptheta + \frac{\upalpha - \uptheta}{1 + e^{\frac{\upbeta - t}{\upgamma}}} $with respect to $\upbeta$ is $\frac{(\uptheta - \upalpha) (e^{\frac{\upbeta - t}{\upgamma}})(\frac{1}{\upgamma})}{1 + (e^{\frac{\upbeta - t}{\upgamma}})^2}$and so is nonlinear because it contains $\upbeta$.}Equation \ref{eq:taylor} shows the general formula for a Taylor series such that

```{=tex}
\begin{align}
P^N(f(x), a)= \sum^{N}_{n = 0} \frac{f^na}{n !}(x-a)^n,
(\#eq:taylor)
\end{align}
```

\noindent where $N$ is the highest derivative order of the function $f(a)$ that is taken beginning from a zero-value derivative order ($n=0$), $a$ is the point where the Taylor series is derived, and $x$ is the point where the
Taylor series is evaluated. As an example, consider  $f(x) = \cos(x)$. Note that, across the continuum of $x$ values (i.e., from $-\infty$ to $\infty$), $\cos(x)$ returns values between -1 and 1 in an oscillatory manner. Computing the second-order Taylor series approximation of $f(x) = \cos(x)$ yields the following function shown in Equation \ref{eq:example-taylor}:

```{=tex}
\begin{align} 
P^2(\cos(x), a) &=  \frac{\frac{\partial^0 \cos(a)}{\partial a^0}}{0!}(x -a)^0 + \frac{\frac{\partial^1 \cos(a)}{\partial a^1}}{1!}(x -a)^1 + \frac{\frac{\partial^2 \cos(a)}{\partial a^2}}{2!} (x -a)^2 \nonumber \\ 
&=  \frac{\cos(0)}{0!}(x -0)^0 - \frac{\sin(0)}{1!}(x -0)^1 - \frac{\cos(0)}{2!}(x -0)^2  \nonumber \\ 
&=  \frac{1}{1}1 - \frac{0}{1}x - \frac{1}{2}x^2  \nonumber \\ 
P^2(\cos(x), 0) &=  1- \frac{1}{2}x^2. 
  (\#eq:example-taylor)
\end{align}
```
\noindent Note that that the second-order Taylor series of $\cos(x)$
perfectly estimates $\cos(x)$ when the point of evaluation $x$ is set
equal to the point of derivation $a$ and estimates $\cos(x)$ with an
increasing amount of error as the difference between $x$ and $a$
increases (see Example \ref{exm:taylor-estimates}).

```{example, taylor-estimates, echo=T}
Estimates of Taylor series approximation of $f(x) = \cos(x)$ as the difference between the point of evaluation $\mathrm{x}$ and the point of derivation $\mathrm{a}$ increases.

 \noindent \textup{Taylor series approximation of $f(x) = \cos(x)$ estimates values that are exactly equal to the values returned by $f(x) = \cos(x)$ when the point of evaluation \textit{x} is set to the point of derivation \textit{a}. The example below computes the value predicted by the Taylor series approximation of $f(x) = \cos(x)$ and by $f(x) = \cos(x)$ when \textit{x} = \textit{a} = 0.}

\useshortskip
\begin{align*}
P^2(\cos(x=0), a=0) &= \cos(x=0) \nonumber \\ 
1- \frac{1}{2}x^2 &=  \cos(0) \nonumber \\ 
1- \frac{1}{2}0^2 &=  1 \nonumber \\ 
1- 0 &=  1 \nonumber \\ 
1 &=  1 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

 \noindent \textup{Taylor series approximation of $f(x) = \cos(x)$ estimates a value that is clearly not equal ($neq$) to the value returned by $f(x) = \cos(x)$when the difference between the point of evaluation \textit{x} and the point of derivation \textit{a} is smaller. The example below computes the value predicted by the Taylor series approximation of $f(x) = \cos(x)$ and by $f(x) = \cos(x)$ when \textit{x} = 1 and  \textit{a} = 0.} 

\useshortskip
\begin{align*}
P^2(\cos(x = 1), 0) &\thickapprox \cos(x = 1) \nonumber \\ 
1- \frac{1}{2}x^2 &\thickapprox   \cos(1) \nonumber \\ 
1- \frac{1}{2}1^2 &\thickapprox   0.54 \nonumber \\ 
1- 0.5 &\thickapprox   0.54 \nonumber \\ 
0.5 &\thickapprox 0.54 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

 \noindent\textup{Taylor series approximation of $f(x) = \cos(x)$ estimates a value that is clearly not equal ($neq$) to the value returned by $f(x) = \cos(x)$ when the difference between the point of evaluation \textit{x} and the point of derivation \textit{a} is larger The example below computes the value predicted by the Taylor series approximation of $f(x) = \cos(x)$ and by $f(x) = \cos(x)$ when \textit{x} = 4 and  \textit{a} = 0.} 


\useshortskip
\begin{align*}
P^2(\cos(x = 4), 0) &\neq \cos(x = 4) \nonumber \\ 
1- \frac{1}{2}x^2 &\neq  \cos(4) \nonumber \\ 
1- \frac{1}{2}4^2 &\neq  -0.65 \nonumber \\ 
1- 16 &\neq  -0.65 \nonumber \\ 
0.5 &\neq  -0.65 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

\noindent \hrulefill
```

\noindent Figure \ref{fig:taylor-vs-nonlin} plots the nonlinear function
of $\cos(x)$ and its second-order Taylor series $P^2(\cos(x)) = 1- \frac{1}{2}x^2$. The
second order Taylor series perfectly estimates $\cos(x)$ when the point
of evaluation ($x$) equals the point of derivation ($a$; $x = a = 0$),
but incurs an increasingly large amount of error as the difference
between the point of evaluation and the point of derivation increases.
For example, at $x = 10$, $\cos(10) = -0.84$, but the Taylor series
outputs a value of -49.50 ($P^2(cos(50)) = 1- \frac{1}{2}10^2 = -49.50$). Therefore, Taylor series' are approximations because they are locally accurate.

```{r taylor-vs-nonlin, include=F, eval=F}

x <- seq(from = 0, to = 10, by = 0.1)
taylor_data <- 1- 0.5*(x)^2
cos_data <- cos(x)

combined_data <- data.frame('x' = x, Taylor = taylor_data, Cos = cos_data, check.names = F)

combined_data_long <- combined_data %>% 
  pivot_longer(cols = c('Cos' , 'Taylor'), names_to = 'curve_type', 
               names_ptypes = factor())

taylor_vs_nonlin_plot <- ggplot(data = combined_data_long, mapping = aes(x = x, y = value, group = curve_type, linetype = curve_type)) + 
  geom_line(size = 1) + 
  labs(linetype = 'Curve type') + 
  annotate(geom = 'text', label = 'P^2*(cos(x)) == 1 - frac(1, 2)*x^2', x = 5, y = -30, parse=T, size = 7) + 
  
  #expression(paste(P^{2}*(cos(x)), ', ', b == 6))

  annotate(geom = 'text', label = 'f(x) == cos(x)', x = 8, y = -10, parse=T, size = 7) + 
  
  geom_segment(inherit.aes = F, mapping = aes(x = 8, xend = 8, y = -9, yend = -1), 
               arrow = arrow(length = unit(0.3, 'cm')), size = 0.5) +
  
  geom_segment(inherit.aes = F, mapping = aes(x = 5, xend = 5, y = -28, yend = -13), 
               arrow = arrow(length = unit(0.3, 'cm')), size = 0.5) +
  scale_y_continuous(name = 'Curve value') + 
  scale_x_continuous(name = 'Point of evaluation (x)', breaks = seq(from = 0, to = 10, by =1)) + 
  theme_classic(base_family = 'Helvetica') + 
  theme(legend.text = element_text(size = 12, color = 'black'), 
        legend.title = element_text(size = 16, color = 'black'), 
        axis.text =element_text(size = 12, color = 'black'), 
        axis.title = element_text(size = 16, color = 'black')) 
  
ggsave(filename = 'Figures/taylor_vs_nonlin.pdf', plot =taylor_vs_nonlin_plot, width = 9, height = 6)


```

```{=tex}
\begin{figure}[H]
  \caption{Estimation Accuracy of Taylor Series Approximation of Nonlinear Function (cos(x))}
  \label{fig:taylor-vs-nonlin}
  \includegraphics{Figures/taylor_vs_nonlin} \hfill{}
  \caption*{Note. \textup{The second order Taylor series perfectly estimates $\cos(x)$ when the point of evaluation ($x$) equals the point of derivation ($a$; $x = a = 0$), but incurs an increasingly large amount of error as the difference between the point of evaluation and the point of derivation increases. For example, at $x = 10$, $\cos(x) = -0.84$, but the Taylor series outputs a value of -49.50 ($P^2(cos(50)) = 1- \frac{1}{2}10^2 = -49.50$). }}
\end{figure}
```

##### Taylor Series Approximation of the Logistic Function

Given that a Taylor series provides a linear approximation of a
nonlinear function and the structural equation modelling framework is linear, the structured latent curve modelling approach uses Taylor series approximations to construct linear representations of nonlinear functions [@browne1991; @browne1993]. In the current simulations, a Taylor series approximation was constructed for the logistic function (Equation \ref{eq:logistic}). Note that, because the logistic function had four parameters ($\uptheta$,
$\upalpha$, $\upbeta$, $\upgamma$), derivatives were computed with
respect to each of the parameters. Using a derivative order set to one
($n = 1$), the following Taylor series was constructed for the logistic
function (Equation \ref{eq:logistic-approx}):

```{=tex}
\begin{align}
 P^1(L(\Uptheta, t)) = L + \frac{\partial L}{\partial \uptheta}(x_{\uptheta}-a_{\uptheta})^1 + \frac{\partial L}{\partial \upalpha}(x_{\upalpha}-a_{\upalpha})^1 + \frac{\partial L}{\partial \upbeta}(x_{\upbeta}-a_{\upbeta})^1 + \frac{\partial L}{\partial \upgamma_{\upgamma}}(x-a_{\upgamma})^1, 
(\#eq:logistic-approx)
\end{align}
```
\noindent where $\mathbf{L(\Uptheta, t)}$ represents the logistic function shown below in
Equation \ref{eq:logistic}:

```{=tex}
\begin{align}
  \mathbf{L(\Uptheta, t)} = \uptheta + \frac{\upalpha - \uptheta}{{1 + e^\frac{\upbeta - t}{\upgamma}}} + \upepsilon, 
(\#eq:logistic)
\end{align}
```

\noindent with $\Uptheta = [\uptheta, \upalpha, \upbeta, \upgamma]$ and  $\mathbf{L(\Uptheta, t)}$ being a vector of scores at all $\mathbf{t}$ time points. In the current context, because each parameter of the logistic function had a unique meaning (see section on [data generation][Data generation]), the point of derivation $a$ differed for each parameter---using the same $a$ value for each parameter to construct the
Taylor series approximation of the logistic function would have yielded
a practically useless equation. Because the logistic Taylor series
approximation (Equation \ref{eq:logistic-approx}) was deployed in a
statistical model (i.e., the structural equation modelling framework), the derivation values
($a_{\uptheta}$, $a_{\upalpha}$, $a_{\upbeta}$, $a_{\upgamma}$) were set
to the mean values estimated by the analysis for each parameter. Thus, the
derivation values were replaced with the following terms:

-   $a_{\uptheta} = \hat{\uptheta}$
-   $a_{\upalpha} = \hat{\upalpha}$
-   $a_{\upbeta} = \hat{\upbeta}$
-   $a_{\upgamma} = \hat{\upgamma}$

\noindent where that a caret $\hat{}$ indicates the mean value estimated for a parameter by the analysis. In order to estimate curves for each $p$ person, the values of
evaluation ($x_{\uptheta}$, $x_{\upalpha}$, $x_{\upbeta}$,
$x_{\upgamma}$) corresponded to the parameter values computed for a
given person ($\uptheta_p$, $\upalpha_p$, $\upbeta_p$, $\upgamma_p$). Thus, the evaluation values were replaced with the following terms:

-   $x_{\uptheta} = \uptheta_p$
-   $x_{\upalpha} = \upalpha_p$
-   $x_{\upbeta} = \upbeta_p$
-   $x_{\upgamma} = \upgamma_p$

\noindent Substituting the above values for the derivation and evaluation values of $x$ and $a$ in the initial logistic Taylor series approximation (Equation \ref{eq:logistic-approx}) yielded the following expression for the logistic Taylor series approximation (Equation \ref{eq:taylor-full}):

```{=tex}
\begin{align}
 P^1(L(\Uptheta, t)) = L(\Uptheta, t) + \frac{\partial L}{\partial \uptheta}(\uptheta_i-\hat{\uptheta})^1 + \frac{\partial L}{\partial \upalpha}(\upalpha_i-\hat{\upalpha_i})^1 + \frac{\partial L}{\partial \upbeta}(\upbeta-\hat{\upbeta})^1 + \frac{\partial L}{\partial \upgamma_{\upgamma}}(\upbeta-\hat{\upbeta})^1.
(\#eq:taylor-full)
\end{align}
```

\noindent Therefore, because the Taylor series was derived using the mean values estimated for each parameter ($\hat{\uptheta}$, $\hat{\upalpha}$, $\hat{\upbeta}$,
$\hat{\upgamma}$), it provided a perfect approximation of the estimated
population curve---the evaluation values for each parameter would have
been set to their corresponding mean estimated value. To estimate the curve of any given $p$ person, the evaluation values could be offset from their corresponding derivation value (i.e., mean estimated value for a parameter) by using the set of parameter values computed for that person ($\uptheta_p$, $\upalpha_p$, $\upbeta_p$, $\upgamma_p$). Note that, because Taylor series approximations are only locally accurate, the predicted curves for any given $p$ person become increasingly inaccurate curves as the difference between the derivation and evaluation values increases (e.g., $\uptheta_i-\hat{\uptheta}$).

##### Fitting the Logistic Taylor Series Approximation Into the Structual Equation Modelling Framework 

Although the logistic Taylor series approximation provides an accurate
estimation of the logistic function, the function in (Equation
\ref{eq:taylor-full}) is modified in the structured latent curve
modelling approach so that it can more effectively fit into the structural equation modelling framework (Equation \ref{eq:sem-framework}). The partial derivative information is stored in the matrix $\mathbf{\Uplambda}$ such that

$$ 
\mathbf{\Uplambda} = 
\begin{bmatrix}
\frac{\partial L(\Uptheta, t_1)}{\partial \uptheta} & \frac{\partial L(\Uptheta, t_1)}{\partial \upalpha}  &  \frac{\partial L(\Uptheta, t_1)}{\partial \upbeta} & \frac{\partial L(\Uptheta, t_1)}{\partial \upgamma}   \\ 
\frac{\partial L(\Uptheta, t_2)}{\partial \uptheta}  & \frac{\partial L(\Uptheta, t_2)}{\partial \upalpha} &  \frac{\partial L(\Uptheta, t_2)}{\partial \upbeta} & \frac{\partial L(\Uptheta, t_2)}{\partial \upgamma} & \\ 
\vdots & \vdots & \vdots & \vdots \\ 
\frac{\partial L(\Uptheta, t_n)}{\partial \uptheta} & \frac{\partial L(\Uptheta, t_n)}{\partial \upalpha}  & \frac{\partial L(\Uptheta, t_n)}{\partial \upbeta} & \frac{\partial L(\Uptheta, t_n)}{\partial \upgamma} \\
\end{bmatrix}.
$$

\noindent As in the structural equation modelling framework where each column of
$\mathbf{\Uplambda}$ specified a basis curve (i.e., a loading of a
growth parameter onto all time points), each column of $\mathbf{\Uplambda}$ here
in the structured latent curve modelling approach contains the loadings
of a logistic function parameter onto all the $n$ time points, with the loadings being determined by the partial derivative of logistic function with respect to that parameter. To predict unique curves for each person, each column can be multiplied by a specific weight $\mathbf{\upiota_p}$ that contains person-specific deviations from each mean estimated parameter value as shown below:

$$ 
\mathbf{\upiota_p} = 
\begin{bmatrix}
\hat{\uptheta} - \uptheta_p   \\ 
\hat{\upalpha} - \upalpha_p   \\ 
\hat{\upbeta} - \upbeta_p \\ 
\hat{\upgamma_i} - \upgamma_p \\
\end{bmatrix},
$$ 


\noindent where a caret ($\hat{}$) indicates the mean value estimated
for a given parameter and a subscript $p$ indicates a parameter value
computed for a person. With a matrix $\mathbf{\Uplambda}$ containing
logistic function parameter loadings and a vector $\mathbf{\upiota_p}$ containing
person-specific weights, the Taylor series of Equation
\ref{eq:taylor-full} that predicted a person's scores over time can be
rewritten to become the following expression of Equation
\ref{eq:slcm-nonsem}:

```{=tex}
\begin{align}
 \mathbf{y_p} = \mathbf{L(\Uptheta, t)} + \mathbf{\Uplambda\upiota_p} + \mathbf{\mathcal{E}_p}.
 (\#eq:slcm-nonsem)
\end{align}
```

\noindent Importantly, because of the logistic function ($\mathbf{L(\Uptheta, t)}$) in the
above expression (Equation \ref{eq:slcm-nonsem}), the model no longer
fits into the general structural equation modelling framework (Equation \ref{eq:sem-framework}). To modify Equation \ref{eq:slcm-nonsem} such that it fits into the structural equation modelling framework, the structured latent curve modelling approach recognizes that the logistic function ($\mathbf{L(\Uptheta, t)}$) is invariant under a scaling constant and uses this property to rewrite  $\mathbf{L(\Uptheta, t)}$ as a weighted sum of the partial derivative loading matrix [$\mathbf{\Uplambda}$\; @shapiro1987]. Briefly, the
logistic function vector $\mathbf{L(\Uptheta, t)}$ is invariant under a constant scaling
property because, given some constant scalar value $k \ge 0$ and a set
of parameter values ($\Uptheta$), there exists another set of parameter
values ($\tilde{\Uptheta}$) that can produce the same values (see
Equation \ref{eq:icsf} and Example \ref{exm:icsf-ex} below).

```{=tex}
\begin{align}
 k\mathbf{L(\Uptheta, t)} = \mathbf{L(\tilde{\Uptheta}, t)}
 (\#eq:icsf)
\end{align}
```

```{example, icsf-ex, echo=T}
Invariability under a constant scaling factor of logistic function (Equation \ref{eq:logistic}).  

\noindent \textup{Given $t = [0, 1, 2, 3]$, $\Uptheta = [\uptheta = 3.00$, $\upalpha = 3.32$, $\upbeta = 180.00$, $\upgamma = 20.00$], and some constant scaling factor $k = 2.00$, then there exists some set of parameter values $\tilde{\Uptheta}$ that produces the same values as $kL(\Uptheta)$. In the current example, $\tilde{\Uptheta} = [\uptheta = 6.00$, $\upalpha = 6.64$, $\upbeta = 180.00$, $\upgamma = 20.00$].} 

\useshortskip
\begin{align*}
\mathbf{kL(\Uptheta, t)} &= \mathbf{L(\tilde{\Uptheta}, t)} \nonumber \\ 
2*[3.00, 3.02, 3.30, 3.32] &=  [6.00, 6.04, 6.60, 6.64] \nonumber \\ 
[6.00, 6.04, 6.60, 6.64]  &= [6.00, 6.04, 6.60, 6.64] 1 \nonumber \\ 
\end{align*}
\useshortskip
\vspace*{-25mm}

\noindent \hrulefill
```

\noindent If a function has the property of being invariant under a
scaling factor, then it can also be expressed as the following
matrix-vector product shown in Equation \ref{eq:logistic-matrix-vector}
[@shapiro1987]:

\begin{align}
 \mathbf{L(\Uptheta, t)} = \mathbf{\Lambda\uptau},
(\#eq:logistic-matrix-vector)
\end{align}

\noindent where $\mathbf{\Uplambda}$ contains the partial derivative loadings\footnote{This is also known as a Jacobian matrix.} and
$\mathbf{\uptau}$ is a vector whose values are otbained by pre-multiplying the output of the logistic function ($\mathbf{L(\Uptheta, t)}$) by the inverse of the partial derivative loading matrix ${\Lambda\uptau}^{-1}$. Solving for $\mathbf{\uptau}$ yields a vector whose contents contain the mean values estimated for parameters that enter the logistic function in a linear way and
zeroes for parameters that enter the function in a nonlinear way (i.e.,
parameters that exist within their own partial derivative). Hence, $\mathbf{\uptau}$ is often called a mean vector [@blozis2004; @preacher2015].  In the current example, $\uptheta$ and $\upalpha$ enter the logistic function
in a linear way and $\upbeta$ and $\upgamma$ enter the logistic function
in a nonlinear way and so the first two entries of $\mathbf{\uptau}$
contain the values estimated for $\uptheta$ and $\upalpha$ (i.e.,
$\hat{\uptheta}$ and $\hat{\upalpha}$) and the last two entries contain zeroes. Example \ref{exm:tau-vector} below shows that the first two
values of $\mathbf{\uptau}$ are indeed the values estimated for
$\uptheta$ and $\upalpha$ and the last two values are zero.

```{example, tau-vector, echo=T}
Computation of mean vector $\mathbf{\uptau}$. 
  
 \noindent \textup{Given the parameter estimates of $\hat{\uptheta} = 3.00$, $\hat{\upalpha} = 3.32$, $\hat{\upbeta} = 180.00$, and $\hat{\upgamma} = 20.00$ and $\mathbf{t}$ = [0, 1, 2, 3], $\mathbf{\uptau}$ = [3.00, 3.32, 0, 0], then } 

\useshortskip
\begin{align*}
\mathbf{L(\Uptheta, t)} &= \mathbf{\Lambda\uptau} \\ 
[3.00, 3.02, 3.30, 3.32] &= \begin{bmatrix}
1.00 & 0.00 & 0.00  & 0.00 \\ 
0.95  & 0.05 & -0.00 & 0.00 \\ 
0.05 & 0.95 & -0.00 & -0.00 \\ 
0.00 & 1.00  & 0.00 & 0.00 \\
\end{bmatrix} \mathbf{\uptau} \\ 
\begin{bmatrix}
1.00 & 0.00 & 0.00  & 0.00 \\ 
0.95  & 0.05 & -0.00 & 0.00 \\ 
0.05 & 0.95 & -0.00 & -0.00 \\ 
0.00 & 1.00  & 0.00 & 0.00 \\
\end{bmatrix}^{-1}
\begin{bmatrix} 
3.00 \\ 3.02 \\ 3.30 \\ 3.32
\end{bmatrix} &=  \mathbf{\Lambda\uptau} \\ 
 \mathbf{\uptau} &= [3.00, 3.32, 0, 0]\\
\end{align*}
\vspace*{-25mm}

\noindent \hrulefill
```

\noindent With $\mathbf{L(\Uptheta, t)} = \mathbf{\Uplambda\uptau}$, Equation \ref{eq:slcm-nonsem} can be rewritten in a linear equation as shown below in Equation \ref{eq:taylor-linear}:

\begin{align}
 \mathbf{y_p} = \mathbf{\Uplambda\uptau} + \mathbf{\Uplambda\upiota_p} + \mathbf{\mathcal{E}_p}.
 (\#eq:taylor-linear)
 \end{align}
 
\noindent The mean vector $\mathbf{\uptau}$ and vector of
person-specific deviations $\mathbf{\upiota_p}$ can be combined into a
new vector $\mathbf{s_p}$ that represents the person-specific weights
applied to the basis curves in $\mathbf{\Uplambda}$ such that

$$  
\mathbf{s_p} = \mathbf{\uptau + \upiota_p} =
\begin{bmatrix} 
\hat{\uptheta} + \hat{\uptheta} - \uptheta_p \\ 
\hat{\upalpha} + \hat{\upalpha} - \upalpha_p \\ 
0 + \hat{\upbeta} - \upbeta_p \\ 
0 + \hat{\upgamma} - \upgamma_p \\ 
\end{bmatrix}
$$

\noindent and 

\begin{align}
\mathbf{y_p} = \mathbf{\Uplambda s_p} + \mathbf{\mathcal{E}_p}. 
(\#eq:taylor-final)
\end{align}

\noindent Because the expected value of the person-specific weights
($\mathbf{s_p}$) is the mean vector ($\mathbf{\uptau}$;
$\mathbb{E}[{\mathbf{s_p}}] = \mathbf{\uptau}$, the expected set
of scores predicted across all people ($\mathbb{E}[{\mathbf{y_p}}]$) gives back the original expression for the logistic function matrix-vector product in Equation
\ref{eq:logistic-matrix-vector} as shown below in Equation \ref{eq:expected-value}:

\begin{align}
 \mathbb{E}[{\mathbf{y_p}}] = \mathbf{\Uplambda\uptau} = \mathbf{L(\Uptheta, t)}. 
(\#eq:expected-value)
\end{align}

\noindent Therefore, the structured latent curve modelling approach
successfully reproduces the output of the nonlinear logistic function
(Equation \ref{eq:logistic}) with the linear function of Equation
\ref{eq:taylor-final}. Note that that no error term exists in Equation \ref{eq:expected-value} because the expected value of the error
values is zero ($\mathbb{E}[{\mathbf{\mathcal{E}_p}}] = 0$).

##### Estimating Parameters in the Structured Latent Curve Modelling Approach 

To estimate parameter values, the full-information maximum
likelihood shown in Equation \ref{eq:fiml-person} was computed for each
person (i.e., likelihood of observing a $p$ person's data given the
estimated parameter values):

\begin{align}
\mathcal{L}_p = k_p \ln(2\pi) + \ln(|\mathbf{\Sigma_p}| + (\mathbf{y_p} - \mathbf{\upmu_p})^\top \mathbf{\Sigma_p}^{-1}(\mathbf{y_p} - \mathbf{\upmu_p}),
(\#eq:fiml-person)
\end{align}


\noindent where $k_p$ is the number of non-missing values for a given
$p$ person, $\mathbf{\Sigma_p}$ is the model-implied covariance matrix
with rows and columns filtered at time points where person $p$ has
missing data, $\mathbf{y_p}$ is a vector containing the data points that
were collected for a $p$ person (i.e., filtered data), and
$\mathbf{\upmu_p}$ is the model-implied mean vector that is filtered at
time points where person $p$ has missing data. Note that, because all
simulations assumed complete data across all times points, no filtering
procedures were executed [for a review of the filtering procedure, see @boker2020, Chapter 5]. Thus, computing the above full-information
maximum likelihood in Equation \ref{eq:fiml-person} was equivalent to
computing the below likelihood function in Equation
\ref{eq:ml-estimation}:

\begin{align}
\mathcal{L}_p = k_p \ln(2\pi) + \ln(|\mathbf{\Sigma}| + (\mathbf{y_p} - \mathbf{\upmu})^\top \mathbf{\Sigma}^{-1}(\mathbf{y_p} - \mathbf{\upmu}),  
(\#eq:ml-estimation)
\end{align}

\noindent where $\mathbf{\Sigma}$ is the model-implied covariance matrix,
$\mathbf{y_p}$ contains the data collected from a $p$ person, and
$\mathbf{\upmu}$ is the model-implied mean vector. The model-implied
covariance matrix $\mathbf{\Sigma}$ is computed using Equation
\ref{eq:covariance} below:

\begin{align}
\mathbf{\Sigma} = \mathbf{\Uplambda\Uppsi\Uplambda} + \mathbf{\Upomega}_{\mathcal{E}},   
(\#eq:covariance)
\end{align}

\noindent where $\mathbf{\Uppsi}$ is the random-effect covariance matrix
and $\mathbf{\Upomega}_{\mathcal{E}}$ contains the error variances at
each time point. The mean vector $\mathbf{\upmu}$ was computed using
Equation \ref{eq:mean-structure} shown below:

\begin{align}
\mathbf{\upmu} = \mathbf{\Uplambda\uptau}. 
(\#eq:mean-structure)
\end{align}

\noindent Parameter estimation was conducted by finding values for the model-implied
covariance matrix $\mathbf{\Sigma}$ and the model-implied mean vector
$\mathbf{\upmu}$ that maximized the sum of log-likelihoods across all $P$ people
(see Equation \ref{eq:max-ll} below):

\begin{align}
\mathcal{L} = \underset{\mathbf{\Sigma},\mathbf{\upmu} }{\argmax} \sum^P_{p = 1} \mathcal{L}_p.
(\#eq:max-ll)
\end{align}

\noindent In OpenMx, the above problem was solved using the sequential
least squares quadratic program [for a review, see @kraft1994].








