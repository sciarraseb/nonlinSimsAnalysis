---
shorttitle        : "Measurement timing"
format          : "pandoc"
header-includes:
  - \usepackage{nccmath}
  - \usepackage{caption}
  - \usepackage{textcomp} #for copyright symbol on title page
  - \usepackage{longtable}
  - \usepackage{setspace}
  - \usepackage{biblatex}
  - \usepackage{booktabs}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \usepackage{xcolor}
  - \usepackage{amsthm} 
  - \usepackage{amsmath} ##needed for argmax
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \usepackage{setspace} #needed to doublespace caption text (using \doublespacing)
  - \usepackage[labelfont = {bf, up}]{caption} 
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \usepackage{upgreek}  #required for non-italicized Greek letters
  - \usepackage{subcaption}
  - \captionsetup[figure]{labelfont={normalfont, bf}, singlelinecheck=false, labelsep=newline}
  - \DeclareCaptionJustification{double}{\DoubleSpacing}
  - \DeclareCaptionFont{figCaptionFont}{\fontfamily{phv}} #sets caption font to sans serif font of Helvetica 
  - \DeclareCaptionFont{figCaptionSize}{\footnotesize} #set caption font size to footnote 
  - \DeclareCaptionFont{figCaptionStyle}{\textup}  #set caption font to non-italicized font  
  - \DeclareCaptionLabelSeparator{captionSep}{\newline\newline} #separates figure label and figure title with required white space
  - \captionsetup[figure]{labelfont={figCaptionStyle, bf}, font = {figCaptionFont,figCaptionSize, figCaptionStyle}, labelsep = captionSep, justification=raggedright}
  - \captionsetup[table]{font = {figCaptionFont,figCaptionSize,figCaptionStyle}, labelfont={bf}, labelsep=captionSep, justification = raggedright, margin = {0cm,0cm}}
  - \newenvironment{helvenv}{\fontfamily{phv}\selectfont}{}
  - \raggedbottom #ensures text starts from top of page and any white space is at the botom

#environment numbering 
  - \setcounter{section}{0} 
  - \makeatletter \renewcommand\thesection{} \renewcommand\thesubsection{\@arabic\c@section.\@arabic\c@subsection} \makeatother
  
  - \newtheorem{theorem}{Theorem}
  - \newtheorem{example}[theorem]{Example}
  

  #modifies heading levels of 4-5 to follow apa7
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

  
author: 
  - name          : "Sebastian Sciarra "
    affiliation   : "1"
    corresponding : yes    
    email         : "ssciarra@uoguelph.ca"
affiliation: 
  - id            : "1"
    institution   : "University of Guelph"
keywords          : "measurement timing, nonlinear "
wordcount         : "5554 words"
floatsintext      : yes
linkcolor         : blue
figsintext        : yes 
figurelist        : no
tablelist         : no
footnotelist      : no
numbersections    : yes
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa7"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: 72
bibliography: dissertation_references.bib
---

```{r package_loading, include=F}
#load packages
library(easypackages)
packages <- c('devtools','tidyverse', 'RColorBrewer', 'parallel', 'data.table', 'kableExtra', 'ggtext', 'egg', 'nonlinSims','papaja', 
              'ggbrace', 'cowplot')
libraries(packages)
load_all()
```

```{r knitting_setup, echo=F, message = F, warning = F}
#import raw data files (needed for computing variances)
exp_1_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_1_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "measurement_spacing", "midpoint"), factor)
  
exp_2_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_2_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "measurement_spacing", "sample_size"), factor)

exp_3_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_3_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "time_structuredness", "sample_size"), factor)

#unfiltered data 
param_summary_exp_1 <- readRDS(file = 'data/uf_param_summary_exp_1.RData')
param_summary_exp_2 <- readRDS(file = 'data/uf_param_summary_exp_2.RData')
param_summary_exp_3 <- readRDS(file = 'data/uf_param_summary_exp_3.RData')

#create analytical versions of summary data + converts vars to sds
exp_1_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_1, exp_num = 1)
exp_2_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_2, exp_num = 2)
exp_3_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_3, exp_num = 3)

combined_analytical_exp_1 <- rbind(exp_1_analytical$likert, exp_1_analytical$days)
combined_analytical_exp_2 <- rbind(exp_2_analytical$likert, exp_2_analytical$days)
combined_analytical_exp_3 <- rbind(exp_3_analytical$likert, exp_3_analytical$days)

#create condition summary data sets 
cond_summary_exp_1 <- compute_condition_summary(param_summary_data = combined_analytical_exp_1, facet_var = 'measurement_spacing', 
                          ind_vars = c('number_measurements', 'measurement_spacing', 'midpoint'))
cond_summary_exp_2 <- compute_condition_summary(param_summary_data = combined_analytical_exp_2, facet_var = 'measurement_spacing', 
                  ind_vars = c('number_measurements', 'measurement_spacing', 'sample_size'))

cond_summary_exp_3 <- compute_condition_summary(param_summary_data = combined_analytical_exp_3, facet_var = 'time_structuredness', 
                          ind_vars = c('number_measurements', 'sample_size', 'time_structuredness'))
```


```{r pre_knitting_setup_unfiltered, echo=F, eval=F, include=F}
#code should be computed before knitting to decrease knitting time 
#load data from experiments
exp_1 <- read_csv(file = 'data/exp_1_data.csv')
exp_2 <- read_csv(file = 'data/exp_2_data.csv')
exp_3 <- read_csv(file = 'data/exp_3_data.csv')

#compute parameter summary statistics  
param_summary_exp_1 <- compute_parameter_summary(data = exp_1, exp_num = 1)
param_summary_exp_2 <- compute_parameter_summary(data = exp_2, exp_num = 2)
param_summary_exp_3 <- compute_parameter_summary(data = exp_3, exp_num = 3)

#necessary factor conversions 
param_summary_exp_1$number_measurements <- factor(param_summary_exp_1$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_1$midpoint <- factor(param_summary_exp_1$midpoint, levels = c(80, 180,280))

param_summary_exp_2$number_measurements <- factor(param_summary_exp_2$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_2$sample_size <- factor(param_summary_exp_2$sample_size, levels = c(30, 50, 100, 200, 500, 1000))

param_summary_exp_3$number_measurements <- factor(param_summary_exp_3$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_3$sample_size <- factor(param_summary_exp_3$sample_size, levels = c(30, 50, 100, 200, 500, 1000))

#write data sets 
#save parameter summary files as RData files so that metadata are correctly stored (e.g., factor levels, variable types)
saveRDS(object = param_summary_exp_1, file = 'data/uf_param_summary_exp_1.RData')
saveRDS(object = param_summary_exp_2, file = 'data/uf_param_summary_exp_2.RData')
saveRDS(object = param_summary_exp_3, file = 'data/uf_param_summary_exp_3.RData')
```

```{r pre_knitting_setup, echo=F, eval=F}
#code should be computed before knitting to decrease knitting time 
#load data from experiments
exp_1 <- read_csv(file = 'data/exp_1_data.csv')  
exp_2 <- read_csv(file = 'data/exp_2_data.csv')
exp_3 <- read_csv(file = 'data/exp_3_data.csv')

#data cleaning procedure for each data set
#filter out values for each parameter in each condition 
exp_1_filtered <- remove_outliers(data = exp_1)
exp_2_filtered <- remove_outliers(data = exp_2)
exp_3_filtered <- remove_outliers(data = exp_3)
  
#convert variance values to SD values for random effect parameters 
exp_1_cleaned <- convert_var_to_sd(param_summary_datadata = exp_1_filtered)
exp_2_cleaned <- convert_var_to_sd(data = exp_2_filtered)
exp_3_cleaned <- convert_var_to_sd(data = exp_3_filtered)

#compute summary values 
param_summary_exp_1 <- compute_parameter_summary(data = exp_1_cleaned, exp_num = 1)
param_summary_exp_2 <- compute_parameter_summary(data = exp_2_cleaned, exp_num = 2)
param_summary_exp_3 <- compute_parameter_summary(data = exp_3_cleaned, exp_num = 3)

#write data sets 
#save parameter summary files as RData files so that metadata are correctly stored (e.g., factor levels, variable types)
saveRDS(object = param_summary_exp_1, file = 'data/param_summary_exp_1.RData')
saveRDS(object = param_summary_exp_2, file = 'data/param_summary_exp_2.RData')
saveRDS(object = param_summary_exp_3, file = 'data/param_summary_exp_3.RData')
```



```{r obsolete_metadata_storing_importing, eval=F, include=F}
##store classes of each column 
col_types_exp_1 <- unlist(lapply(X = param_summary_exp_1, FUN = class))
col_types_exp_2 <- unlist(lapply(X = param_summary_exp_2, FUN = class))
col_types_exp_3 <- unlist(lapply(X = param_summary_exp_3, FUN = class))

#create data.frame that stores classes of data columns 
col_types_df <- data.frame('exp_num' = c(rep(x = 1, times = length(col_types_exp_1)),
                                         rep(x = 2, times = length(col_types_exp_2)),
                                         rep(x = 3, times = length(col_types_exp_3))), 
                           'variable_types' = c(col_types_exp_1, col_types_exp_2, col_types_exp_3))

#create data fame that stores levels of factor variables 
exp_1_factor_levels <- extract_factor_levels(param_summary_data = param_summary_exp_1)
exp_2_factor_levels <- extract_factor_levels(param_summary_data = param_summary_exp_2)
exp_3_factor_levels <- extract_factor_levels(param_summary_data = param_summary_exp_3)

##store classes of each column 
col_types_exp_1 <- unlist(lapply(X = param_summary_exp_1, FUN = class))
col_types_exp_2 <- unlist(lapply(X = param_summary_exp_2, FUN = class))
col_types_exp_3 <- unlist(lapply(X = param_summary_exp_3, FUN = class))

#create data.frame that stores classes of data columns 
col_types_df <- data.frame('exp_num' = c(rep(x = 1, times = length(col_types_exp_1)),
                                         rep(x = 2, times = length(col_types_exp_2)),
                                         rep(x = 3, times = length(col_types_exp_3))), 
                           'variable_types' = c(col_types_exp_1, col_types_exp_2, col_types_exp_3))

#create data fame that stores levels of factor variables 
exp_1_factor_levels <- extract_factor_levels(param_summary_data = param_summary_exp_1)
exp_2_factor_levels <- extract_factor_levels(param_summary_data = param_summary_exp_2)
exp_3_factor_levels <- extract_factor_levels(param_summary_data = param_summary_exp_3)

#write meta data + data for factor levels 
write_csv(x = col_types_df, file = 'data/col_types.csv')
write_csv(x = exp_1_factor_levels, file = 'data/exp_1_factor_levels.csv')
write_csv(x = exp_2_factor_levels, file = 'data/exp_2_factor_levels.csv')
write_csv(x = exp_3_factor_levels, file = 'data/exp_3_factor_levels.csv')

#read in .csv files with experimental data already in summarized version 
col_types_df <-read_csv(file = 'data/col_types.csv')
exp_1_factor_levels <- read_csv(file = 'data/exp_1_factor_levels.csv')
exp_2_factor_levels <- read_csv(file = 'data/exp_2_factor_levels.csv')
exp_3_factor_levels <- read_csv(file = 'data/exp_3_factor_levels.csv')

#relevel factors columns 
param_summary_exp_1 <- relevel_factors(param_summary_data = param_summary_exp_1, factor_levels_df = exp_1_factor_levels)
param_summary_exp_2 <- relevel_factors(param_summary_data = param_summary_exp_2, factor_levels_df = exp_2_factor_levels)
param_summary_exp_3 <- relevel_factors(param_summary_data = param_summary_exp_3, factor_levels_df = exp_3_factor_levels)
```


```{=tex}
\begin{titlepage}
  \begin{center}
    \vspace*{3cm}
    
  \textbf{Is Timing Everything? Measurement Timing and the Ability to Sccurately Model Longitudinal Data}
    
  \vspace{2cm} 
    by \\ Sebastian Sciarra 
    
      
  \vspace{2cm} 
   In partial fulfilment of requirements \\ 
   for the degree of \\
   Doctor of Philosophy \\ 
   in \\ 
   Psychology
    

  \vspace{2cm} 
    Guelph, Ontario, Canada \\ 
    \textcopyright \text{ Sebastian Sciarra, September 2022}

  
  \end{center}
\end{titlepage}
```

```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# Introduction

```{=tex}
\nointerlineskip
\vfill
\newpage
```

"Neither the behavior of human beings nor the activities of organizations can be defined without reference to time, and temporal aspects are critical for understanding them" [@navarro2015, p.136].

The topic of time has received a considerable amount of
attention in organizational psychology over the past 20 years. Examples
of well-received articles published around the beginning of the 21^st^
century have discussed how investigating time is important for
understanding patterns of change and boundary conditions of theory
[@zaheer1999], how longitudinal research is necessary for disentangling
different types of causality [@mitchell2001], and explicated a pattern
of organizational change [or institutionalization\; @lawrence2001].
Since then, articles have emphasized the need to address time in
specific areas such as performance [@fisher2008; @dalal2014], teams [@roe2012], and goal setting [@fried2004] and, more generally, throughout organizational research [@george2000; @roe2008; @ployhart2010; @sonnentag2012; @navarro2015; @shipp2015; @kunisch2017; @vantilborgh2018; @aguinis2021]. 

The importance of time has also been recognized in organizational theory. In defining a theoretical contribution, @whetten1989 discussed that time must be discussed in regard to setting boundary conditions (i.e., under what circumstances does the theory apply) and in specifying relations between variables over time [see also @mitchell2001; @george2000]. Even if a considerable number of organizational theories do not adhere to the definition of @whetten1989, theoretical models in organizational psychology consist of path diagrams that delineate the causal underpinnings of a process. Given that temporal precedence is a necessary condition for establishing causality [@mill2011], time has a role, whether implicitly or explicitly, in organizational theory. 

(ref:maxwell2007) @maxwell2007
(ref:roe2014) @roe2014
(ref:mitchell2013) @mitchell2013

Despite the considerable emphasis that has been placed on investigating processes over time and its ubiquity in organizational theory, the prevalence of longitudinal research has historically remained low. One study examined the prevalence of longitudinal research from 1970--2006 in five organizational psychology journals and found that 4% of articles used longitudinal designs (Roe, 2014). Another survey of two applied psychology journals in 2005 found that 9.5% (10 of 105 studies) of studies used longitudinal designs [@roe2008]. Similarly, two surveys of studies employing longitudinal designs with mediation analysis found that, across five journals, only 9.7% (7 of 72 studies) did so in 2005 [@maxwell2007] and 16.3% (15 of 92 studies) did so in 2006 [@mitchell2013].\footnote{Note that the definition of a longitudinal design in (ref:maxwell2007) and (ref:mitchell2013) required that measurements be taken over at least three time points so that measurements of the predictor, mediator, and outcome variables were separated over time.} Thus, the prevalence of longitudinal research has remained low. 

The six sections that follow will explain why longitudinal research is necessary and the factors that must be considered in conducting such research. The first section will provide an explanation for why conducting longitudinal research essential for understanding temporal dynamics. The second section will provide an overview of patterns of change that are likely to emerge over time. The third section will provide an overview of the issues involved in designing longitudinal studies. The fourth section issue will provide an overview of how the effects of longitudinal issues on modelling accuracy can be investigated. The fifth section will provide a systematic review of the research that has investigated issues involved in conducting longitudinal research, and the sixth section will briefly explain methods of modelling nonlinear change. A summary of the three simulation experiments that were conducted in my dissertation will then be provided. 

## The Need to Conduct Longitudinal Research

Longitudinal research must be conducted because the results of cross-sectional analyses may not agree with the results of longitudinal analyses. Unfortunately, researchers commonly discuss the results of cross-sectional analyses as if they have been obtained with a longitudinal design. One fitting example of the assumption that cross-sectional findings are equivalent to longitudinal findings comes from the large number of studies employing mediation analysis. Given that mediation is used to understand chains of causality in psychological processes [@baron1986], it would thus make sense to pair mediation analysis with a longitudinal design since understanding causality, after all, requires temporal precedence. Unfortunately, the majority of studies that have used mediation analysis have done so using cross-sectional designs---with estimates of 90.3% [@maxwell2007] and 83.7% [@mitchell2013]---and have often discussed the results as if they were longitudinal. Investigations into whether mediation results remain equivalent across cross-sectional and longitudinal designs have repeatedly concluded that using mediation analysis on cross-sectional data can return different, and sometimes completely opposite, results from using it on longitudinal data [@cole2003; @maxwell2007; @maxwell2011; @mitchell2013; @olaughlin2018]. Therefore, as a default, results from mediation analyses on cross-sectional and longitudinal data should be treated as non-equivalent. 

The non-equivalence of cross-sectional and longitudinal results that occurs with mediation analysis is, unfortunately, not due to a specific set of
circumstances that only arise with mediation analysis, but a consequence of a broader systematic cause that affects the results of almost every
analysis. To understand why cross-sectional and longitudinal analyses seldom yield similar results, it is first important to realize that variance is central to many statistical analyses---correlation, regression, factor analysis, and mediation are some examples. Thus, if variance remains unchanged across cross-sectional and longitudinal data sets, then analyses of either data set would return the same results. Importantly, variance only remains equal across cross-sectional and longitudinal data sets if two conditions put forth by ergodic theory are satisfied [@molenaar2004; @molenaar2009]. If these two conditions are met, then a process is said to be **ergodic**: Variance remains unchanged across cross-sectional and longitudinal data sets (for a detailed overview of ergodicity, see [Technical Appendix A][Technical Appendix A: Ergodicity and the Need to Conduct Longitudinal Research]). Unfortunately, the two conditions required for ergodicity are highly unlikely to be satisfied and so cross-sectional findings will often deviate from longitudinal findings. 

Given that cross-sectional and longitudinal analyses are, in general, unlikely to return equivalent findings, it is unsurprising that several investigations in organizational research---and psychology as a whole---have found these analyses to return different results. Beginning with an example from @curran2011, heart attacks are less likely to occur in people who exercise regularly (longitudinal finding), but more likely to happen when exercising (cross-sectional finding). Moving on to the simple analysis of correlation, studies find differences in correlation magnitudes between cross-sectional and longitudinal data sets [for meta-analytic example, see @nixon2011; @fisher2018].\footnote{Note that (ref:fisher2018) also found the variability of longitudinal correlations to be considerably larger than the variability of cross-sectional correlations.} Moving on to perhaps the most commonly employed analysis in organizational research of mediation, several articles have highlighted how mediation analysis on cross-sectional data can return different, and sometimes completely opposite, results when applied on longitudinal data [@cole2003; @maxwell2007; @maxwell2011; @olaughlin2018]. As one final example, when ergodicity (an equivalence between cross-sectional and longitudinal findings) has been examined with factor analysis, perhaps the most interesting result has emerged: The well-documented five-factor model of personality seldom arises when analyzing person-level data that was obtained by measuring personality on 90 consecutive days [@hamaker2005]. Therefore, it appears that cross-sectional analyses may rarely return the same results as longitudinal analyses. 

If cross-sectional results seldom match longitudinal results, then an understanding of temporal dynamics can only be obtained by conducting longitudinal research. Fortunately, technological advancements have allowed researchers to more easily conduct longitudinal research in two ways. First, the use of the experience sampling method [@beal2015] in conjunction with modern information transmission technologies---whether through phone applications or short message services---allows data to sometimes be sampled over time with relative ease. Second, the development of analyses for longitudinal data (along with their integration in commonly used software) that enable person-level data to be modelled such as multilevel models [@raudenbush2002], growth mixture models [@wang2007], and dynamic factor analysis [@ram2013] provide researchers with avenues to explore the temporal dynamics of psychological processes. With one recent survey estimating that 43.3% of mediation studies (26 of 60 studies) used a longitudinal design [@olaughlin2018], it appears that the prevalence of longitudinal research has increased from the 9.5% [@roe2008] and 16.3% [@mitchell2013] values estimated at the beginning of the 21^st^ century. Although the frequency of longitudinal research appears to have increased over the past 20 years, several avenues exist where the quality of longitudinal research can be improved, and my dissertation focuses on investigating how these avenues.  

## Understanding Patterns of Change That Emerge Over Time

Change can occur in many ways over time. One pattern of change commonly assumed to occur over time is that of linear change. When change follows a linear pattern, the rate of change over time remains constant. Unfortunately, a linear pattern places demanding restrictions on possible patterns of change. If change were to follow a linear pattern, then any pauses in change (or plateaus) or changes in direction would not occur and effects would simply grow over time. Unfortunately, effect sizes have been shown to diminish over time [for meta-analytic examples, see @cohen1993; @griffeth2000; @hom1992; @riketta2008; @steel1984; @steel1990]. Moreover, many variables display cyclic patterns of change over time, with mood [@larsen1990], daily stress [@bodenmann2010], and daily drinking behaviour [@huh2015] as some examples. Therefore, change over is unlikely to follow a linear pattern.

A more realistic pattern of change to occur over time is a nonlinear pattern [for a review, see @cudeck2007]. Nonlinear change allows nonconstant rates of change such that change may occur more rapidly during certain periods of time, stop altogether, or reverse direction. When looking at patterns of change observed across psychology, examples appear in the declining rate of speech errors throughout child development [@burchinal1991], forgetting rates in memory [@murre2015], development of habits over time [@fournier2017], and the formation of opinions over time [@xia2020]. Given nonlinear change appears more likely than linear change, my dissertation will assume change over time to be nonlinear. 

## Challenges Involved in Conducting Longitudinal Research

(ref:podsakoff2003ostroff2002) [@podsakoff2003; for an example, see @ostroff2002]

Conducting longitudinal research presents researchers with several challenges. When conducting longitudinal research, many challenges encountered in conducting cross-sectional research are amplified [for a review, see @bergman1993].\footnote{It should be noted that conducting a longitudinal study does alleviate some issues encountered in conducting cross-sectional research. For example, taking measurements over multiple time points likely reduces common method variance (ref:podsakoff2003ostroff2002).} As some examples, greater efforts have to be made to to prevent the compounding effects over time of attrition on missing data [@newman2008; @dillman2014] and the adverse effects of well-documented biases such as demand characteristics [@orne1962] and social desirability [@nederhof1985] have to be countered at each time point. Outside of amplifying the challenges associated with conducting cross-sectional research, conducting longitudinal research also presents new challenges. Analyses of longitudinal data have to consider complications such as how to model error structures [@grimm2010a], check for measurement non-invariance over time [the extent to which a construct is measured with equivalent accuracy over time\; @vandeschoot2012], and how to center data to appropriately answer research questions [@enders2007; @wang2015]. 

Although researchers must contend with several issues in conducting longitudinal research, three issues are of particular interest in my dissertation. The first issue concerns how many measurements to use in a longitudinal design. The second issue concerns how to space the measurements. The third issue focuses on how much error is incurred if the time structuredness of the data is overlooked. The sections that follow will review each of these issues. 

### Number of Measurements

Researchers have to decide on the number of measurements to include in a longitudinal study. Although using more measurements increases the accuracy of results---as noted in the results of several studies [e.g., @coulombe2016; @timmons2015; @finch2017; @fine2019]---taking additional measurements often comes at a cost that a researcher may be unable account for with a limited budget. One important point to mention is that a researcher designing a longitudinal study must take at least three measurements for to obtain a reliable estimate of change and, perhaps more importantly, to allow a nonlinear pattern of change to be modelled [@ployhart2010].

### Spacing of Measurements

Additionally, a researcher must decide on the spacing of measurements in a longitudinal study. Although discussions of measurement spacing often recommend that researchers use theory and previous studies to implement measurement spacings that  [@mitchell2001; @cole2003; @collins2006; @dormann2014, @dormann2015], organizational theories seldom delineate a period of time over which a process unfolds, and so the majority of longitudinal research uses intervals of convention and/or convenience to space measurements [@mitchell2001; @dormann2014]. Unfortunately, using measurement spacing lengths that do not account for the temporal pattern of change of a psychological process can lead to inaccurate results [e.g., @chen2014]. As an example, @cole2009 provide show how correlation magnitudes are affected by the choice of measurement spacing intervals. 

### Time Structuredness

Last, and perhaps most pernicious, analyses of longitudinal data are likely to incur error from an assumption they make about data collection conditions. Many analyses assume that, across all collection points, participants provide their data at the same time. Unfortunately, such a high level of regularity in the response patterns of participants is unlikely: Participants are more likely to provide their data over some period of time after a data collection window has opened. As an example, consider a study that collects data from participants at the beginning of each month. If participants respond with perfect regularity, then they would all provide their data at the exact same time (e.g., noon on the second day of each month). If the participants respond with imperfect regularity, then they would provide their at different times after the beginning of each month. The regularity of responding observed across participants in a longitudinal study determines the time structuredness of the data and the sections that follow will provide overview of time structuredness. 

#### Time-Structured Data

(ref:mehta2005mehta2000) [@mehta2005; @mehta2000]

Many analyses assume that data are *time structured*: Participants provide data at the same time at each collection point. By assuming time-structured data, an analysis can incur error because it will map time intervals of inappropriate lengths onto the time intervals that occurred between participant's responses.\footnote{It should be noted that, although seldom implemented, analyses can be accessorized to handle time-unstructured data by using definition variables (ref:mehta2005mehta2000).} As an example of the consequences of incorrectly assuming data to be time structured, consider a study that assessed the effects of an intervention on the development of leadership by collecting leadership ratings at four time points each separated by four weeks [@day2011]. The employed analysis assumed time-structured data; that is, each each participant provided ratings on the same day---more specifically, the exact same moment---each time these ratings were collected. Unfortunately, it is unlikely that the data collected from participants were time structured: At any given collection point, some participants may have provided leadership ratings at the beginning of the week, while others may only provide ratings two weeks after the survey opened. Importantly, ratings provided two weeks after the survey opened were likely influenced by changes in leadership that occurred over the two weeks. If an analysis incorrectly assumes time-structured data, then it assumes each participant has the same response rate and, therefore, will incorrectly attribute the amount of time that elapses between most participants' responses. For instance, if a participant only provides a leadership rating two weeks after having received a survey (and six weeks after providing their previous rating), then using an analysis that assumes time-structured data would incorrectly assume that each collection point of this participant is separated by four weeks (the interval used in the experiment) and would, consequently, model the observed change as if it had occurred over four weeks. Therefore, incorrectly assuming data to be time structured leads an analysis to overlook the unique response rates of participants across the collection points and, as a consequence, incur error [@mehta2000; @mehta2005; @coulombe2016]. 

#### Time-Unstructured Data

Conversely, some analyses assume that data are *time unstructured*: Participants provide data at different times at each collection point. Given the unlikelihood of one response pattern describing the response rates of all participants in a given study, the data
obtained in a study are unlikely to be time structured. Instead, and because participants are likely to exhibit unique response
patterns in their response rates, data are likely to be time unstructured. One way to conceptualize the distinction between time-structured and time-unstructured data is on a continuum. On one end of the continuum, participants all provide data with identical response patterns, thus giving time-structured data. When participants show unique response patterns, the resulting data are time unstructured, with the extent of time-unstructuredness depending on the length of the response windows. For example, if data are collected at the beginning of each month and participants only have one day to provide data at each time, then, assuming a unique response rate for each participant, the resulting data will have a low amount of time unstructuredness. Alternatively, if data are collected at the beginning of each month and participants have 30 days to provide data each time, then, assuming a unique response rate for each participant, the resulting data will have a high amount of time unstructuredness. Therefore, the continuum of time struturedness has time-structured data on one end and time-unstructured data with long response rates on another end.  

### Summary

In summary, researchers must contend with several issues when conducting longitudinal research. In addition to contending with issues encountered in conducting cross-sectional research, researchers must contend with new issues that arise from conducting longitudinal research. Three issues of particular importance in my dissertation are the number of measurements, the spacing of measurements, and incorrectly assuming data to be time structured. 


## How are the Effects of Longitudinal Issues on Modelling Accuracy Investigated?

In the next section, I will present the results of the systematic review of the literature that has investigated the issues of measurement number, measurement spacing, and time structuredness. Before presenting the results of the systematic review, I will provide an overview of the method used to investigate issues involved in conducting longitudinal research. Correspondingly, the paragraphs that follow will provide an overview of the Monte Carlo method and contrast it with the empirical method. 

To understand how the effects of longitudinal issues on modelling accuracy can be investigated, the empirical method commonly employed in psychological research will first be reviewed with an emphasis on its shortcomings (see Figure \ref{fig:MonteCarlo-comparison}). Consider an example where a researcher
wants to estimate a population mean ($\upmu$) and understand how sampling error affects the accuracy of the estimate. Using the empirical method, the researcher samples data and then estimates the population mean ($\upmu$) by computing the mean of the sampled data. Because collected samples are almost always contaminated by a variety of methodological and/or statistical deficiencies (such as sampling error, measurement error, assumption violations, etc.), the estimation of the population parameter is likely to be imperfect. Unfortunately, to estimate the effect of sampling error on the accuracy of the population mean estimate ($\upmu$), the researcher would need to know the value of the population mean; without knowing the value of the population mean, it is impossible to know how much error was incurred in estimating the population mean and, as as a result, impossible to know the extent to which sampling error contributed to this error. Therefore, although a study following the empirical approach can provide estimates of population parameters, it cannot estimate how much error is introduced by a methodological or statistical deficiency because the population parameter value is almost always unknown.

The Monte Carlo method solves the shortcomings of the empirical method by setting the value of a population parameter. Figure \ref{fig:MonteCarlo-comparison} shows that the Monte Carlo method works in the opposite direction of the empirical approach: Instead of collecting a sample, the Monte Carlo method begins by assigning a value to at least one parameter to define a population. Many sample data sets are then generated from the defined population, with some methodological deficiency built in to each data set. In the current example, each data set is generated to have a specific amount of missing data. Each generated sample is then analyzed and the population estimates of each statistical model are averaged and compared to the pre-determined parameter value.\footnote{A statistical deficiency can also be introduced in the analysis of each generated data set.} The difference between the average of the estimates and the known population value constitutes bias in parameter estimation (i.e., parameter bias). In the current example, the missing data manipulation causes a systematic underestimation, on average, of the population parameter. By randomly generating data, the Monte Carlo method can determine how a variety of methodological and statistical factors affect the accuracy of a model [for a review, see @robert2010].

```{=tex}
\begin{figure}[H]
  \caption{Depiction of Monte Carlo Method}
  \label{fig:MonteCarlo-comparison}
  \includegraphics{Figures/Monte_Carlo_comparison} \hfill{}
    \caption*{Note. \textup{Comparison of empirical approach with the Monte Carlo approach. The empirical approach begins with a collected sample and then estimates the population parameter using an appropriate statistical model. The difference between the estimated and population value can be conceptualized as error. Because the population value is generally unknown in the empirical approach, it cannot estimate how much error is introduced by any given methodological or statistical deficiency. To estimate how much error is introduced by any given methodological or statistical deficiency, the Monte Carlo method needs to be used, which constitutes four steps. The Monte Carlo method first defines a population by setting parameter values. Second, many samples are generated from the pre-defined population, with some methodological deficiency built in to each data set (in this case, each sample has a specific amount of missing data). Third, each generated sample is then analyzed and the population estimates of each statistical model are averaged and compared to the pre-determined parameter value. Fourth, the difference between the estimate average and the known population value defines the extent to which the missing data manipulation affected parameter estimation (the difference between the population and average estimated population value is the parameter bias).}}
\end{figure}
```

Monte Carlo simulations have been used to evaluate a variety of methodological and statistical deficiencies. Beginning with the simple bivariate correlation, Monte Carlo simulations have shown that realistic values of sample size and measurement accuracy produce considerable variability in estimated correlation values [@stanley2014]. Monte Carlo simulations have also provided valuable insights into more complicated statistical analyses. In investigating more complex statistical analyses, simulations have shown that mediation analyses are biased to produce results of complete mediation because the statistical power to detect direct effects falls well below the statistical power to detect indirect effects [@kenny2014]. Finally, as an example of the utility of Monte Carlo simulations for evaluating growth mixture models, Monte Carlo simulations have shown that class enumeration accuracy (the ability to identify the correct number of response groups) decreases with nonnormal data [@bauer2003]. Given the ability of the Monte Carlo method to evaluate statistical methods, the   experiments in my dissertation used it to evaluate the effects of measurement number, measurement spacing, and time structuredness on modelling accuracy.\footnote{My simulation experiments also investigated the effects of sample size and nature of change on modelling accuracy.} 


## Systematic Review of Simulation Literature

To understand the extent to which issues involved in conducting longitudinal research had been investigated, I conducted a systematic review of the simulation literature. The sections that follow will first present the method I followed in systematically reviewing the literature and then summarize the findings of the review. 

### Systematic Review Methodology 

I identified the following keywords through citation searching and independent reading: "growth curve", "time-structured analysis", "time structure", "temporal design", "individual measurement occasions", "measurement intervals", "methods of timing", "longitudinal data analysis", "individually-varying time points", "measurement timing", "latent difference score models", "parameter bias", and "measurement spacing". I entered these keywords entered into the PsycINFO database (on July 23, 2021) and any paper that contained any one of these key words and the word "simulation" in any field was considered a viable paper (see Figure \@ref(fig:prismaDiagram) for information about filtering of the reports). The search returned 165 reports, which I screened by reading the abstracts. Initial screening led to the removal of 60 reports because they did not contain any simulation experiments. Of the remaining 105 papers, I removed 2 more popers  because they could not accessed [@stockdale2007; @tiberio2008]. Of the remaining 103 identified simulation studies, I deemed a paper as relevant if it investigated the effects of any design and/or analysis factor relating to conducting longitudinal research (i.e., number of measurements, spacing of measurements, and/or time structuredness) and did so using the Monte Carlo simulation method. Of the remaining 103 studies, I removed 89 studies being removed because they did not meet the inclusion criteria, leaving fourteen studies to be included the review, with. I also found an additional 3 studies through citation searching, giving a total of 17 studies. 

(ref:errorStructures) [@gasimova2014; @liu2021; @liu2015; @miller2017; @murphy2011]
(ref:fine2019) [@fine2019]
(ref:fine2019fine2020) [@fine2019; @fine2020]
(ref:fine2019text) @fine2019
(ref:fine2020text) @fine2020
(ref:coulombe2016miller2017) [and from previous simulation experiments of @coulombe2016; @miller2017]

The findings of my systematic review are summarized in Tables \@ref(tab:systematicReviewCount)--\@ref(tab:systematicReview). Tables \@ref(tab:systematicReviewCount)--\@ref(tab:systematicReview) differ in one way: Table \@ref(tab:systematicReviewCount) indicates how many studies investigated each effect, whereas Table \@ref(tab:systematicReview) provides the reference of each study and detailed information about each study's method. Otherwise, all other details of Tables \@ref(tab:systematicReviewCount)--\@ref(tab:systematicReview) are identical. The first column lists the longitudinal design factor (alongside with sample size) and the corresponding two- and three-way interactions. The second and third columns list whether each effect has been investigated with linear and nonlinear patterns of change, respectively. Shaded cells indicate effects that have not been investigated, with cells shaded in light blue indicating effects that have not been investigated with linear patterns of change and cells shaded in dark blue indicating effects that have not been investigated with nonlinear patterns of change.\footnote{Table \ref{tab:systematicReview} lists the effects that each study (identified by my systematic review) investigated and notes the following methodological details (using superscript letters and symbols): the type
of model used in each paper, assumption and/or manipulation of complex error structures
(heterogeneous variances and/or correlated residuals), manipulation of missing data,
and/or pseudo-time structuredness manipulation. Across all 17 simulation studies, 5 studies (29\%) assumed complex error structures (ref:errorStructures), 1 study (6\%) manipulated missing data (ref:fine2019), and 2 studies (12\%) contained a pseudo-time structuredness manipulation (ref:fine2019fine2020) . Importantly, the pseudo-time structuredness manipulation used in (ref:fine2019text) and (ref:fine2020text) differed from the manipulation of time
structuredness used in the current experiments (ref:coulombe2016miller2017) in that it randomly generated longitudinal data such that a given person could provide all their data before another person provided any data.}


```{=tex}
\begin{figure}[H]
  \caption{PRISMA Diagram Showing Study Filtering Strategy}
  \label{fig:prismaDiagram}
  \includegraphics{Figures/prisma_diagram} \hfill{}
    \caption*{Note. \textup{PRISMA diagram for systematic review of simulation research that investigates measurement timing}}
\end{figure}
```

### Systematic Review Results 

Although the current research appeared to sufficiently fill the cells of Table \@ref(tab:systematicReviewCount), two patterns suggest that arguably the most important cells (or effects) have not been investigated. First, it appears that simulation research has invested more effort in investigating the effects of longitudinal design factors with linear patterns than with nonlinear patterns of change. In counting the number of effects that remain unaddressed with linear and nonlinear patterns of change, a total of five cells (or effects) have not been


```{r systematicReviewCount, echo=F}
#table_1 <-  data.frame('Effect' = c('\\textbf{Main effects}', 'Number of measurements (NM)', 'Spacing of measurements (SM)', 'Time structuredness #(TS)', 'Sample size (S)', 
#                                    '\\textbf{Two-way interactions}', 'NM x SM', 'NM x TS', 'NM x S', 'SM x TS', 'SM x S', 'TS x S',
#                                    '\\textbf{Three-way interactions}', 'NM x SM x TS', 'NM x SM x S', 'NM x TS x S', 'SM x TS x S'), 
#               'Linear pattern' = c('', '11 studies', '1 study', '2 studies','11 studies', '', '1 study',  '1 study', '9 studies', '\\textbf{Cell #2}', 
#                                    '\\textbf{Cell 4}', '1 study', '', '\\textbf{Cell 6}', '\\textbf{Cell 8}', 
#                                    '1 study', ' \\textbf{Cell 11}'),
#               'Nonlinear pattern' = c('', '7 studies', '1 study', '2 studies', '7 studies', '', '1 study', ' \\textbf{Cell 1}', '5 studies', 
#                                       ' \\textbf{Cell 3}', ' \\textbf{Cell 5}' , '2 studies', '', ' \\textbf{Cell 7}',
#                                       ' \\textbf{Cell 9}', ' \\textbf{Cell 10}', '\\textbf{Cell 12}'), check.names = F)

table_1 <-  data.frame('Effect' = c('\\textbf{Main effects}', 'Number of measurements (NM)', 'Spacing of measurements (SM)', 'Time structuredness (TS)', 'Sample size (S)', 
                                    '\\textbf{Two-way interactions}', 'NM x SM', 'NM x TS', 'NM x S', 'SM x TS', 'SM x S', 'TS x S',
                                    '\\textbf{Three-way interactions}', 'NM x SM x TS', 'NM x SM x S', 'NM x TS x S', 'SM x TS x S'), 
               'Linear pattern' = c('', '11 studies', '1 study', '2 studies','11 studies', '', '1 study',  '1 study', '9 studies', '\\textbf{Cell 2}', 
                                    '\\textbf{Cell 4}', '1 study', '', '\\textbf{Cell 6}', '\\textbf{Cell 8}', 
                                    '1 study', ' \\textbf{Cell 11}'),
               'Nonlinear pattern' = c('', '6 studies', '1 study', '1 study', '7 studies', '', '1 study', ' \\textbf{Cell 1 (\\hyperref[Exp3]{Exp. 3})}', '5 studies', 
                                       ' \\textbf{Cell 3}', ' \\textbf{Cell 5 (\\hyperref[Exp2]{Exp. 2})}' , '2 studies', '', ' \\textbf{Cell 7}',
                                       ' \\textbf{Cell 9 (\\hyperref[Exp2]{Exp. 2})}', ' \\textbf{Cell 10 (\\hyperref[Exp3]{Exp. 3})}', '\\textbf{Cell 12}'), check.names = F)

kbl(x = table_1, booktabs = TRUE, format = 'latex', longtable = TRUE, 
    linesep = c('\\cmidrule{1-3}',
        rep(' ', times = 3), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}', 
        rep(' ', times = 5), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}',
        rep(' ', times = 3)), 
    align = c('l', 'c', 'c'), 
    caption = 'Number of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\\textit{n} = 17)', 
    escape=F) %>%
   column_spec(2, background = c(rep('white', times = 9), 
                                 rep('#acddfa', times = 1), 
                                 '#acddfa',
                                 'white', 'white',
                                 rep('#acddfa', times = 1), 
                                 rep('#acddfa', times = 1),
                                 'white',
                                 '#acddfa'), 
              width = '9cm') %>%
  column_spec(3, background = c(rep('white', times = 7), 
                                '#9fc5e8',
                                'white', 
                                '#9fc5e8',
                                '#9fc5e8',
                                'white', 'white', 
                                 rep('#9fc5e8', times = 1), 
                                 rep('#9fc5e8', times = 2),
                                 '#9fc5e8'),  
              width = '9cm') %>% 
   kable_styling(latex_options= c('hold_position', 'repeat_header'), font_size = 10, position = 'left') %>%
  footnote(general = 'Cells are only numbered for effects that have not been investigated (Cell 5 is the only exception). Cells shaded in light blue indicate effects that have not been investigated for linear patterns of change and cells shaded in light green indicate effects that have not been investigated for nonlinear patterns of change.', general_title = '\\\\textit{Note.\\\\hspace{0pc}}', footnote_as_chunk = T, escape = F, threeparttable = T) %>%
  landscape(margin = '1cm')
```

(ref:Coulombe2016) @coulombe2016

(ref:Timmons2015) @timmons2015

(ref:ORourke2021) @orourke2021

(ref:Miller2017) @miller2017

(ref:Liu2020) @liu2019

(ref:Liu2021) @liu2021

(ref:Fine2019) @fine2019

(ref:Wu2017) @wu2017

(ref:Finch2017) @finch2017

(ref:Coulombe2016b) @coulombe2016b

(ref:Newsom2020) @newsom2020

(ref:Fine2020) @fine2020

(ref:Wu2014) @wu2014

(ref:Ye2016) @ye2016

(ref:Gasimova2014) @gasimova2014

(ref:Murphy2011) @murphy2011

(ref:Aydin2014) @aydin2014

(ref:Liu2015) @liu2015

```{r systematicReview, echo=F}
table_1 <-  data.frame(
  'Effect' = c('\\textbf{Main effects}', 'Number of measurements (NM)', 'Spacing of measurements (SM)', 'Time structuredness (TS)', 'Sample size (S)', 
               '\\textbf{Two-way interactions}', 'NM x SM', 'NM x TS', 'NM x S', 'SM x TS', 'SM x S', 'TS x S',
               '\\textbf{Three-way interactions}', 'NM x SM x TS', 'NM x SM x S', 'NM x TS x S', 'SM x TS x S'), 
                       
'Linear pattern' = c('', 
                     '(ref:Timmons2015)$^{a}$; (ref:Murphy2011)$^{{\\mho}b}$; (ref:Gasimova2014)$^{{\\mho}c}$; (ref:Wu2014)$^{a}$; (ref:Coulombe2016b)$^{a}$;(ref:Ye2016)$^{a}$; (ref:Finch2017)$^{a}$; (ref:ORourke2021)$^{d}$; (ref:Newsom2020)$^{a}$; (ref:Coulombe2016)$^{a}$', 
                     '(ref:Timmons2015)$^{a}$', 
                     '(ref:Aydin2014)$^{a}$; (ref:Coulombe2016)$^{a}$',
                     '(ref:Murphy2011)$^{\\mho}{b}$; (ref:Gasimova2014)$^{{\\mho}c}$; (ref:Wu2014)$^{a}$; (ref:Coulombe2016b)$^{a}$;(ref:Ye2016)$^{a}$; (ref:Finch2017)$^{a}$; (ref:ORourke2021)$^{d}$; (ref:Newsom2020)$^{a}$; (ref:Coulombe2016)$^{a}$;(ref:Aydin2014)$^{a}$; (ref:Coulombe2016)$^{a}$', 
                     '', 
                     '(ref:Timmons2015)$^{a}$',  
                     '(ref:Coulombe2016)$^{a}$', 
                     '(ref:Murphy2011)$^{{\\mho}b}$; (ref:Gasimova2014)$^{{\\mho}c}$; (ref:Wu2014)$^{a}$; (ref:Coulombe2016b)$^{a}$;(ref:Ye2016)$^{a}$; (ref:Finch2017)$^{a}$; (ref:ORourke2021)$^{d}$; (ref:Newsom2020)$^{a}$; (ref:Coulombe2016)$^{a}$', 
                     '\\textbf{Cell 2}', 
                     '\\textbf{Cell 4}', 
                     '(ref:Aydin2014)$^{a}$', 
                     '', 
                     '\\textbf{Cell 6}', 
                     '\\textbf{Cell 8}', 
                     '(ref:Coulombe2016)$^{a}$', 
                     '\\textbf{Cell 11}'),

'Nonlinear pattern' = c('', 
                        '(ref:Timmons2015)$^{a}$; (ref:Finch2017)$^{a}$; (ref:Fine2019)$^{e{\\circ}{\\triangledown}}$; (ref:Fine2020)$^{e,f{\\triangledown}}$;(ref:Liu2020)$^{g}$; (ref:Liu2021)$^{{\\mho}h}$; (ref:Liu2015)$^{{\\mho}g}$', 
                        '(ref:Timmons2015)$^{a}$', 
                        '(ref:Miller2017)$^{{\\mho}a}$; (ref:Liu2015)$^{{\\mho}g}$', 
                        '(ref:Finch2017)$^{a}$; (ref:Fine2019)$^e{{\\circ}{\\triangledown}}$; (ref:Fine2020)$^{e,f{\\triangledown}}$;(ref:Liu2020)$^{g}$; (ref:Liu2021)$^{{\\mho}h}$; (ref:Liu2015)$^{{\\mho}g}$; (ref:Miller2017)$^{{\\mho}a}$', 
                        '', 
                        '(ref:Timmons2015)$^{a}$', 
                        '\\textbf{Cell 1}', 
                        '(ref:Finch2017)$^{a}$; (ref:Fine2019)$^{e{\\circ}{\\triangledown}}$; (ref:Fine2020)$^{e,f{\\triangledown}}$;(ref:Liu2020)$^{g}$; (ref:Liu2021)$^{{\\mho}h}$',
                        '\\textbf{Cell 3}', 
                        '\\textbf{Cell 5}' , 
                        '(ref:Liu2015)$^{{\\mho}g}$; (ref:Miller2017)$^{{\\mho}a}$', 
                        '', 
                        '\\textbf{\\centering{\\arraybackslash{Cell 7}}}', 
                        '\\textbf{Cell 9}', 
                        '\\textbf{Cell 10}', 
                        '\\textbf{Cell 12}'), check.names = F)

kbl(x = table_1, booktabs = TRUE, format = 'latex', longtable = TRUE, 
  linesep = c('\\cmidrule{1-3}',
        rep(' ', times = 3), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}', 
        rep(' ', times = 5), '\\addlinespace\\addlinespace\\cmidrule{1-3}', '\\cmidrule{1-3}',
        rep(' ', times = 3)), 
  align = c('l', 'c', 'c'), 
  caption = 'Summary of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\\textit{n} = 17)', 
  escape=F) %>%

  column_spec(2, background = c(rep('white', times = 9), 
                                 rep('#acddfa', times = 1), 
                                 '#acddfa',
                                 'white', 'white',
                                 rep('#acddfa', times = 1), 
                                 rep('#acddfa', times = 1),
                                 'white',
                                 '#acddfa'), 
              width = '9cm') %>%
  column_spec(3, background = c(rep('white', times = 7), 
                                '#9fc5e8',
                                'white', 
                                '#9fc5e8',
                                '#9fc5e8',
                                'white', 'white', 
                                 rep('#9fc5e8', times = 1), 
                                 rep('#9fc5e8', times = 2),
                                 '#9fc5e8'),  
              width = '9cm',  bold = ifelse(grepl(pattern = '^\\d+', x = table_1$Nonlinear),T, F)) %>% 
  
  kable_styling(latex_options= c('hold_position', 'repeat_header'), font_size = 10, position = 'left') %>%
  footnote(general = 'Cells are only numbered for effects that have not been investigated (Cell 5 is the only exception). Cells shaded in light blue indicate effects that have not been investigated for linear patterns of change and cells shaded in dark blue indicate effects that have not been investigated for nonlinear patterns of change.', 
           general_title = '\\\\textit{Note.\\\\hspace{-1.2pc}}', footnote_as_chunk = T, symbol_title = '\\\\newline', 
           alphabet_title = '\\\\newline', escape = F, threeparttable = T, 
           alphabet = c('Used latent growth curve model.', 
                        'Used second-order latent growth curve model.',
                        'Used hierarchical Bayesian model.',
                        'Used bivariate latent change score model', 
                        'Used functional mixed-effects model.',
                        'Used nonlinear mixed-effects model.',
                        'Used bilinear spline model.', 
                        'Used parallel bilinear spline model.'), 
           symbol_manual = c('$\\\\circ$', '$\\\\mho$', '$\\\\triangledown$'), 
           symbol = c('Manipulated missing data.', 'Assumed complex error structure (heterogeneous variances and/or correlated residuals).', 
                      'Contained pseudo-time structuredness manipulation.')) %>%
  landscape(margin = '1cm')
```

\noindent investigated with linear patterns of change, but a total of seven cells have not been investigated with nonlinear patterns of change. Given that change over time is more likely to follow a nonlinear than a linear pattern [for a review, see @cudeck2007], it could be argued that most simulation research has investigated the effect of longitudinal design factors under unrealistic conditions. Second, all the cells corresponding to the three-way interactions with nonlinear patterns of change had not been investigated (cells 7, 9, 10, and 12 of Table \ref{tab:systematicReviewCount}), meaning that almost no study had conducted a comprehensive investigation into longitudinal issues. Therefore, no simulation study has comprehensively investigated longitudinal issues under on modelling nonlinear patterns of change. 

### Next Steps

Given that longitudinal research is needed to understand the temporal dynamics of psychological processes, it is necessary to understand how longitudinal design and analysis factors interact with each other (and with sample size) in affecting the accuracy with which nonlinear patterns of change are modelled. With no study having conducted a comprehensive investigation of how longitudinal design and analysis factors affect the modelling of nonlinear change patterns, my simulation experiments were designed to address this gap in the literature. Specifically, my simulation experiments investigated how measurement number, measurement spacing, and time structuredness affected the accuracy with which a nonlinear change pattern was modelled (see Cells 1, 5, 9, and 10 of Table \ref{tab:systematicReviewCount}). 

## Methods of Modelling Nonlinear Patterns of Change Over Time

Because my simulation experiments assumed change over time to be nonlinear, it is important to provide an overview of how nonlinear change was modelled. In this section, I will provide a brief review on how nonlinear change can be modelled and will contrast the commonly employed polynomial approach with the lesser known nonlinear function approach. 

```{r nonlinear-plot-code, echo=F, include=F}
#regress outcome_value on time using the nonlinear function

nonlin_data <- read_csv(file = 'data/nonlin_data.csv')

nonlin_output <- round(data.frame(summary(nls(
  formula = obs_score ~ SSfpl(input = measurement_day, A = theta, B = alpha, xmid = beta, scal = gamma), 
  data = nonlin_data, 
  control = nls.control(maxiter = 100)))$coefficients), digits = 3)

nonlin_output$parameter <- rownames(nonlin_output)

#extract coefficient values
theta <- as.numeric(nonlin_output$Estimate[nonlin_output$parameter =='theta'])
alpha <- round(nonlin_output$Estimate[nonlin_output$parameter == 'alpha'], 2)
beta <- round(nonlin_output$Estimate[nonlin_output$parameter == 'beta'])
gamma <- round(nonlin_output$Estimate[nonlin_output$parameter == 'gamma'])

#AIC_nonlin <- formatC(round(AIC(nls(formula = outcome_value ~
#SSdlf(time = time, asym = alpha, a2 = theta, xmid = beta, scal = gamma),
#data = pos_responders)), digits = 2), format = 'f', digits = 2)
```

```{r polynomial-vs-nonlinear-plot, echo=F, include=F}
#create function to roun to two decimal places
round_two_decimals <- function(number) {
  
  rounded_number <- as.numeric(formatC(round(number, digits = 2), format = 'f', digits = 3))
  return(rounded_number)
}

nonlin_data <- read_csv(file = 'data/nonlin_data.csv')

#regress outcome_value on time using the linear function
polynomial_output <- data.frame(summary(nls(
  formula = obs_score ~ a + b*measurement_day + c*measurement_day^2 + d*measurement_day^3, 
  data = nonlin_data,
  start = list(a = 1, b = 1, c = 1, d = 0.5)))$coefficients)

polynomial_output$parameter <- rownames(polynomial_output)

#extract coefficient values & AIC value
a <- 3.09 #round(polynomial_output$Estimate[polynomial_output$parameter == 'a'], 2)
b <- -0.0018 #round(polynomial_output$Estimate[polynomial_output$parameter == 'b'], 4)
c <- 2.02e-05 #round(polynomial_output$Estimate[polynomial_output$parameter == 'c'], 5)
d <- -3.54e-08 #round(polynomial_output$Estimate[polynomial_output$parameter == 'd'], 8)

#AIC_polynomial <- formatC(round(AIC(nls(
#  formula = obs_score ~ a + b*measurement_day + c*measurement_day^2 + d*measurement_day^3, 
#  data = nonlin_data,
#  start = list(a = 1, b = 1, c = 1, d = 0.5))), 2), format = 'f', digits = 3)

measurement_day <- seq(from = 1, to = 360, by = 1)

poly_nonlin_pred_scores <- data.frame('measurement_day' = measurement_day, 
                                      'pred_score' = a + b*measurement_day + c*measurement_day^2 + d*measurement_day^3)


font_size <- 15
title_font <- 45
axis_text_size <- 30
axis_title_size <- 40

poly_pred_plot <- ggplot(poly_nonlin_pred_scores, aes(x = measurement_day, y = pred_score)) +
  geom_line(size = 2) +
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(limits = c(2.9, 3.5), breaks = seq(from = 3, to = 3.5, by = 0.25)) +
  scale_x_continuous(breaks = seq(from = 0, to = 360, by = 60), limits = c(0, 360)) +
  labs(x = 'Day', y = 'Predicted value', size = 16) +
  ggtitle(label = 'A: Response pattern predicted \n by polynomial (linear) function') +
  annotate(geom = 'text', x = 100, y = 3.45, label = 'y == italic(a) + italic(b)*x + italic(c)*x^2 + italic(d)*x^3',
           parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 45, y = 3.35, label = paste('italic(a) == ', a), parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 55, y = 3.30, label = paste('italic(b) == ', b), parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 63, y = 3.25, label = paste('italic(c) == ', c), parse = T, family = 'Helvetica', size = font_size) +
  annotate(geom = 'text', x = 70, y = 3.20, label = paste('italic(d) == ', d), parse = T, family = 'Helvetica', size = font_size) +
  #annotate(geom = 'text', x = 10, y = 1.70, label = paste('AIC == ', AIC_lin), parse = T) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'), 
        axis.line = element_line(size = 2))


nonlin_pred <- data.frame('measurement_day' =  measurement_day, 
                          'pred_score' = theta + (alpha - theta)/(1 + exp((beta - measurement_day)/gamma)))

nonlin_function_plot <- ggplot(nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) +
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(limits = c(2.90, 3.5), breaks = seq(from = 3, to = 3.5, by = .25)) +
  scale_x_continuous(breaks = seq(0, 360, by = 60), limits = c(0, 360)) +
  labs(x = 'Day', y = 'Predicted value', size = 16, tag = 'B') +
  ggtitle(label = 'B: Response pattern predicted \n by logistic (nonlinear) function') +
  annotate(geom = 'text', x = 80, y = 3.45, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) +
  #beta
  annotate(geom = 'text', x = 40, y = 3.35, label = paste('theta == 3.00'), parse = T, size = font_size) +
  annotate(geom = 'text', x = 55, y = 3.30, label = paste('alpha == ', alpha), parse = T, size = font_size) + 
  annotate(geom = 'text', x = 55, y = 3.25, label = paste('beta == ', beta), parse = T, size = font_size) + 
  annotate(geom = 'text', x = 50, y = 3.20, label = paste('gamma == ', gamma), parse = T, size = font_size) + 
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'), 
        axis.line = element_line(size = 2))

interpretation_plot <- ggarrange(poly_pred_plot, nonlin_function_plot, ncol = 2)
ggsave(plot = interpretation_plot, filename = 'Figures/polynomial_vs_nonlinear_plot.pdf', width = 24, height = 12)
```

In dealing with nonlinear response patterns, one important question concerns the analysis. Although nonlinear change can be modelled in a variety of ways---with latent change score models [e.g., @orourke2021] and spline models [e.g., @fine2020] as some examples---the following paragraphs will contrast the commonly used polynomial function with the lesser known nonlinear functions in modelling nonlinear change.\footnote{The definition of a nonlinear function is mathematical in nature. Specifically, a nonlinear function contains at least one parameter that exists in the corresponding partial derivative. For example, in the logistic function $\uptheta + \frac{\upalpha - \uptheta}{1 + exp^(\frac{\upbeta - t}{\upgamma}}$ is nonlinear because $\upbeta$ exists in $\frac{\partial y}{\partial \upbeta}$ (in addition to $\upgamma$ existing in its corresponding partial derivative). The $n^{th}$ order polynomial function of $y = a + bx + cx^2 + ... + nx^n$ is linear because  the partial derivatives with respect to the parameters (i.e., $1, x^2, ..., x^n$) do not contain the associated parameter.}

Consider an example where an organization introduces a new incentive system with the goal of increasing the motivation of its employees. To assess the effectiveness of the incentive system, employees provide motivation ratings every month days over a period of 360 days. Over the 360-day period, the motivation levels of the employees increase following an s-shaped pattern of change over time. One analyst decides to model the observed change using a **polynomial function** shown below in Equation \ref{eq:polynomial}: 

```{=tex}
\begin{align}
  y = \mathit{a} + \mathit{b}x + \mathit{c}x^2 + \mathit{d}x^3.
  (\#eq:polynomial)
\end{align}
```

\noindent A second analyst decides to model the observed change using a **logistic function** shown below in Equation \ref{eq:logistic1}:

```{=tex}
\begin{align}
  y = \uptheta + \frac{\upalpha - \uptheta}{1 + e^{\frac{\upbeta -time}{\upgamma}}}
  (\#eq:logistic1)
\end{align}
```

\noindent  Figure \ref{fig:polynomial-vs-logistic}A shows the response pattern predicted by the polynomial function of Equation \ref{eq:polynomial} with the estimated values of each parameter ($a$, $b$, $c$, and $d$) and Figure \ref{fig:polynomial-vs-logistic}B shows the response pattern predicted by the logistic function (Equation \ref{eq:logistic1}) along with the values estimated for each parameter ($\uptheta$, $\upalpha$, $\upbeta$, and $\upgamma$). Although the logistic and polynomial functions predict nearly identical response patterns, the parameters of the logistic function have the following meaningful interpretations (see Figure \ref{fig:combined_plot_1}):

-   $\uptheta$ specifies the value at the first plateau (i.e., the starting value) and so is called the **baseline** parameter (see Figure \ref{fig:combined_plot_1}A).
-   $\upalpha$ specifies the value at the second plateau (i.e., the ending value) and so is called the the **maximal elevation** parameter (see Figure \ref{fig:combined_plot_1}B).
-   $\upbeta$ specifies the number of days required to reach the half the difference between the first and second plateau (i.e., the midway point) and so is called the **days-to-halfway-elevation** parameter (see Figure \ref{fig:combined_plot_1}C). 
-   $\upgamma$ specifies the number of days needed to move from the midway point to approximately 73% of the difference between the starting and ending values (i.e., satiation point) nd so is called the **halfway-triquarter delta** parameter (see Figure \ref{fig:combined_plot_1}D).

\noindent Applying the parameter meanings of the logistic function to the parameter values estimated by using the logistic function (Equation \ref{eq:logistic1}), the predicted response pattern begins at a value of `r theta` (baseline) and reaches a value of `r alpha` (maximal elevation) by the end of the 360-day period. The midway point of the curve is reached after `r beta` days (days-to-halfway elevation) and the satiation point is reached `r gamma`days later (halfway-triquarter delta; or `r beta + gamma` days after the beginning of the incentive system is introduced). When looking at the polynomial function, aside from the '$a$' parameter indicating the starting value, it is impossible to meaningfully interpret the values of any of the other parameter values. Therefore, using a nonlinear function such as the logistic function provides a meaningful way to interpret nonlinear change.



```{=tex}
\begin{figure}[H]
  \caption{Response Patterns Predicted by Polynomial (Equation \ref{eq:polynomial}) and Logistic (Equation \ref{eq:logistic1}) Functions}
  \label{fig:polynomial-vs-logistic}
  \includegraphics{Figures/polynomial_vs_nonlinear_plot} \hfill{}
  \caption*{Note. \textup{The top panel response pattern predicted by the polynomial function of Equation @ref(eq:polynomial) and the bottom panel shows the response pattern predicted by the logistic function of Equation @ref(eq:logistic1)}}
\end{figure}
```

## Overview of Simulation Experiments 

To investigate the effects of longitudinal design and analysis factors on modelling accuracy, three Monte Carlo experiments were conducted. Before summarizing the simulation experiments, one point needs to be mentioned regarding the maximum number of independent variables used in each experiment. No simulation experiment manipulated more than three variables because of the difficulty associated with interpreting interactions between four or more variables. Even among academics, the ability to correctly interpret interactions sharply declines when the number of independent variables increases from three to four [@halford2005]. Therefore, none of my simulation experiments manipulated more than three variables so that results could be readily interpreted. 

To summarize the three simulation experiments, the independent variables of each simulation experiment are listed below: 

* Experiment 1: number of measurements, spacing of measurements, and nature of change. 
* Experiment 2: number of measurements, spacing of measurements, and sample size. 
* Experiment 3: number of measurements, sample size, and time structuredness. 

\noindent The sections that follow will present each of the simulation experiments and their corresponding results. 

```{r logistic-interpretation-plot, eval=F, include=F}
#setup variables for logistic curve 
time <- seq(from = 1, to = 360, by = 1)

#df for points 
point_df <- data.frame('day' = c(1, beta, beta+gamma, 360), 
                       'curve_score' = c(theta, beta, gamma, alpha), 
                       'beta_brace' = factor(c('beta', 'beta', 'NA', 'NA')),
                       'beta_label' = rep('d[beta]', times = 4), 
                       
                       'gamma_brace' = factor(c('NA', 'gamma', 'gamma', 'NA')), 
                       'gamma_label' = rep('d[gamma]', times = 4),
                       
                       'total_brace' = factor(c('total', 'NA', 'NA', 'total')), 
                       'total_label' = rep('d[total]', times = 4))

font_size <- 8
title_font <- 30
axis_text_size <- 20
axis_title_size <- 24

theta_plot_1 <- ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(A:~Baseline~(theta)))) + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 440, y = 3.05, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) +

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


alpha_plot_1 <-ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(B:~Maximal~elevation~(alpha)))) + 
  #theta 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text',  x = 440, y = 3.27, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.20, label = 'alpha == 3.32', parse = T, size = font_size) + 

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


beta_plot_1 <- ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label =  expression(bold(C:~Days~to~halfway~elevation~(beta)))) + 
  #theta 
    geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = beta, xend = beta, y = 3.16, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 55, y = 3.14, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 55, y = 3.08, label = 'beta == 199~days', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

gamma_plot_1 <- ggplot(data = nonlin_pred, aes(x = measurement_day, y = pred_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(D:~`Halfway-triquarter`~'delta'~(gamma)))) +
  
  #gamma 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = beta + gamma, xend = beta + gamma, y = 3.233, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 100, y = 3.21, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.15, label = 'gamma == 21~days', parse = T, size = font_size) + 
  
  #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

combined_plot_1 <- ggarrange(theta_plot_1, alpha_plot_1, beta_plot_1, gamma_plot_1)
ggsave(plot = combined_plot_1, filename = 'Figures/combined_plot_1.pdf', width = 18, height = 12)


complete_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 1) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360))+
  annotate(geom = 'text', x = 50, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = 5) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 3) +

  #beta
  annotate(geom = 'text', x = 85, y = 3.15, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 90, y = 3.12, label = 'beta == 180~days', parse = T, size = font_size) +
  geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 180, xend = 180, y = 3.16, yend = 2.98, linetype = 2, size = 0.3) + #vertical dashed line 
  
  #gamma
  annotate(geom = 'text', x = 97, y = 3.233, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.21, label = 'gamma == 40~days', parse = T, size = font_size) + 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 220, xend = 220, y = 3.233, yend = 2.98, linetype = 2, size = 0.3)+  #vertical dashed line  
  
  coord_cartesian(clip = 'off') + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 3, size = 0.3) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 425, y = 3.03, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 425, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) + 

  #alpha 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 3, size = 0.3) + #vertical dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) +      #horizontal arrow
  annotate(geom = 'text', x = 430, y = 3.30, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 430, y = 3.26, label = 'alpha == 3.32', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold',size = title_font), 
        axis.title = element_text(size = 16), 
        axis.text = element_text(size = 13, colour = 'black'))
  
    ##brace information
  #stat_brace(data = point_df %>% filter(beta_brace == 'beta'), 
  #           mapping = aes(group = beta_brace, label = beta_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #stat_brace(data = point_df %>% filter(gamma_brace == 'gamma'), 
  #           mapping = aes(group = gamma_brace, label = gamma_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) +
  #
  #stat_brace(data = point_df %>% filter(total_brace == 'total'), 
  #           mapping = aes(group = total_brace, label = total_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #description box 
  #annotate(geom = 'rect', xmin = 235, xmax = 355, ymin = 3.02, ymax = 3.15, alpha = 0.1, color = 'black') + 
  #annotate(geom = 'text', x = 295, y = 3.13, label = 'd[total] == alpha~-~theta == 0.32', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.09, label = 'd[beta] == 0.5~(d[total]) == 0.16', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.06, label = 'd[gamma] == 0.23~(d[total]) == 0.07', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.03, label = 'd[beta]~+~d[gamma] == 0.73~(d[total]) == 0.23', parse = T, size = 4.5) + 

ggsave(plot = complete_plot, filename = 'Figures/complete_logistic_exp_plot.pdf', width = 9, height = 6)
```

```{=tex}
\begin{figure}[H]
  \caption{Description Each Parameters Logistic Function (Equation \ref{eq:logistic1})}
  \label{fig:combined_plot_1}
  \includegraphics{Figures/combined_plot} \hfill{}
  \caption*{Note. \textup{Panel A shows that the baseline parameter ($\uptheta$) sets the starting value of the of curve, which in the current example has a value of 3.00 ($\uptheta$ = 3.00). Panel B shows that the maximal elevation parameter ($\upalpha$) sets the ending value of the curve, which in the current example has a value of 3.32 ($\upalpha$ = 3.32). Panel C shows that the days-to-halfway elevation parameter ($\upbeta$) sets the number of days needed to reach 50\% of the difference between the baseline and maximal elevation. In the current example, the baseline-maximal elevation difference is 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway elevation parameter defines the number of days needed to reach a value of 3.16. Given that the days-to-halfway elevation parameter is set to 180 in the current example ($\upbeta = 180$), then 180 days are neededto go from a value of 3.00 to a value of 3.16. Panel D shows that the halfway-triquarter delta parameter ($\upgamma$) sets the number of days needed to go from halfway elevation to approximately 73\% of the baseline-maximal elevation difference of 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32). Given that 73\% of the baseline-maximal elevation difference is 0.23 and the halfway-triquarter delta is set to 40 days ($\upgamma = 40$), then 40 days are needed to go from the halfway point of 3.16 to the triquarter point of approximately 3.23).}}
\end{figure}
```



```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# Experiment 1

```{=tex}
\nointerlineskip
\vfill
\newpage
```

Experiment 1 was investigated how modelling accuracy was affected by using different measurement spacing schedules. To provide a more comprehensive investigation of modelling accuracy under each measurement spacing schedule, the nature of change of the nonlinear curve was also manipulated by changing the value of the days-to-halfway elevation parameter (specifically, the fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$]). Manipulating the nature of change provided insight into the nature of change that could be most accurately modelled under each measurement spacing schedule. 

Experiment 1 investigated how the modelling accuracy of a nonlinear pattern was affected under conditions characterized by different measurement spacing schedules, measurement numbers, and natures of change (see Table \ref{tab:experimentOverview}). Convergence success rate was computed for each cell and percent bias was computed for each parameter in each cell. Variables held constant were sample size (*N* = 225), the distribution of errors over time (independent and identically distributed), and absence of missing data. The results of Experiment 1 provided insight into whether an optimal measurement schedule existed across different combinations of measurement number and spacing.

## Methods

### Variables Used in Simulation Experiment 

#### Independent Variables
##### Number of Measurements

(ref:loehlin2017) [@loehlin2017]

The exact set of values used by @coulombe2016 for the number of
measurements could not be used in my simulations because doing so would
have created non-identified models. Specifically, the smallest value
used for the number of measurements in @coulombe2016 of 3 measurements
could not be used in my simulations because it would not have provided
sufficient degrees of freedom for estimating the nonlinear latent growth
curve model in my simulations. The model used in my simulations
estimated 9 parameters (*p* = 9; 4 fixed-effects + 4 random-effects + 1
error) and so the minimum number of measurements (or observed variables)
required for model identification (and to allow model comparison) would
was 4.\footnote{Degrees of freedom is
calculated by multiplying the number of observed variables (\textit{p})
by \textit{p} + 1 and dividing it by 2 (\textit{p}[{\textit{p} +
1}]/2; see (ref:loehlin2017)}Because my proposed simulation experiments
were intended to map onto the manipulations used by @coulombe2016, the
second-smallest value used for the number of measurements in
@coulombe2016 was 5 (see Table \ref{tab:coulombe2016}), and so my
simulations used 5 measurements as the smallest measurement number
value. Importantly, a larger value of 11 was added to test for a
possible effect of a high measurement number. Therefore, my simulation
experiments used the following values in manipulating the number of
measurements: 5, 7, 9, and 11 (see Table \ref{tab:myValues}).

##### Spacing of Measurements

The only study to manipulate measurement spacing (to my knowledge) was
@timmons2015. Measurement spacing in @timmons2015 was manipulated in the
following four ways:

1)  **Equal spacing**: measurements were divided by intervals of
    equivalent lengths.
    
2)  **Time-interval increasing spacing**: intervals that divided measurements
    increased in length over time.

3)  **Time-interval decreasing spacing**: intervals that divided measurements
    decreased in length over time.

4)  **Middle-and-extreme spacing**: measurements were clustered near the
    beginning, middle, and end of the data collection period.

\noindent To maintain consistency with the established literature, my
experiments manipulated measurement spacing in the same way as
@timmons2015 presented above. (Note that I developed a procedure for generating measurement schedules for each of the four measurement spacing conditions in [Appendix A][Appendix A: Procedure for generating measurement schedules in measurement spacing conditions]). 

Table \@ref(tab:measurementDays) lists the measurement days that were
used for all measurement spacing-measurement number cells. The first
column lists the type of measurement spacing (i.e., equal, time-interval
increasing, time-interval decreasing, or middle-and-extreme); the second
column lists the number of measurements (5, 7, 9, or 11); the third
column lists the measurement days that correspond to each measurement
number-measurement spacing condition; and the fourth column lists the
interval lengths that characterize each set of measurements. Note that
the interval lengths are equal for the equal spacing, increase over time
for the time-interval increasing spacing, and decrease over time for the
time-interval decreasing spacing, For cells with middle-and-extreme
spacing, the measurement days and and interval lengths corresponding to
the middle of the measurement window have been emboldened.

```{r measurementDays, echo=F}
time_period <- 360
num_measurements <- seq(from = 5, to = 11, by = 2)
smallest_int_length <- 30

#meausurement days 
equal_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'equal')$measurement_days
equal_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'equal')$measurement_days
equal_9 <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'equal')$measurement_days
equal_11 <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'equal')$measurement_days

time_inc_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days
time_inc_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days
time_inc_9 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days, 2)
time_inc_11 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_inc')$measurement_days, 2)

time_dec_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days
time_dec_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days
time_dec_9 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days, 2)
time_dec_11 <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_dec')$measurement_days, 2)

mid_ext_5 <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days
mid_ext_7 <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days
mid_ext_9 <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days
mid_ext_11 <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'mid_ext')$measurement_days


#measurement intervals
equal_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'equal')$interval_lengths
equal_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'equal')$interval_lengths
equal_9_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'equal')$interval_lengths
equal_11_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'equal')$interval_lengths

time_inc_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths
time_inc_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths
time_inc_9_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths, 2)
time_inc_11_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_inc')$interval_lengths, 2)

time_dec_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths
time_dec_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths
time_dec_9_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths, 2)
time_dec_11_int <- round(compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'time_dec')$interval_lengths, 2)

mid_ext_5_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 5, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths
mid_ext_7_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 7, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths
mid_ext_9_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 9, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths
mid_ext_11_int <- compute_measurement_schedule(time_period = time_period, num_measurements = 11, smallest_int_length,measurement_spacing = 'mid_ext')$interval_lengths


measurement_days_df <- data.frame('Measurement spacing' = c('Equal', '', '', '', 
                                                            'Time-interval increasing', '', '', '', 
                                                            'Time-interval decreasing', '', '', '', 
                                                            'Middle-and-extreme', '', '', ''), 
                                  'Number of measurements' = rep(num_measurements, times = 4), 
                                  'Measurement days' = c(paste(equal_5, collapse = ', '), 
                                                         paste(equal_7, collapse = ', '), 
                                                         paste(equal_9, collapse = ', '), 
                                                         paste(equal_11, collapse = ', '), 
                                                         
                                                         paste(time_inc_5, collapse = ', '),
                                                         paste(time_inc_7, collapse = ', '),
                                                         paste(time_inc_9, collapse = ', '),
                                                         paste(time_inc_11, collapse = ', '),
                                                         
                                                         paste(time_dec_5, collapse = ', '),
                                                         paste(time_dec_7, collapse = ', '),
                                                         paste(time_dec_9, collapse = ', '),
                                                         paste(time_dec_11, collapse = ', '),
                                                         
                                                        '1, \\textbf{150, 180, 210}, 360',
                                                        '1, 30, \\textbf{150, 180, 210}, 330, 360',
                                                        '1, 30, 60, \\textbf{150, 180, 210}, 300, 330, 360',
                                                        '1, 30, 60, \\textbf{120, 150, 180, 210, 240,} 300, 330, 360'), 
                                  
                                  'Interval lengths' = c(paste(equal_5_int, collapse = ', '), 
                                                         paste(equal_7_int, collapse = ', '), 
                                                         paste(equal_9_int, collapse = ', '), 
                                                         paste(equal_11_int, collapse = ', '), 
                                                         
                                                         paste(time_inc_5_int, collapse = ', '),
                                                         paste(time_inc_7_int, collapse = ', '),
                                                         paste(time_inc_9_int, collapse = ', '),
                                                         paste(time_inc_11_int, collapse = ', '),
                                                         
                                                         paste(time_dec_5_int, collapse = ', '),
                                                         paste(time_dec_7_int, collapse = ', '),
                                                         paste(time_dec_9_int, collapse = ', '),
                                                         paste(time_dec_11_int, collapse = ', '),
                                                         
                                                        '150, \\textbf{30, 30}, 150',
                                                         '30, 120, \\textbf{30, 30}, 120, 30',
                                                         '30, 30, 90, \\textbf{30, 30}, 90, 30, 30',
                                                         '30, 30, 60, \\textbf{30, 30, 30, 30}, 60, 30, 30'),
                                  check.names = F)

kbl(x = measurement_days_df, booktabs = TRUE, format = 'latex', longtable = T, 
      align = c('l', 'l'), 
    linesep = c(rep('', times = 3),
            '\\cmidrule{1-4}\\addlinespace'), 
      caption = 'Measurement Days Used for All Measurement Number-Measurement Spacing Conditions ', 
    escape=F) %>%
   kable_styling(latex_options= c('hold_position'), font_size = 10, position = 'left') %>%  
    footnote(general =  "For conditions with middle-and-extreme spacing, the measurement days and and interval lengths corresponding to the middle of the measurement window have been emboldened.",  threeparttable = T,  escape = F, general_title = '\\\\textit{Note.}\\\\hspace{-1.25pc}')%>%
  landscape(margin = '1cm')

```


##### Population Values Set for The Fixed-Effect Days-to-Halfway Elevation Parameter $\upbeta_{fixed}$ (Nature of Change)

The nature of change was manipulated by setting the days-to-halfway elevation parameter ($\upbeta_{fixed}$) to a value of either 80, 180, or 280 days (see Table \ref{tab:myValues} and Figure \ref{fig:combined_plot}A). 

#### Dependent Variables

##### Convergence Success Rate

The proportion of iterations in a cell where models converged defined
the **convergence success rate**.\footnote{Specifically, convergence was obtained if the convergence code returned by OpenMx was 0.} Equation \@ref(eq:convergence) below shows the calculation used to compute the convergence success rate:

```{=tex}
\begin{align}
  \text{Convergence success rate} =  \frac{\text{Number of models that successfully converged}}{n},
  (\#eq:convergence) 
\end{align}
```
\noindent where *n* represents the cell size.

##### Bias

Bias was calculated to evaluate the accuracy with which each logistic
function parameter was estimated. As shown below in Equation
\@ref(eq:bias), **bias** was obtained by calculating the difference
between the population value set for a parameter and the average
estimated value in each cell.

\useshortskip

```{=tex}
\begin{align}
  \text{Bias} =  \text{Population value for parameter} - \text{Average estimated value}
  (\#eq:bias) 
\end{align}
```

### Overview of Data Generation and Analysis

#### Data Generation

Data for each simulation experiment were generated using R [@rstudio].
To generate the data, the **multilevel logistic function** shown below
in Equation \@ref(eq:logFunction-generation) was used:

```{=tex}
\begin{align}
  y_{ij} = \uptheta_j + \frac{\upalpha_j - \uptheta_j}{{1 + e^\frac{\upbeta_j - time_i}{\upgamma_j}}} + \upepsilon_{ij}, 
(\#eq:logFunction-generation)
\end{align}
```

\noindent where $\uptheta$ represents the baseline parameter, $\upalpha$
represents the maximal elevation parameter, $\upbeta$ represents the
days-to-halfway elevation parameter, and $\upgamma$ represents
triquarter-halfway delta parameter. Note that, values for $\uptheta$,
$\upalpha$, $\upbeta$, and $\upgamma$ were generated for each *j* person
across all *i* time points, with an error value being randomly generated
at each *i* time point($\upepsilon_{ij}$). In other words, unique
response patterns were generated for each person in each of the 1000
data sets generated per cell.

Figure \ref{fig:combined_plot}A shows that the baseline parameter
($\uptheta$) sets the starting value of the curve, which in the current
example has a value of 3.00 ($\uptheta$ = 3.00). Figure
\ref{fig:combined_plot}B shows that the maximal elevation parameter
($\upalpha$) sets the ending value of the curve, which in the current
example has a value of 3.32 ($\upalpha$ = 3.32). Note that a difference
of 0.32 was selected to represent the average effect size in
organizational research [@bosco2015\; for more information, see section on [population values][Population Values Used for Logistic Function Parameters]]. Figure \ref{fig:combined_plot}C shows that the days-to-halfway elevation
parameter ($\upbeta$) sets the number of days needed to reach 50% of the
difference between the baseline and maximal elevation. In the current
example, the baseline-maximal elevation difference is 0.32
($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway
elevation parameter defines the number of days needed to reach a value
of 3.16. Given that the days-to-halfway elevation parameter is set to
180 in the current example ($\upbeta = 180$), then 180 days are needed
to go from a value of 3.00 to a value of 3.16. Figure
\ref{fig:combined_plot}D shows that the halfway-triquarter delta
parameter ($\upgamma$) sets the number of days needed to go from halfway
elevation to approximately 73% of the baseline-maximal elevation
difference of 0.32. Given that 73% of the baseline-maximal elevation
difference is 0.23 and the halfway-triquarter delta is set to 40 days
($\upgamma = 40$), then 40 days are needed to go from the halfway point
of 3.16 to the triquarter point of approximately 3.23.

```{r logistic-interpretation-plot1, eval=F, include=F}
#setup variables for logistic curve 
time <- seq(from = 1, to = 360, by = 1)
theta <- 3
alpha <- 3.32
beta <- 180
gamma <- 40

logistic_data <- data.frame('day' = time, 
                            'curve_score' = theta + (alpha - theta)/(1 + exp((beta - time)/gamma))) 

#make first and last values exactly equal to theta and alpha 
logistic_data$curve_score[c(1, 360)] <- c(theta, alpha)

baseline <- logistic_data$curve_score[logistic_data$day == 1]
halfway_value <- logistic_data$curve_score[logistic_data$day == beta]
triquarter_value <- logistic_data$curve_score[logistic_data$day == beta + gamma]
maximal_elevation <- logistic_data$curve_score[logistic_data$day == 360]

#df for points 
point_df <- data.frame('day' = c(1, beta, beta+gamma, 360), 
                       'curve_score' = c(baseline, halfway_value, triquarter_value, maximal_elevation), 
                       'beta_brace' = factor(c('beta', 'beta', 'NA', 'NA')),
                       'beta_label' = rep('d[beta]', times = 4), 
                       
                       'gamma_brace' = factor(c('NA', 'gamma', 'gamma', 'NA')), 
                       'gamma_label' = rep('d[gamma]', times = 4),
                       
                       'total_brace' = factor(c('total', 'NA', 'NA', 'total')), 
                       'total_label' = rep('d[total]', times = 4))

font_size <- 8
title_font <- 30
axis_text_size <- 20
axis_title_size <- 24

theta_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(A:~Baseline~(theta)))) + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 440, y = 3.05, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) + 

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


alpha_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(B:~Maximal~elevation~(alpha)))) + 
  #theta 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 5, size = 1) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text',  x = 440, y = 3.27, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 440, y = 3.20, label = 'alpha == 3.32', parse = T, size = font_size) + 

   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))


beta_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label =  expression(bold(C:~Days~to~halfway~elevation~(beta)))) + 
  #theta 
    geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 180, xend = 180, y = 3.16, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 55, y = 3.14, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 55, y = 3.08, label = 'beta == 180~days', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

gamma_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 2) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360)) +
  annotate(geom = 'text', x = 80, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = font_size) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 5) +
  coord_cartesian(clip = 'off') + 
  
  ggtitle(label = expression(bold(D:~`Halfway-triquarter`~'delta'~(gamma)))) +
  
  #gamma 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 1, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 220, xend = 220, y = 3.233, yend = 2.98, linetype = 2, size = 1) + #vertical dashed line 
  annotate(geom = 'text', x = 100, y = 3.21, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.15, label = 'gamma == 40~days', parse = T, size = font_size) + 
  
  #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold', size = title_font,  hjust = 0), 
        axis.title = element_text(size = axis_title_size), 
        axis.text = element_text(size = axis_text_size, colour = 'black'), 
        plot.tag = element_text(face = 'bold'))

combined_plot <- ggarrange(theta_plot, alpha_plot, beta_plot, gamma_plot)
ggsave(plot = combined_plot, filename = 'Figures/combined_plot.pdf', width = 18, height = 12)


complete_plot <- ggplot(data = logistic_data, aes(x = day, y = curve_score)) + 
  geom_line(size = 1) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(name = 'Curve value', limits = c(3, 3.35), breaks = c(3, 3.16, 3.23, 3.32)) + 
  scale_x_continuous(name = 'Day', breaks = c(0, beta, beta + gamma, 360))+
  annotate(geom = 'text', x = 50, y = 3.28, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = 5) + 
  geom_point(data = point_df, mapping = aes(x = day, y = curve_score), size = 3) +

  #beta
  annotate(geom = 'text', x = 85, y = 3.15, label = 'Days to halfway \nelevation', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 90, y = 3.12, label = 'beta == 180~days', parse = T, size = font_size) +
  geom_segment(x = 130, xend = 175, y = 3.16, yend = 3.16, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 180, xend = 180, y = 3.16, yend = 2.98, linetype = 2, size = 0.3) + #vertical dashed line 
  
  #gamma
  annotate(geom = 'text', x = 97, y = 3.233, label = 'Halfway-triquarter delta', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 105, y = 3.21, label = 'gamma == 40~days', parse = T, size = font_size) + 
  geom_segment(x = 175, xend = 215, y = 3.233, yend = 3.233, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  geom_segment(x = 220, xend = 220, y = 3.233, yend = 2.98, linetype = 2, size = 0.3)+  #vertical dashed line  
  
  coord_cartesian(clip = 'off') + 
  #theta 
  geom_segment(x = 10, xend = 360, y = baseline, yend = baseline, linetype = 3, size = 0.3) + #horizontal dashed line  
  geom_segment(x = 400, xend = 365, y = baseline, yend = baseline, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) + #horizontal arrow
  annotate(geom = 'text', x = 425, y = 3.03, label = 'Baseline \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 425, y = 3.00, label = 'theta == 3.00', parse = T, size = font_size) + 

  #alpha 
  geom_segment(x = -3, xend = 360, y = maximal_elevation, yend = maximal_elevation, linetype = 3, size = 0.3) + #vertical dashed line  
  geom_segment(x = 400, xend = 365, y = maximal_elevation, yend = maximal_elevation, size = 0.2, arrow = arrow(length = unit(0.3, 'cm'))) +      #horizontal arrow
  annotate(geom = 'text', x = 430, y = 3.30, label = 'Maximal \nelevation \n(y-axis)', size = font_size, fontface = 'bold') +
  annotate(geom = 'text', x = 430, y = 3.26, label = 'alpha == 3.32', parse = T, size = font_size) + 
  
   #themes
  theme_classic(base_family = 'Helvetica', base_size = 13) +
  theme(plot.margin = unit(c(0, 1, 0.1, 0.1), units="cm"), 
        plot.title = element_text(face='bold',size = title_font), 
        axis.title = element_text(size = 16), 
        axis.text = element_text(size = 13, colour = 'black'))
  
    ##brace information
  #stat_brace(data = point_df %>% filter(beta_brace == 'beta'), 
  #           mapping = aes(group = beta_brace, label = beta_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #stat_brace(data = point_df %>% filter(gamma_brace == 'gamma'), 
  #           mapping = aes(group = gamma_brace, label = gamma_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) +
  #
  #stat_brace(data = point_df %>% filter(total_brace == 'total'), 
  #           mapping = aes(group = total_brace, label = total_label), labelsize = 4.5, parse = T,  width = 15, rotate = 90) + 
  #
  #description box 
  #annotate(geom = 'rect', xmin = 235, xmax = 355, ymin = 3.02, ymax = 3.15, alpha = 0.1, color = 'black') + 
  #annotate(geom = 'text', x = 295, y = 3.13, label = 'd[total] == alpha~-~theta == 0.32', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.09, label = 'd[beta] == 0.5~(d[total]) == 0.16', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.06, label = 'd[gamma] == 0.23~(d[total]) == 0.07', parse = T, size = 4.5) + 
  #annotate(geom = 'text', x = 295, y = 3.03, label = 'd[beta]~+~d[gamma] == 0.73~(d[total]) == 0.23', parse = T, size = 4.5) + 

ggsave(plot = complete_plot, filename = 'Figures/complete_logistic_exp_plot.pdf', width = 9, height = 6)
```

```{=tex}
\begin{figure}[H]
  \caption{Description of Each Parameter of Four-Parameter Logistic Function}
  \label{fig:combined_plot}
  \includegraphics{Figures/combined_plot} \hfill{}
  \caption*{Note. \textup{Panel A shows that the baseline parameter ($\uptheta$) sets the starting value of the of curve, which 
in the current example has a value of 3.00 ($\uptheta$ = 3.00). Panel B shows that the maximal elevation parameter ($\upalpha$) sets the ending value of the curve, which in the current example has a value of 3.32 ($\upalpha$ = 3.32). Panel C shows that the days-to-halfway elevation parameter ($\upbeta$) sets the number of days needed to reach 50\% of the difference between the baseline and maximal elevation. In the current example, the baseline-maximal elevation difference is 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway elevation parameter defines the number of days needed to reach a value of 3.16. Given that the days-to-halfway elevation parameter is set to 180 in the current example ($\upbeta = 180$), then 180 days are neededto go from a value of 3.00 to a value of 3.16. Panel D shows that the halfway-triquarter delta parameter ($\upgamma$) sets the number of days needed to go from halfway elevation to approximately 73\% of the baseline-maximal elevation difference of 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32). Given that 73\% of the baseline-maximal elevation difference is 0.23 and the halfway-triquarter delta is set to 40 days ($\upgamma = 40$), then 40 days are needed to go from the halfway point of 3.16 to the triquarter point of approximately 3.23).}}
\end{figure}
```

The logistic growth function (Equation \ref{eq:logFunction-generation}
was used because it is a common pattern of organizational change [or
institutionalization\; @lawrence2001]. Institutionalization curves follow
an s-shaped pattern of the logistic growth function, and so their rates
of change can be represented by the days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta$, $\upgamma$,
respectively), and the success of the change can be defined by the
magnitude of the difference between baseline and maximal elevation
parameters ($\upalpha$ - $\uptheta$, respectively).

##### Population Values Used for Logistic Function Parameters

(ref:bosco2015) [@bosco2015]

Table \@ref(tab:parameterValues) lists the parameter values that will be
used for the population parameters. Given that the decisions for setting the values for the baseline, maximal elevation, and residual variance parameters were informed by past research, the discussion that follows highlights how these decisions were made. The difference between the baseline and maximal elevation
parameters ($\uptheta$ and $\upalpha$, respectively) corresponded to the effect size most commonly observed in organizational research [i.e., the 50^th^ percentile effect size value\; @bosco2015]. Because the meta-analysis of @bosco2015 computed effect sizes as correlations, the the 50^th^ percentile effect size value of $r = .16$ was computed to a standardized effect size using the following conversion function shown in Equation \ref{eq:conversion-effect} [@borenstein2009, Chapter 7]:

\begin{align}
d = \frac{2r}{\sqrt{1 - r^2}}, 
(\#eq:conversion-effect)
\end{align}

\noindent where $r$ is the correlation effect size. Using Equation \ref{eq:conversion-effect}, a correlation value of $r = .16$ becomes a standardized effect size value of $d = 0.32$. For the value residual variance parameter, the its value in @coulombe2016 was set to the value used for the value of the intercept variance parameter. In the current context, the intercept of the logistic function (Equation \ref{eq:logistic1}) is the baseline parameter.\footnote{The definition of an intercept parameter is the value of a curve when no time has elapsed, and this is precisely the definition of the baseline parameter ($\uptheta$). Therefore, the variance of the intercept parameter carries the same meaning as the variance of the baseline prameter ($\uptheta_{random}$).} Given that the value for the of the variability of the baseline parameter was 0.05 (albeit in standard deviation units), the value used for the residual variance parameter was 0.05 ($\upepsilon = 0.05$). 

To facilitate interpretation of the results, data were generated to resemble the commonly used Likert (range of 1--5) by using a standard deviation of 1.00 and change was assumed to occur over a
period of 360 days. The decision to generate data in the context of a
360-day period was made because many organizational processes are often
governed by annual events (e.g., performance reviews, annual returns,
regulations, etc.). Importantly, because @coulombe2016 set covariances
between parameters to zero, all the simulation experiments used
zero-value covariances.

```{r parameterValues, echo=F}
#specify parameters for parameter table 
theta <- 3
alpha <- 3 + .32*1
beta <- 180
gamma <- 20

sd_alpha <- 0.05
sd_theta <- 0.05
sd_beta <- 10
sd_gamma <- 4

sd_error <- 0.05


#table of parameter values
parameterValues_df <- data.frame('Parameter' = c('Parameter means',
                                         'Baseline, $\\uptheta$',
                                         'Maximal elevation, $\\upalpha$', 
                                         'Days-to-halfway elevation, $\\upbeta$', 
                                         'Triquarter-halfway delta, $\\upgamma$', 
                                         
                         'Variability and covariability (in standard deviations)', 
                              'Baseline standard deviation, $\\uppsi_{\\uptheta}$',
                              'Maximal elevation standard deviation, $\\uppsi_{\\upalpha}$', 
                              'Days-to-halfway elevation standard deviation, $\\uppsi_{\\upbeta}$',
                              'Triquarter-halfway delta standard deviation, $\\uppsi_{\\upgamma}$',
                         
                              'Baseline-maximal elevation covariability, $\\uppsi_{\\uptheta\\upalpha}$',
                              'Baseline-days-to-halfway elevation covariability, $\\uppsi_{\\uptheta\\upbeta}$',
                              'Baseline-triquarter-halfway delta covariability, $\\uppsi_{\\uptheta\\upgamma}$',
                         
                              'Maximal elevation-days-to-halfway elevation covariability, $\\uppsi_{\\upalpha\\upbeta}$',
                              'Maximal elevation-triquarter-halfway delta covariability, $\\uppsi_{\\upalpha\\upgamma}$',
                         
                              'Days-to-halfway elevation-triquarter-halfway delta covariability, $\\uppsi_{\\upbeta\\upgamma}$',
                          
                              'Residual standard deviation, $\\uppsi_{\\upepsilon}$'), 
                         'Value' = c('', theta, alpha, beta, gamma, 
                                     '', sd_theta, sd_alpha, sd_beta, sd_gamma, 
                                     0, 0, 0, 0, 0,0,  sd_error), check.names = F)

#round numbers to that they print with two significant numbers
parameterValues_df$Value <- round(as.numeric(as.character(parameterValues_df$Value)), 3)
parameterValues_df$Value <- formatC(round(parameterValues_df$Value, 3), format='f', digits=2)

#replace '  NA' with empty string 
parameterValues_df$Value[parameterValues_df$Value ==" NA"] <- ''


kbl(parameterValues_df, booktabs = TRUE, format = 'latex', longtable = T, 
    linesep = c(rep('', times = 4), '\\addlinespace\\addlinespace', 
                rep('', times = 11))
    , 
    align = c('l', 'c'), 
    caption = "Values Used for Multilevel Logistic Function Parameters", 
    escape = F) %>%
   add_indent(positions = c(2:5, 7:16, 17), level_of_indent = 2) %>%
   kable_styling(latex_options= c('hold_position', 'repeat_header'), position = 'left', font_size = 10) %>%
  footnote(general =  "The difference between $\\\\alpha$ and $\\\\theta$ corresponds to the 50$\\\\mathrm{^{th}}$ percentile Cohen's $d$ value of 0.32 in organizational psychology (Bosco et al., 2015).",  threeparttable = T,  escape = F, general_title = '\\\\textit{Note.}\\\\hspace{-1pc}') %>%
   column_spec(column = 1, width = '12 cm')
```

#### Data Analysis
##### Nonlinear Latent Growth Curve Model Used to Analyze Each Generated Data Set

Each data set generated by the multilevel logistic function (Equation \ref{eq:logFunction-generation}) was analyzed using a specialized latent growth curve model. To fit the logistic function (Equation \ref{eq:logistic}) to a given data set, a linear approximation of the logistic function was needed so that it could fit within the structural equation modelling framework---a linear framework.\footnote{The logistic function (Equation \ref{eq:logistic}) cannot be directly inserted into the structural equation modelling framework because it is a linear framework and only allows matrix-matrix, matrix-vector, and vector-vector operations. Unfortunately, these operations cannot directly reproduce logistic function operations and so a linear approximation of the logistic function is constructed so that it can be inserted into the structural equation modelling framework.} To construct a linear approximation of the logistic function, a first-order Taylor series was constructed for the logistic function. For a detailed explanation of how the logistic function was fit into the structural equation modelling framework, see [Technical Appendix B][Technical Appendix B: Using Nonlinear Function in the Structural Equation Modelling Framework]. 

##### Analysis of Dependent Variables

Among the several effect size metrics---at a broad level, effect size
metrics can represent standardized differences or variance-accounted-for
measures that are corrected or uncorrected for sampling error---the corrected
variance-accounted-for effect size metric of partial $\upomega^2$ was
chosen. Partial $\upomega^2$ has two desirable properties. First, partial
$\upomega^2$ is a less biased estimate of effect size than other
variance-accounted for measures were computed to investigate the effects
of experimental variables on the bias and variability of each logistic
parameter estimate. The effect size metric of partial $\upomega^2$
provides an unbiased estimate of effect size with larger cell sizes
(e.g., $n \ge 100$; [@okada2013]). Second, partial $\upomega^2$ is more
robust to assumption violations of normality and homogeneity of variance
[@yigit2018]. Given that the variability of parameter estimates was
often non-normally distributed across cells, effect size values computed
with using partial $\upomega^2$ should be relatively less biased than
other variance-accounted-for effect size metrics (e.g., $\eta^2$). To
compute partial $\upomega^2$ value for each experimental effect,
Equation \ref{eq:partial-omega} shown below was used:

```{=tex}
\begin{align}
\text{partial} \upomega^2 = \frac{\sigma^2_{effect}}{\sigma^2_{effect} + MSE} 
(\#eq:partial-omega)
\end{align}
```

\noindent where $\sigma^2_{effect}$ represents the variance attributable
to an effect and $MSE$ is the mean squared error.

##### Analysis of Convergence Success Rate

For the analysis of convergence success rate, the mean convergence
success rate was computed for each condition (see section on
[convergence success rate]). Because convergence rates exhibited little
variability across cells due to the nearly unanimous high rates,
examining the effects of any independent variable on these rates using
partial $\upomega^2$ values would provided little information, and so
only descriptive statistics were reported (see [Appendix
B](#appendix-a-convergence-rates))

##### Analysis of Bias

Bias was computed by calculating the raw difference between the each
parameter's estimate from a given model's output in a cell iteration and
the corresponding population value used for the parameter (see Equation
\ref{eq:bias}). 

##### Analysis of Variability in Parameter Estimation

To compute partial $\upomega^2$ values for variability in parameter
estimation, a Brown-Forsythe test was computed and the appropriate
sum-of-squares terms were used to compute partial $\upomega^2$ values.
To compute the Brown-Forsythe test, median absolute values were computed
from the median for each estimated value in each cell as shown in
Equation \ref{eq:brown-forsythe} shown below:
$$\text{Median absolute deviation}_{cell} = |\text{Parameter estimate}_i - \text{Median parameter estimate}_{cell}| $$
\begin{align}
\text{Median absolute deviation} = |\text{Parameter estimate} - \text{Median parameter estimate}_{cell}|.
(\#eq:brown-forsythe)
\end{align}

\noindent An ANOVA was then computed on the median absolute deviation
values, with the terms in Equation \ref{eq:partial-omega} extracted from
the ANOVA output to compute partial $\upomega^2$ values. Note that
deviations from the median value in each cell were used because using
the median protects against the biasing effects of skewed distributions
that were observed in the current simulation experiments [@brown1974]. 

## Results of Experiment 1
### Overview of Data Processing and Modelling Procedure

Due to the considerable number of analyses conducted in each
experiment the sections that follow serve to equip
the reader with a framework to efficiently navigate the results sections
of each experiment. In the following sections, I will present overviews
of the following topics: a) Data pre-processing, b) meaning of logistic function parameters, c) presentation of variability in parameter estimation, d) interpretation of parameter estimation plots, and e) the model estimation procedure. 

#### Pre-Processing of Data and Model Convergence

After collecting the output from the simulations, non-converged models
(and their corresponding parameter estimates) were removed from
subsequent analyses. Tables \ref{tab:conv-exp-1}--\ref{tab:conv-exp-3}
in [Appendix B](#appendix-a-convergence-rates) provide the convergence
success rates for each cell in each of the three simulation experiments.
Model convergence was almost always above 90% and convergence rates
rates below 90% only occurred in the following frequencies in each
experiment:

-   Experiment 1: two instances of sub-90% model convergence rates (see
    Table \ref{tab:conv-exp-1})
-   Experiment 2: two instances of sub-90% model convergence rates (see
    Table \ref{tab:conv-exp-2})
-   Experiment 3: two instances of sub-90% model convergence rates (see
    Table \ref{tab:conv-exp-3})

\noindent Note that all instances of sub-90% model convergence occurred
with five measurements.

#### Review of Logistic Function Parameters Used to Generate Data


Data in each experimental condition were generated using the the
logistic function shown below in Equation
\ref{eq:logFunction-generation2}:

```{=tex}
\begin{align}
y_{pi} = \uptheta_p + \frac{\upalpha_p - \uptheta_p}{{1 + e^\frac{\upbeta_p - time_i}{\upgamma_p}}} + \upepsilon_{pi}.
(\#eq:logFunction-generation2)
\end{align}
```

\noindent where $\uptheta$ represents the baseline parameter, $\upalpha$
represents the maximal elevation parameter, $\upbeta$ represents the
days-to-halfway elevation parameter, and $\upgamma$ represents
triquarter-halfway delta parameter. Note that, values for $\uptheta$,
$\upalpha$, $\upbeta$, and $\upgamma$ were generated for each *p* person
across all *i* time points, with an error value being randomly generated
at each *i* time point ($\upepsilon_{ij}$). In other words, unique
response patterns were generated for each person in each of the 1000
data sets generated per cell. For a review of the logistic function, see
the section on [data generation][Data generation].

#### Presentation of Variability

Given that sampling error causes any population parameter to be
estimated with some degree of variability and that two of the three
simulation experiments manipulated sample size, variability was expected
to occur in the estimation of the logistic function parameters. Even in
situations where sample size was not manipulated (as in Experiment 1) and
where it may have seemed unusual for other independent variables to affect the
variability with which a parameter was estimated, variables outside of
sample size have been shown to affect the variability of parameter
estimation on occasion (e.g., @coulombe2016). Given that the variability
of parameter estimation may be affected in each simulation experiment,
the paragraphs that follow explain how variability was
operationalized and visualized.

Variability in the estimation of each logistic function parameter for
each experimental condition was operationalized as the range covered
by the middle 95% of estimated values. Consider an example where
variability in the days-to-halfway elevation parameter ($\upbeta$) is
being modelled in an experimental condition. Figure
\ref{fig:beta-histogram} shows a density distribution of values
estimated for the days-to-halfway elevation parameter ($\upbeta$). The
region of the density distribution shaded in gray represents the middle
95% of values estimated for the days-to-halfway elevation parameter
($\upbeta$) and the upper and lower values of this region set the upper
and lower values of the error bar that lies above the density
distribution. Therefore, I used error bars to represent variability in parameter estimation because using density distributions for each of the nine
parameters in each experimental cell would have been impractical given the
large number of cells in each experiment.

To visualize variability across multiple experimental conditions for one
logistic function parameter, parameter estimation plots were
constructed. Figure \ref{fig:beta-density-to-param-plot} shows the
procedure that was followed to create a parameter estimation plot. For
each measurement number-sample size condition, a density distribution was
computed for the values estimated for the days-to-halfway elevation
parameter ($\upbeta$), and the range covered by the middle 95% of values
in the density distribution was used to set the length of each error bar.
The plot at the bottom of Figure \ref{fig:beta-density-to-param-plot} is
a **parameter estimation plot** for the days-to-halfway elevation
parameter (specifically, the fixed-effect days-to-halfway elevation
parameter [$\upbeta_{fixed}$]) with the error bars showing the
variability with which $\upbeta_{fixed}$ is modelled in each sample
size-measurement number condition. This style of error bar was used
to represent variability for each parameter in each experimental
condition.

```{r variability-histograms, eval=F, include=F}
exp_2 <- read_csv(file = 'data/exp_2.csv')

generate_param_density_plot(raw_exp_data = exp_2, param_summary_data = param_summary_exp_2, spacing = 'Equal', num_measurements = 5, sample_size = 30)

```

```{=tex}
\begin{figure}
  \caption{Density Distribution of Values Estimated for the Days-to-Halfway Elevation Parameter ($\upbeta$)}
  \label{fig:beta-histogram}
  \includegraphics[height = 8cm, width = 20cm]{Figures/beta_fixed_Equal_5_30} \hfill{}
  \caption*{Note. \textup{Area shaded in gray represents the middle 95\% of estimated values. The upper and lower limits of the shaded areay define the upper and lower limits of the error bar on top of the density distribution.}}
\end{figure}
```
```{r beta-density-to-param-plot, eval=F, include=F}
num_measurements_levels <- as.numeric(levels(param_summary_exp_2$number_measurements))
sample_size_levels <- range(as.numeric(levels(param_summary_exp_2$sample_size)))
exp_2 <- read_csv(file = 'data/exp_2.csv')

for (num_measurements in num_measurements_levels) {
  
  for(sample_size in sample_size_levels){
    
    generate_param_density_plot(raw_exp_data = exp_2, param_summary_data = param_summary_exp_2, 
                                spacing = 'Equal', num_measurements = num_measurements,
                                sample_size = sample_size)
  }
}
```

```{=tex}
\begin{figure}
  \caption{Depiction of Modelling Procedure for Generating Error Bars on Parameter Estimation Plots}
  \label{fig:beta-density-to-param-plot}
  \includegraphics[height = 25cm, width = 10cm]{Figures/density_to_param_plot} \hfill{}
  \caption*{Note. \textup{Density distributions are generated for each experimental condition and the range covered by the middle 95\% of values in each frequnecy distribution is used to create an error bar. The plot at the bottom is a \textbf{parameter estimation plot} for the days-to-halfway elevation parameter ($\upbeta$) and it shows the accuracy with which $\upbeta$ is estimated across all sample size-measurement number combinations when measurement spacing is equal.}}
\end{figure}
```

#### Interpreting a Parameter Estimation Plot

Given that several parameter estimation plots were presented in the
results sections of each experiment, I will provide an overview of how
to interpret these plots. Parameter estimation plots show two indicators
of estimation accuracy: bias and variability. In the sections that
follow, these two accuracy indicators are shown in parameter estimation
plots.

##### Bias

Figure \ref{fig:param-estimation-ex} shows a parameter estimation plot
for the fixed-effect days-to-halfway elevation parameter
($\upbeta_{fixed}$). Estimates for the fixed-effect days-to-halfway
elevation parameter ($\upbeta_{fixed}$) are shown across all measurement
number-sample size combinations and the shaded gray line indicates the
population value set for the parameter ($\upbeta_{fixed}$ = 180). The
black shapes (squares, circles, triangles, diamonds) indicate the
average estimated value and the error bars show the range of values
covered by the middle 95% of estimated parameters (for a review, see
[Presentation of variability]).

**Bias** describes the extent to which an estimate either over- or
underestimates the population value. Looking at the parameter estimation
plot in Figure \ref{fig:param-estimation-ex}, systematic bias is
represented by the extent to which a black shape lies away from the gray
line (i.e., the population value). In the current example, the average
estimated values in each measurement number-sample size condition lie
close to the population value (the gray line), and so bias is nearly
trivial in each condition.

##### Variability

**Variability** describes by the length of the error bars in the
parameter estimation plot. In the current example of Figure
\ref{fig:param-estimation-ex}, variability decreases monotonically as
sample size and measurement number increase.

```{=tex}
\begin{figure}
  \caption{Parameter Estimation Plot for Fixed-Effect Days-to-Halfway Elevation Parameter ($\upbeta_{fixed}$)}
  \label{fig:param-estimation-ex}
  \includegraphics[height = 33cm, width = 15cm]{Figures/param_estimation_ex} \hfill{}
  \caption*{Note. \textup{The shaded gray line indicates the population value set for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$). Estimates for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$) parameter are shown across all measurement number-sample size combinations and the shaded gray line indicates the population value set for the parameter ($\upbeta_{fixed}$ = 180). The black shapes (squares, circles, triangles, diamonds) indicate the average estimated value and the error bars show the range of values covered by the middle 95\% of estimated parameters. Parameter estimation plots show two markers of estimation accuracy: systematic bias and variability. \textbf{Systematic bias} describes the extent to which an estimate either over- or underestimates the population value. In the parameter estimation plot, systematic bias is represented by the extent to which a black shape falls off from the gray line (i.e., the population value). \textbf{Variability} is described by the length of the error bars in the parameter estimation plot. In the current example, systematic bias is very low because all of the average estimated values lie close to the population value (i.e., shaded gray line) and variability decreases monotonically as sample size increases.}}
\end{figure}
```

#### Identifying Minimal Effect Sizes of Interest for Bias and Variability 

##### Minimal Effect Size for Bias 

##### Minimal Effect Size for Variability

#### Model Estimation Procedure

Because a considerable number of parameters were estimated in each cell,
I will review the modelling procedure as a whole. Figure
\ref{fig:results-plot-primer} shows that each parameter of the logistic
function (for each parameter, see Figure \ref{fig:combined_plot}) was
modelled as a fixed and random effect. Values predicted for fixed-effect
parameters are constant across all individuals, whereas values predicted
for random-effect parameters represent the variability with which a
parameter is
estimated.\footnote{Estimating a random-effect for a parameter allows person-specific values to be computed for the parameter.}
In addition to the random- and fixed-effects estimated for each logistic
function parameter, an error term ($\upepsilon$) was also estimated. For
each cell, parameter estimation plots were be created for each logistic
function parameter that showed the accuracy with which that parameter
was modelled.

One important point to mention is that the results of Experiments 1--3
focused on the effects of experimental variables on day-unit parameters
(days-to-halfway elevation [$\upbeta$; Figure
\ref{fig:combined_plot}C] and halfway-triquarter delta
parameters[$\upgamma$; Figure \ref{fig:combined_plot}D]. Across all
three experiments, experimental variables had little effect on the
estimation of Likert-unit parameters (baseline [$\uptheta$; Figure
\ref{fig:combined_plot}A] and maximal elevation [$\upalpha$]; Figure
\ref{fig:combined_plot}B), and so including their estimation plots would
have added unnecessary length and complexity to the results sections of
each experiment. Note that the parameter estimation plots for
Likert-unit parameters were been included in [Appendix
C](#appendix-c-parameter-estimation-plots-for-likert-unit-parameters).

```{=tex}
\begin{figure}[H]
  \caption{Set of Parameters Estimated in Each Simulation Experiment}
  \label{fig:results-plot-primer}
  \includegraphics[height = 17cm, width = 15cm]{Figures/logistic_results_plot} \hfill{}
  \caption*{Note. \textup{Each parameter of the logistic function (for a review, see Figure \ref{fig:combined_plot}) is modelled as a fixed and random effect. Values predicted for fixed-effect parameters are constant across all individuals, whereas values predicted for random-effect parameters are unique across individuals. In addition the random- and fixed-effects estimated for each logistic function parameter, an error term ($\upepsilon$) is also estimated. For each experimental condition, parameter estimation plots will be created for each logistic function parameter that show the accuracy with which each parameter is modelled.}}
\end{figure}
```
### Results 
#### Equal Spacing

```{r plots-equal-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Equal spacing',
                                x_axis_name = expression("Population value set for"~beta[fixed]), 
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -20, beta_upper = 20, ticks = 5)

```

```{r text-values-equal-exp1, echo=F}
gamma_rand_equal_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 7, midpoint == 180, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci),
         estimate = as.integer(estimate)) 

gamma_fixed_equal_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, midpoint == 80, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

gamma_fixed_equal_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, midpoint == 180, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

gamma_fixed_equal_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, midpoint == 280, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

gamma_fixed_equal_7_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 7, midpoint == 180, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp1_plot_equal} shows the parameter estimation plots
for the day-unit parameters when equal spacing was used (error bars
represent the middle 95% of estimated values and shaded horizontal lines
indicate the population values). Panels A--B show the parameter
estimation plots for the fixed- and random-effect days-to-halfway
elevation parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$,
respectively. Panels C--D show the parameter estimation plots for the
fixed- and random-effect triquarter-halfway elevation parameters
($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that, in
Panel A, estimates for $\upbeta_{fixed}$ are centered around their
corresponding population values (80, 180, and 280 days), and so the
y-axis represents error (in number of days) from the population value. Note that Table \ref{tab:omega-exp1-equal} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing. 

For all simulations presented in Figure \ref{fig:exp1_plot_equal}, only
one instance of bias occurred. With five measurements and a population
value for the fixed-effect days-to-halfway elevation parameter
($\upbeta_{fixed}$) set to 180, the random-effect triquarter-halfway
delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D)
was overestimated, with the average estimated value being
`r gamma_rand_equal_5_180$estimate` days (relative to a population value
of 4.00 days). Across all other conditions in Figure
\ref{fig:exp1_plot_equal}, no instance of bias occurred, as the average
estimated values for all day-unit parameters (as indicated by the black
dots) were close to their respective population values (indicated by the
gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with equal spacing. First,
estimation of all day-unit parameters incurred considerable variability
with five measurements across all population values set for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$ =
{80, 180, 280}). For example, the fixed-effect halfway-triquarter delta
parameter ($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_equal}B) had an
error bar length of approximately
`r gamma_fixed_equal_5_80$upper_ci - gamma_fixed_equal_5_80$lower_ci`
days when $\upbeta_{fixed}$ was 80 (upper bound of
`r gamma_fixed_equal_5_80$upper_ci` days to lower bound of
`r gamma_fixed_equal_5_80$lower_ci` days), an error bar length of
approximately
`r gamma_fixed_equal_5_180$upper_ci - gamma_fixed_equal_5_180$lower_ci`
days when $\upbeta_{fixed}$ was 180 (upper bound of
`r gamma_fixed_equal_5_180$upper_ci` days to lower bound of
`r gamma_fixed_equal_5_180$lower_ci` days), and an error bar length of
approximately
`r gamma_fixed_equal_5_280$upper_ci - gamma_fixed_equal_5_280$lower_ci`
days (upper bound of `r gamma_fixed_equal_5_280$upper_ci` days to lower
bound of `r gamma_fixed_equal_5_280$lower_ci` days) when
$\upbeta_{fixed}$ was 280. Note that, in contrast, estimation for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$;
Figure \ref{fig:exp1_plot_equal}A) showed considerably low variability
with five measurements and a population value of 180. Second, the
considerable variability in parameter estimation observed with five
measurements largely diminished with seven measurements. As an example,
with a population value for $\upbeta_{fixed}$ of 180, the error bar
length for the fixed-effect triquarter-halfway delta parameter
($\upgamma_{fixed}$; Figure \ref{fig:exp1_plot_equal}B) spanned
approximately
`r gamma_fixed_equal_5_180$upper_ci - gamma_fixed_equal_5_180$lower_ci`
days with five measurements (upper bound of
`r gamma_fixed_equal_5_180$upper_ci` to a upper bound of
`r gamma_fixed_equal_5_180$lower_ci` days), but the length of this error
bar decreased to approximately
`r gamma_fixed_equal_7_180$upper_ci - gamma_fixed_equal_7_180$lower_ci`
days with seven measurements (upper bound of
`r gamma_fixed_equal_7_180$upper_ci` to a lower bound
`r gamma_fixed_equal_7_180$lower_ci` days).

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Equal Spacing in Experiment 1}
  \label{fig:exp1_plot_equal}
  \includegraphics{Figures/exp1_plot_days_equal spacing} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp1-equal, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'equal', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'M', 'NM x M'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Equal Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), M = population value set for $\\\\upbeta_{fixed}$ (80, 180, 280), NM x M = interaction between number of measurements and population value set for $\\\\upbeta_{fixed}$. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_equal}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_equal}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_equal}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_equal}D)'))
```

\newpage

#### Time-Interval Increasing Spacing

```{r plots-time-increasing-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Time-interval increasing',
                                x_axis_name = expression("Population value set for"~beta[fixed]), 
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -70, beta_upper = 60, ticks = 10)
```

```{r density-plot-functions, include=F, eval=F}
compute_ind_param_error_bar_range <- function(param_search_name, param_data) {
  
  #extract estimates from single parameter
  ind_param_data <-  param_data %>%
    filter(str_detect(string = param_data$parameter, pattern = param_search_name)) %>% 
    pull(estimate)
  
  param_range <- as.numeric(compute_middle_95_estimate(param_data = ind_param_data))
  
  return(param_range)
}

compute_80_ind_param_error_bar_range <- function(param_search_name, param_data) {
  
  #extract estimates from single parameter
  ind_param_data <-  param_data %>%
    filter(str_detect(string = param_data$parameter, pattern = param_search_name)) %>% 
    pull(estimate)
  
  param_range <- as.numeric(compute_middle_80_estimate(param_data = ind_param_data))
  
  return(param_range)
}

compute_90_ind_param_error_bar_range <- function(param_search_name, param_data) {
  
  #extract estimates from single parameter
  ind_param_data <-  param_data %>%
    filter(str_detect(string = param_data$parameter, pattern = param_search_name)) %>% 
    pull(estimate)
  
  param_range <- as.numeric(compute_middle_90_estimate(param_data = ind_param_data))
  
  return(param_range)
}

compute_density <- function(error_bar_range, group, param_name, density_data) { 
  
  density_data <- density_data[density_data$group == group , ]
  
  density_lower_x <- min(which(density_data$x >= error_bar_range[1]))
  density_upper_x <- max(which(density_data$x <= error_bar_range[2]))
  
  day_values <- density_data$x[density_lower_x:density_upper_x]
  
  
  density_df <- data.frame('parameter' = param_name,
                           'day_value' = day_values,
                           'probability' = density_data$y[density_lower_x:density_upper_x], 
                           'max_density_value' = max(density_data$y),
                           'lower_ci' = error_bar_range[1], 
                           'upper_ci' = error_bar_range[2])

    return(density_df)
}
```

```{r exp1-density-plot-time-inc, include=F, eval=F}
##generate figure showing density distributions if gamma_fizxd at N = 500 and N = 1000 with middle 95% area shaded. Emphasize
##that longer error bars in  N = 1000 are likely a statistical artifact of the random number generating procedure. 
exp_1 <- read_csv('data/exp_1A.csv') %>% filter(code == 0) #load data 
exp_1 <- convert_raw_var_to_sd(raw_data = exp_1) #var to sd conversion 

parameter_names <- c(bquote(expr = bold(A:~gamma[random]~(beta[fixed]~" = 280, 5 measurements"))),
                     bquote(expr =bold(B:~gamma[random]~(beta[fixed]~" = 280, 7 measurements"))))
                     
#1) Extract estimates for four parameter gamma_fixed and beta_rand when N =500 or 1000, num_measurements == 5, spacing == equal. 
##Convert to long format, with parameter name going into unique column and estimate becoming new column 
param_data <- exp_1 %>%
  filter(number_measurements <= 7, measurement_spacing == 'time_inc', midpoint == 280) %>%
  select(locate_ivs(exp_1),'gamma_rand') %>%
  pivot_longer(cols = c(gamma_rand), values_to = 'estimate') %>%
  unite(col = 'parameter', c(number_measurements,name))

#2) Replace parameter values with tag labels. 
param_data$parameter <- factor(param_data$parameter, 
                               levels = c("5_gamma_rand", "7_gamma_rand"), 
                               labels = parameter_names)

base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

param_search_names <- c("A\\:", "B\\:")

param_error_bar_ranges <- lapply(X = param_search_names, compute_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


example_plot <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = example_plot, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_inc_exp1.pdf')

```

```{r exp1-density-plot-time-inc-fixed, include=F, eval=F}
param_error_bar_ranges <- lapply(X = param_search_names, compute_80_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


time_inc_fixed_density <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = time_inc_fixed_density, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_inc_exp1_fixed.pdf')

```

```{r text-values-time-increasing-exp1, echo=F}
beta_time_inc_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_inc_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_inc_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_inc_7_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 7, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```


Figure \ref{fig:exp1_plot_time_increasing} shows the parameter
estimation plots for the day-unit parameters when equal spacing was used
(error bars represent the middle 95% of estimated values and shaded
horizontal lines indicate the population values). Panels A--B show the
parameter estimation plots for the fixed-effect days-to-halfway
elevation and triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively. Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that, in Panel A, estimates for
$\upbeta_{fixed}$ are centered around their corresponding population
values (80, 180, and 280 days), and so the y-axis represents error (in
number of days) from the population value. Note that Table \ref{tab:omega-exp1-time-inc} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under time-interval increasing spacing. 

For all simulations presented in Figure
\ref{fig:exp1_plot_time_increasing}, no instance of bias occurred, as
the average estimated values for all day-unit parameters (as indicated
by the black dots) were close to their respective population values
(indicated by the gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with time-interval increasing
spacing. First, for all day-unit parameters except the random-effect
halfway-triquarter delta parameter ($\upgamma_{random}$; Figure
\ref{fig:exp1_plot_time_increasing}D), variability increased
considerably with five measurements as the population value set for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$)
increased from 80 to 280. As an example, the error bar length for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$;
Figure \ref{fig:exp1_plot_time_increasing}A) spanned approximately
`r beta_time_inc_5_80$upper_ci - beta_time_inc_5_80$lower_ci` (upper
bound of `r beta_time_inc_5_80$upper_ci` days to a lower bound of
`r beta_time_inc_5_80$lower_ci` days) and
`r beta_time_inc_5_180$upper_ci - beta_time_inc_5_180$lower_ci` days
(upper bound of `r beta_time_inc_5_180$lower_ci` days to a lower bound
of `r beta_time_inc_5_180$upper_ci` days) when the $\upbeta_{fixed}$ had
a population value of 80 and 180 days, respectively, but the length
increased to
`r beta_time_inc_5_280$upper_ci - beta_time_inc_5_280$lower_ci` days
when the population value for $\upbeta_{fixed}$ was set to 280 (upper
bound of `r beta_time_inc_5_280$upper_ci` days to a lower bound of
`r beta_time_inc_5_280$lower_ci` days). Second, the large amounts of
variability observed for the parameters with five measurements largely
disappeared with seven or more measurements. As an example, when the
population value for the days-to-halfway elevation parameter
($\upbeta_{fixed}$) was set to 280, the long error bar range of
`r beta_time_inc_5_280$upper_ci - beta_time_inc_5_280$lower_ci` days for
the estimation of $\upbeta_{fixed}$ (Figure
\ref{fig:exp1_plot_time_increasing}A) with five measurements decreased
to a shorter length of
`r beta_time_inc_7_280$upper_ci - beta_time_inc_7_280$lower_ci` days
with seven measurements.

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Increasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_increasing}
  \includegraphics{Figures/exp1_plot_days_time-interval increasing.png} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-time-inc} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp1-time-inc, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'time_inc', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'M', 'NM x M'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Increasing Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), M = population value set for $\\\\upbeta_{fixed}$ (80, 180, 280), NM x M = interaction between number of measurements and population value set for $\\\\upbeta_{fixed}$. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_increasing}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_increasing}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_increasing}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_increasing}D)'))
```

One additional minor point should be mentioned. In Figure
\ref{fig:exp1_plot_time_increasing}D, the variability of values
estimated for the random-effect triquarter-halfway delta
($\upgamma_{fixed}$) parameter counterintuitively increased as the
number of measurements increased from five to seven. Inspection of the
underlying density plots for the random-effect triquarter-halfway delta
parameter ($\upgamma_{fixed}$) in Figure
\ref{fig:exp1_density_time_increasing} showed that the increase was not
due to outliers and, rather, occurred because of a slight increase in
the variability of the parameter estimates (see the estimates located
within the dashed rectangle). When the fixed-effect days-to-halfway
elevation parameter ($\upbeta_{fixed}$) was set to 80, the density
distribution for the random-effect triquarter-halfway delta parameter
($\upgamma_{fixed}$) became slightly flatter as the number of
measurements increased from five (Figure
\ref{fig:exp1_density_time_increasing}A) to seven (Figure
\ref{fig:exp1_density_time_increasing}B).

```{=tex}
\begin{figure} [H]
  \caption{Density Plots of the Random-Effect Halfway-Triquarter Delta ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_increasing}D) With Time-Interval Increasing Spacing in Experiment 1 (95\% Error Bars)}
  \label{fig:exp1_density_time_increasing}
  \includegraphics[height = 15cm, width = 25cm]{Figures/density_plots_time_inc_exp1} \hfill{}
  \caption*{Note. \textup{Regions shaded in in gray represent the the middle 95\% of estimated values and the width of the shaded regions is indicated by the length of the horizontal error bars. The error bar length increases as the number of measurements increases from five to seven.}}
\end{figure}
```


#### Time-Interval Decreasing Spacing

```{r plots-time-decreasing-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Time-interval decreasing',
                                x_axis_name = expression("Population value set for"~beta[fixed]),
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -60, beta_upper = 70, ticks = 10)
```

```{r exp1-density-plot-time-dec, include=F, eval=F}
##generate figure showing density distributions if gamma_fizxd at N = 500 and N = 1000 with middle 95% area shaded. Emphasize
##that longer error bars in  N = 1000 are likely a statistical artifact of the random number generating procedure. 
exp_1 <- read_csv('data/exp_1_zero.csv') %>% filter(code == 0) #load data 
exp_1 <- convert_raw_var_to_sd(raw_data = exp_1) #var to sd conversion 

parameter_names <- c(bquote(expr = bold(A:~gamma[random]~(beta[fixed]~" = 80, 5 measurements"))),
                     bquote(expr =bold(B:~gamma[random]~(beta[fixed]~" = 80, 7 measurements"))))
                     
#1) Extract estimates for four parameter gamma_fixed and beta_rand when N =500 or 1000, num_measurements == 5, spacing == equal. 
##Convert to long format, with parameter name going into unique column and estimate becoming new column 
param_data <- exp_1 %>%
  filter(number_measurements <= 7, measurement_spacing == 'time_dec', midpoint == 80) %>%
  select(locate_ivs(exp_1),'gamma_rand') %>%
  pivot_longer(cols = c(gamma_rand), values_to = 'estimate') %>%
  unite(col = 'parameter', c(number_measurements,name))

#2) Replace parameter values with tag labels. 
param_data$parameter <- factor(param_data$parameter, 
                               levels = c("5_gamma_rand", "7_gamma_rand"), 
                               labels = parameter_names)

base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3, alpha = 0) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

param_search_names <- c("A\\:", "B\\:")

param_error_bar_ranges <- lapply(X = param_search_names, compute_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


density_time_dec_exp_1 <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = density_time_dec_exp_1, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_dec_exp1.pdf')

```

```{r exp1-density-plot-time-dec-fixed, include=F, eval=F}
param_error_bar_ranges <- lapply(X = param_search_names, compute_80_ind_param_error_bar_range, param_data = param_data)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


time_dec_fixed_density <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 1, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, 0.60))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = time_dec_fixed_density, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_time_dec_exp1_fixed.pdf')

```

```{r text-values-time-decreasing-exp1, echo=F}
beta_time_dec_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_dec_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_dec_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_time_dec_7_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 7, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

Figure \ref{fig:exp1_plot_time_decreasing} shows the parameter
estimation plots for the day-unit parameters when equal spacing was used
(error bars represent the middle 95% of estimated values and shaded
horizontal lines indicate the population values). Panels A--B show the
parameter estimation plots for the fixed-effect days-to-halfway
elevation and triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively. Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that, in Panel A, estimates for
$\upbeta_{fixed}$ are centered around their corresponding population
values (80, 180, and 280 days), and so the y-axis represents error (in
number of days) from the population value. Note that Table \ref{tab:omega-exp1-time-dec} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under time-interval decreasing spacing. 

For all simulations presented in Figure
\ref{fig:exp1_plot_time_decreasing}, no instance of bias occurred, as
the average estimated values for all day-unit parameters (as indicated
by the black dots) were close to their respective population values
(indicated by the gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with time-interval decreasing
spacing. First, for all day-unit parameters except the random-effect
halfway-triquarter delta ($\upgamma_{random}$; Figure
\ref{fig:exp1_plot_time_decreasing}D), variability increased
considerably with five measurements and the population value set for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$)
decreased from 280 to 80 days. As an example, the error bar length for
the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$;
Figure \ref{fig:exp1_plot_time_decreasing}A) spanned approximately
`r beta_time_dec_5_280$upper_ci - beta_time_dec_5_280$lower_ci`(upper
bound of `r beta_time_dec_5_280$upper_ci` days to a lower bound of
`r beta_time_dec_5_280$lower_ci` days) and
`r beta_time_dec_5_180$upper_ci - beta_time_dec_5_180$lower_ci` days
(upper bound of `r beta_time_dec_5_180$upper_ci` days to a lower bound
of `r beta_time_inc_5_180$lower_ci` days) when $\upbeta_{fixed}$ had a
population value of 280 and 180 days, respectively, but the error bar
length increased to
`r beta_time_dec_5_80$upper_ci - beta_time_dec_5_80$lower_ci` days
(upper bound of `r beta_time_dec_5_80$upper_ci` days to a lower bound of
`r beta_time_dec_5_80$lower_ci` days) $\upbeta_{fixed}$ had a population
value of 80. Second, the large amounts of variability observed for the
parameters with five measurements largely disappeared with seven or more
measurements. As an example, when the population value for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$;
Figure \ref{fig:exp1_plot_time_decreasing}A) was set to 80, the long
error bar range of
`r beta_time_dec_5_80$upper_ci - beta_time_dec_5_80$lower_ci` days with
five measurements decreased to a shorter length of
`r beta_time_dec_7_80$upper_ci - beta_time_dec_7_80$lower_ci` days with
seven measurements.

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Time-Interval Decreasing Spacing in Experiment 1}
  \label{fig:exp1_plot_time_decreasing}
  \includegraphics{Figures/exp1_plot_days_time-interval decreasing.png} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-time-dec} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp1-time-dec, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'time_dec', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'M', 'NM x M'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Decreasing Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), M = population value set for $\\\\upbeta_{fixed}$ (80, 180, 280), NM x M = interaction between number of measurements and population value set for $\\\\upbeta_{fixed}$. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_decreasing}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_decreasing}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_decreasing}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_decreasing}D)'))
```

```{=tex}
\begin{figure} [H]
  \caption{Density Plots of the Random-Effect Halfway-Triquarter Delta ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_time_decreasing}D) With Time-Interval Decreasing Spacing in Experiment 1 (95\% Error Bars)}
  \label{fig:exp1_density_time_decreasing}
  \includegraphics[height = 15cm, width = 30cm]{Figures/density_plots_time_dec_exp1} \hfill{}
  \caption*{Note. \textup{Regions shaded in in gray represent the the middle 95\% of estimated values and the width of the shaded regions is indicated by the length of the horizontal error bars. The error bar length increases as the number of measurements increases from five to seven.}}
\end{figure}
```
One additional minor point should be mentioned. In Figure
\ref{fig:exp1_density_time_decreasing}D, the variability of values
estimated for the random-effect triquarter-halfway delta
($\upgamma_{fixed}$) parameter counterintuitively increased as the
number of measurements increased from five to seven (see the estimates
located within the dashed rectangle). Inspection of the underlying
density plots for the random-effect triquarter-halfway delta parameter
($\upgamma_{fixed}$) in Figure \ref{fig:exp1_density_time_decreasing}
showed that the increase was not due to outliers and, rather, occurred
because of a slight increase in the variability of the parameter
stimates. When the fixed-effect days-to-halfway elevation parameter
($\upbeta_{fixed}$) was set to 280, the density distribution for the
random-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$)
became slightly flatter as the number of measurements increased from
five (Figure \ref{fig:exp1_density_time_decreasing}A) to seven (Figure
\ref{fig:exp1_density_time_decreasing}B).

#### Middle-and-Extreme Spacing

```{r plots-mid-ext-exp1, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_1_analytical, target_col = 'measurement_spacing', target_value = 'Middle-and-extreme spacing',
                                x_axis_name = expression("Population value set for"~beta[fixed]), 
                                x_axis_var = 'midpoint', exp_num = 'exp1_', beta_lower = -80, beta_upper = 80, ticks = 10)
```

```{r text-values-mid-ext-exp1, echo=F}
beta_mid_ext_5_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, midpoint == 80, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))

beta_mid_ext_5_180 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, midpoint == 180, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_mid_ext_5_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))


beta_mid_ext_7_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 7, midpoint == 280, grepl('beta\\[fixed\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))


beta__rand_mid_ext_7_280 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 7, midpoint == 280, grepl('beta\\[random\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta__rand_mid_ext_7_80 <- exp_1_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 7, midpoint == 80, grepl('beta\\[random\\]', parameter)) %>%
  select(midpoint, lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

Figure \ref{fig:exp1_plot_time_mid_ext} shows the parameter estimation
plots for the day-unit parameters when equal spacing was used (error
bars represent the middle 95% of estimated values and shaded horizontal
lines indicate the population values). Panels A--B show the parameter
estimation plots for the fixed-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively. Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that, in Panel A, estimates for
$\upbeta_{fixed}$ are centered around their corresponding population
values (80, 180, and 280 days), and so the y-axis represents error (in
number of days) from the population value. Note that Table \ref{tab:omega-exp1-time-inc} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under middle-and-extreme spacing. 

For all simulations presented in Figure
\ref{fig:exp1_plot_time_mid_ext}, one instance of bias occurred. With
five measurements and a population value for the fixed-effect
days-to-halfway elevation parameter ($\upbeta_{fixed}$) set to either 80
or 280, the random-effect days-to-halfway elevation parameter
($\upbeta_{random}$; Figure \ref{fig:exp1_plot_time_mid_ext}C) was
slightly overestimated, with respective average estimated values of
`r beta_mid_ext_5_80` and `r beta_mid_ext_5_280` days (relative to a
population value of 10.00 days). For all other conditions across all
day-unit parameters in Figure \ref{fig:exp1_plot_time_mid_ext}, no
instance of bias occurred, as the average estimated values for all
day-unit parameters (as indicated by the black dots) were close to their
respective population values (indicated by the gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with middle-and-extreme spacing.
First, for all the day-unit parameters except the random effect of the
halfway-triquarter delta parameter ($\upgamma_{random}$), variability
increased considerably with five measurements and the fixed-effect
days-to-halfway elevation parameter ($\upbeta_{fixed}$) had population
values of 80 or 280. As an example, the error bar length for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$)
spanned approximately
`r beta_mid_ext_5_180$upper_ci - beta_mid_ext_5_180$lower_ci` days when
the $\upbeta_{fixed}$ had a population value of 180 (upper bound of
`r beta_mid_ext_5_180$upper_ci` days and a lower bound of
`r beta_mid_ext_5_180$lower_ci` days), but the length increased to
`r beta_mid_ext_5_80$upper_ci - beta_mid_ext_5_80$lower_ci` days (upper
bound of `r beta_mid_ext_5_80$upper_ci` days and a lower bound of
`r beta_mid_ext_5_80$lower_ci` days) when $\upbeta_{fixed}$ had a
population value of 80 and
`r beta_mid_ext_5_280$upper_ci - beta_mid_ext_5_280$lower_ci` days
(upper bound of `r beta_mid_ext_5_280$upper_ci` days and a lower bound
of `r beta_mid_ext_5_280$lower_ci` days) when $\upbeta_{fixed}$ had a
population value of 280. Second, the large amounts of variability
observed for the parameters with five measurements largely disappeared
when seven or more measurements were taken. As an example, when the
fixed-effect days-to-halfway elevation parameter($\upbeta_{fixed}$) had
a population value of 280, the error bar range for the
estimation$\upbeta_{fixed}$ spanned approximately
`r beta_mid_ext_5_280$upper_ci - beta_mid_ext_5_280$lower_ci` days with
five measurements, but the error bar length to approximately
`r beta_mid_ext_7_280$upper_ci - beta_mid_ext_7_280$lower_ci` days with
seven measurements. Note that variability remained considerably high for
the random-effect days-to-halfway elevation parameter
($\upbeta_{random}$) with seven measurements the fixed-effect
days-to-halfway elevation parameter ($\upbeta_{fixed}$) had population
values of 80 and 280, with respective ranges of
`r beta__rand_mid_ext_7_80$upper_ci - beta__rand_mid_ext_7_80$lower_ci`
and
`r beta__rand_mid_ext_7_280$upper_ci - beta__rand_mid_ext_7_280$lower_ci`
days.

```{=tex}
\begin{figure}[H]
  \caption{Parameter Estimation Plots for Day-Unit Parameters With Middle-and-Extreme Spacing in Experiment 1}
  \label{fig:exp1_plot_time_mid_ext}
  \includegraphics{Figures/exp1_plot_days_middle-and-extreme spacing} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that, in Panel A, estimates for $\upbeta_{fixed}$ are centered around their corresponding population values (80, 180, and 280 days), and so the y-axis represents error (in number of days) from the population value. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-mid-ext} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp1-mid-ext, echo=F}
print_bias_var_omega_table(exp_data = exp_1_raw, target_col = 'measurement_spacing', target_value = 'mid_ext', 
ind_vars = c('number_measurements', 'midpoint'), 
ind_var_acronyms = c('NM', 'M', 'NM x M'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Middle-and-Extreme Spacing in Experiment 1',
footnote = 'NM = number of measurements (5, 7, 9, 11), M = population value set for $\\\\upbeta_{fixed}$ (80, 180, 280), NM x M = interaction between number of measurements and population value set for $\\\\upbeta_{fixed}$. \\\\phantom{ indicate conditions where}', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp1_plot_time_mid_ext}D)'))
```



## Discussion of Experiment 1

```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# Experiment 2

```{=tex}
\nointerlineskip
\vfill
\newpage
```

Experiment 2 investigated how the modelling accuracy of a nonlinear pattern was affected under conditions characterized by different measurement spacing schedules, measurement numbers, and sample sizes (see Table \ref{tab:experimentOverview}). Convergence success rate was computed for each cell and percent bias was computed for each parameter in each cell. Variables held constant were the nature of change (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] = 180), the distribution of errors over time (independent and identically distributed), and absence of missing data. 


### Variables Used in Simulation Experiment 

#### Independent Variables
##### Number of Measurements

(ref:loehlin2017) [@loehlin2017]

The exact set of values used by @coulombe2016 for the number of
measurements could not be used in my simulations because doing so would
have created non-identified models. Specifically, the smallest value
used for the number of measurements in @coulombe2016 of 3 measurements
could not be used in my simulations because it would not have provided
sufficient degrees of freedom for estimating the nonlinear latent growth
curve model in my simulations. The model used in my simulations
estimated 9 parameters (*p* = 9; 4 fixed-effects + 4 random-effects + 1
error) and so the minimum number of measurements (or observed variables)
required for model identification (and to allow model comparison) would
was 4.\footnote{Degrees of freedom is
calculated by multiplying the number of observed variables (\textit{p})
by \textit{p} + 1 and dividing it by 2 (\textit{p}[{\textit{p} +
1}]/2; see (ref:loehlin2017)}Because my proposed simulation experiments
were intended to map onto the manipulations used by @coulombe2016, the
second-smallest value used for the number of measurements in
@coulombe2016 was 5 (see Table \ref{tab:coulombe2016}), and so my
simulations used 5 measurements as the smallest measurement number
value. Importantly, a larger value of 11 was added to test for a
possible effect of a high measurement number. Therefore, my simulation
experiments used the following values in manipulating the number of
measurements: 5, 7, 9, and 11 (see Table \ref{tab:myValues}).

##### Spacing of Measurements
#### Sample Size

Sample size values were borrowed from @coulombe2016 with one difference.
Because my experiments investigated the effects of measurement timing
factors on the ability to model nonlinear patterns, which are inherently
more complex than linear patterns of change, a sample size value of *n*
= 1000 was added as the largest sample size. Therefore, the following
values were used for my sample size manipulation: 30, 50, 100, 200, 500,
and 1000 (see Table \ref{tab:myValues}). Importantly, in experiments where
sample size was not manipulated (i.e., Experiment 1), the sample size
value used corresponded to the average sample size used in
organizational research [*n* = 225\; @bosco2015].


#### Dependent Variables

##### Convergence Success Rate

See section on [convergence success rate][convergence success rate]

##### Bias

See section on [bias][bias]


### Overview of Data Generation and Analysis

#### Data Generation

See section on [data generation][Data Generation]

##### Population Values Used for Logistic Function Parameters
See section on [population values][Population Values Used for Logistic Function Parameters]
#### Data Analysis
##### Nonlinear Latent Growth Curve Model Used to Analyze Each Generated Data Set
See section ... 

##### Analysis of Dependent Variables

##### Analysis of Convergence Success Rate

##### Analysis of Bias


##### Analysis of Variability in Parameter Estimation

### Results

#### Equal Spacing

```{r plots-equal-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Equal spacing',
                               x_axis_name = expression("Sample size ("*italic(N)*")"), 
                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 160, beta_upper = 210,
                               ticks = 5)
```

```{r exp2-density-plot-equal, include=F, eval=F}
##generate figure showing density distributions if gamma_fizxd at N = 500 and N = 1000 with middle 95% area shaded. Emphasize
##that longer error bars in  N = 1000 are likely a statistical artifact of the random number generating procedure. 
exp_2 <- read_csv('data/exp_2.csv') %>% filter(code == 0) #load data 
exp_2 <- convert_raw_var_to_sd(raw_data = exp_2) #var to sd conversion 

parameter_names <- c(bquote(expr = bold(A:~gamma[fixed]~"(N=500, 5 measurements)")),
                     bquote(expr = bold(B:~beta[random]~"(N=500, 5 measurements)")),
                     bquote(expr = bold(C:~gamma[fixed]~"(N=1000, 5 measurements)")),
                     bquote(expr = bold(D:~beta[random]~"(N=1000, 5 measurements)")))
                     
#1) Extract estimates for four parameter gamma_fixed and beta_rand when N =500 or 1000, num_measurements == 5, spacing == equal. 
##Convert to long format, with parameter name going into unique column and estimate becoming new column 
param_data_equal_exp_2 <- exp_2 %>%
  filter(number_measurements == 5, measurement_spacing == 'equal', sample_size %in% c(500, 1000)) %>%
  select(locate_ivs(exp_2),'gamma_fixed', 'beta_rand') %>%
  pivot_longer(cols = c(gamma_fixed, beta_rand), values_to = 'estimate') %>%
  unite(col = 'parameter', sample_size:name) 

#2) Replace parameter values with tag labels. 
param_data_equal_exp_2$parameter <- factor(param_data_equal_exp_2$parameter, 
                               levels = c("500_gamma_fixed", "500_beta_rand",
                                          "1000_gamma_fixed", "1000_beta_rand"), 
                               labels = parameter_names)

base_density_plot <- ggplot(data = param_data_equal_exp_2, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

param_search_names <- c("A\\:", "B\\:", "C\\:", "D\\:")

param_error_bar_ranges <- lapply(X = param_search_names, compute_ind_param_error_bar_range, param_data = param_data_equal_exp_2)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data_equal_exp_2, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
density_data <- ggplot_build(plot = base_density_plot)$data[[1]]

plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


density_plot_equal_exp2 <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
    facet_wrap_custom( ~ parameter, scales = "free", ncol = 2, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(3,scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(4, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.90, by = 0.10),
                                                                                                   limits = c(0, .90))))) + 
  
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 60, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = density_plot_equal_exp2, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plot_fixed_equal_exp2.pdf')

```

```{r exp2-density-plot-equal-fixed, include=F, eval=F}
param_error_bar_ranges <- lapply(X = param_search_names, compute_90_ind_param_error_bar_range, param_data = param_data_equal_exp_2)

#3) Create base density plot so that density data can be created
base_density_plot <- ggplot(data = param_data_equal_exp_2, mapping = aes(x = estimate, group = parameter)) +
   geom_density(size = 3) 

##create density data for each parameter 
plot_ready_data <- rbindlist(pmap(.l = list(error_bar_range = param_error_bar_ranges, 
               group = 1:length(param_error_bar_ranges), 
               param_name =  as.character(parameter_names)), .f = compute_density, density_data = density_data))
plot_ready_data$parameter <- factor(plot_ready_data$parameter)


density_equal_fixed_exp2 <- base_density_plot +
    scale_x_continuous(limits = c(0, 30), breaks = seq(from = 0, to = 30, by = 5), 
                       name = 'Value of parameter estimate (days)') + 
  
  #shaded filling
      geom_area(data = plot_ready_data, mapping = aes(x = day_value, y = probability), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 3) + 
    #error bar
    geom_errorbarh(data = plot_ready_data, inherit.aes = F,
                   mapping = aes(xmin = lower_ci, xmax = upper_ci, 
                                 y = max_density_value, height = 0.05), size = 3) + 

    #vertical dashed lines for error bars
    ##lower limit 
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$lower_ci, xend = plot_ready_data$lower_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +
    ##upper limit
    geom_segment(inherit.aes = F, data = plot_ready_data, x = plot_ready_data$upper_ci, xend = plot_ready_data$upper_ci,
                 y = 0, yend = plot_ready_data$max_density_value, linetype = 2, size = 2) +  
    
    #facet_wrap(facets = ~ parameter, nrow = 2, ncol = 2) + 
    
     facet_wrap_custom( ~ parameter, scales = "free", ncol = 2, nrow = 2 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(scale_override(1,
                                                                                scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(2, scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(3,scale_y_continuous(name =  'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.60, by = 0.10),
                                                                                                   limits = c(0, .60))),
                                                                 scale_override(4, scale_y_continuous(name = 'Density (proportion of estimates)',
                                                                                                   breaks = seq(from = 0, to = 0.90, by = 0.10),
                                                                                                   limits = c(0, .90))))) + 
    #plot aesthetics
    theme_classic() +

    theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 55, margin = unit(c(t = 0, r = 6, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
      axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
  
  
  #create PDF of faceted plot
  set_panel_size(p = density_equal_fixed_exp2, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/density_plots_equal_fixed_exp2.pdf')

```

```{r plots-equal-exp2-fixed, include=F, eval=F}
exp_2_analytical$days$upper_ci <- exp_2_analytical$days$upper_ci_90
exp_2_analytical$days$lower_ci <- exp_2_analytical$days$lower_ci_90

generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Equal spacing',
                               x_axis_name = 'Sample size (*N*)', 
                               x_axis_var = 'sample_size', exp_num = 'exp2_fixed_', beta_lower = 165, beta_upper = 205,
                               ticks = 5)
```

```{r text-values-equal-exp2, echo=F}
#fixed-effect halfway-triquarter delta (5, 100)
gamma_fixed_equal_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_equal_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter(7, 100)
gamma_fixed_equal_7_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 7, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect halfway-triquarter delta (5, 200)
gamma_rand_equal_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))
```


Figure \ref{fig:exp2_plot_equal} shows the parameter estimation plots
for the day-unit parameters when equal spacing was used (error bars
represent the middle 95% of estimated values and shaded horizontal lines
indicate the population values). Panels A--B show the parameter
estimation plots for the fixed-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively.Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp2-equal} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under equal spacing. 

For all simulations presented in Figure \ref{fig:exp2_plot_equal}, one
instance of bias occurred. With five measurements and a sample size no
larger than 200, estimation of the random-effect halfway-triquarter
delta parameter ($\upgamma_{random}$; Figure \ref{fig:exp2_plot_equal}D)
was considerably overestimated. For example, the average population
value estimated for the random-effect halfway-triquarter delta parameter
($\upgamma_{random}$) parameter was `r gamma_rand_equal_5_200$estimate`
days with five measurements and a sample size of 200 (relative to a
population value of 4.00 days). Across all other conditions in Figure
\ref{fig:exp1_plot_time_mid_ext}, no instance of bias occurred, as the
average estimated values for all day-unit parameters (as indicated by
the black dots) were close to their respective population values
(indicated by the gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with equal spacing.First, for the
fixed- and random-effects of the halfway-triquarter delta parameters
($\upgamma_{fixed}$ and $\upgamma_{random}$), variability was
considerably high with five measurements and a sample size no larger
than 200. For example, the error bar length for the fixed-effect
halfway-triquarter delta parameter ($\upgamma_{fixed}$; Figure
\ref{fig:exp2_plot_equal}B) spanned approximately
`r gamma_fixed_equal_5_200$upper_ci -gamma_fixed_equal_5_200$lower_ci`
days (upper bound of `r gamma_fixed_equal_5_200$upper_ci` days to a
lower bound of `r gamma_fixed_equal_5_200$lower_ci` days) with five
measurements and a sample size of 200. Note that, similarly, variability
for the fixed- and random-effects of the days-to-halfway elevation
parameters ($\beta_{fixed}$ and $\beta_{random}$) was lower than the
variability for the halfway-triquarter delta parameters, but still
moderately high at sample sizes below 200 with five measurements. As an
example, the error bar length for the fixed-effect days-to-halfway
elevation parameter($\beta_{fixed}$; Figure \ref{fig:exp2_plot_equal}A)
spanned approximately
`r beta_fixed_equal_5_200$upper_ci - beta_fixed_equal_5_200$lower_ci`
days (upper bound of `r beta_fixed_equal_5_200$upper_ci` days to a lower
bound of `r beta_fixed_equal_5_200$lower_ci` days) with five
measurements and a sample size of 200. Second, variability across all
day-unit parameters was nearly trivial across all sample size levels
with seven or more measurements. For example, the error bar length for
the the fixed-effect halfway-triquarter delta parameter
($\upgamma_{fixed}$; Figure \ref{fig:exp2_plot_equal}B) spanned
approximately
`r gamma_fixed_equal_7_200$upper_ci - gamma_fixed_equal_7_200$lower_ci`
(upper bound of `r gamma_fixed_equal_7_200$upper_ci` days to a lower
bound of `r gamma_fixed_equal_7_200$lower_ci` days) days with seven
measurements and a sample size of 100.

```{r density-ranges, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 500)
beta_fixed_equal_5_500 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 500, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 500)
beta_fixed_equal_5_1000 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Equal spacing', number_measurements == 5, sample_size == 1000, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))
```

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Equal Spacing in Experiment 2}
  \label{fig:exp2_plot_equal}
  \includegraphics{Figures/exp2_plot_days_equal spacing} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-equal} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp2-equal, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'equal', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Equal Spacing in Experiment 2',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_equal}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_equal}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_equal}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_equal}D)'))
```

#### Time-Interval Increasing Spacing

```{r plots-time-increasing-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Time-interval increasing',
                                                             x_axis_name = expression("Sample size ("*italic(N)*")"), 

                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 160, beta_upper = 210,
                               ticks = 5)
```

```{r text-values-time-increasing-exp2, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_time_inc_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (7, 100)
beta_fixed_time_inc_7_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval increasing', number_measurements == 7, sample_size == 100, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp2_plot_time_increasing} shows the parameter
estimation plots for the day-unit parameters when equal spacing was used
(error bars represent the middle 95% of estimated values and shaded
horizontal lines indicate the population values). Panels A--B show the
parameter estimation plots for the fixed-effect days-to-halfway
elevation and triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively. Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp2-time-inc} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under time-interval increasing spacing. 


For all simulations presented in Figure
\ref{fig:exp2_plot_time_increasing}, no instance of bias occurred, as
the average estimated values for all day-unit parameters (as indicated
by the black dots) were close to their respective population values
(indicated by the gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with time-interval increasing
spacing. First, for the fixed- and random-effect days-to-halfway
elevation parameter ($\upbeta_{fixed}$, $\upbeta_{random}$), variability
was considerably high with five measurements and a sample size no larger
than 200. For example, the error bar length for the fixed-effect
days-to-halfway elevation parameter ($\upbeta_{fixed}$; Figure
\ref{fig:exp2_plot_time_increasing}A) spanned approximately
`r beta_fixed_time_inc_5_200$upper_ci - beta_fixed_time_inc_5_200$lower_ci`
days (upper bound of `r beta_fixed_time_inc_5_200$upper_ci` days to
lower bound of `r beta_fixed_time_inc_5_200$lower_ci` days) with five
measurements and a sample size of 200. Note that variability for the
fixed-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$;
Figure \ref{fig:exp2_plot_time_increasing}B) was considerably high with
five measurements and a sample size no larger than 100. Second,
variability across all day-unit parameters was nearly trivial with seven
measurements across all sample size levels. For example, the error bar
length for the fixed-effect days-to-halfway elevation parameter
($\upbeta_{fixed}$; Figure \ref{fig:exp2_plot_time_increasing}A) spanned
approximately
`r beta_fixed_time_inc_7_200$upper_ci - beta_fixed_time_inc_7_200$lower_ci`
days (upper bound of `r beta_fixed_time_inc_7_200$upper_ci` days to a
lower bound of `r beta_fixed_time_inc_7_200$lower_ci` days) with seven
measurements and a sample size of 200.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Interval Increasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_increasing}
  \includegraphics{Figures/exp2_plot_days_time-interval increasing} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-time-inc} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp2-time-inc, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'time_inc', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Increasing Spacing in Experiment 2',
footnote = 'NM = number of measurements, S = sample size, NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_time_increasing}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_time_increasing}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_time_increasing}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_time_increasing}D)'))
```

#### Time-Interval Decreasing Spacing

```{r plots-time-decreasing-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Time-interval decreasing',
                                                                                             x_axis_name = expression("Sample size ("*italic(N)*")"), 


                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 155, beta_upper = 210,
                               ticks = 5)
```

```{r text-values-time-decreasing-exp2, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_time_dec_5_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (7, 100)
beta_fixed_time_dec_7_200 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Time-interval decreasing', number_measurements == 7, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp2_plot_time_decreasing} shows the parameter
estimation plots for the day-unit parameters when equal spacing was used
(error bars represent the middle 95% of estimated values and shaded
horizontal lines indicate the population values). Panels A--B show the
parameter estimation plots for the fixed-effect days-to-halfway
elevation and triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively. Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp2-time-dec} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under time-interval decreasing spacing.  

For all simulations presented in Figure
\ref{fig:exp2_plot_time_decreasing}, no instance of bias occurred, as
the average estimated values for all day-unit parameters (as indicated
by the black dots) were close to their respective population values
(indicated by the gray lines).

Two general patterns of results emerged in the estimation of day-unit
parameters with time-interval decreasing spacing. First, for the fixed-
and random-effect days-to-halfway elevation parameter
($\upbeta_{fixed}$, ($\upbeta_{random}$), variability was considerably
high with five measurements and a sample size no larger than 200. For
example, the error bar length for the fixed-effect days-to-halfway
elevation parameter ($\upbeta_{fixed}$; Figure
\ref{fig:exp2_plot_time_decreasing}A) spanned approximately
`r beta_fixed_time_dec_5_200$upper_ci - beta_fixed_time_dec_5_200$lower_ci`
days (upper bound of `r beta_fixed_time_dec_5_200$upper_ci` days to
lower bound of `r beta_fixed_time_dec_5_200$lower_ci` days) with five
measurements and a sample size of 200. Note that variability for the
fixed-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$) was
considerably high with five measurements and a sample size no larger
than 100. Second, variability across all day-unit parameters was nearly
trivial with seven measurements across all sample size levels. For
example, the error bar length for the fixed-effect days-to-halfway
elevation parameter ($\beta_{fixed}$; Figure
\ref{fig:exp2_plot_time_decreasing}A) spanned approximately
`r beta_fixed_time_dec_7_200$upper_ci - beta_fixed_time_dec_7_200$lower_ci`
days (upper bound of `r beta_fixed_time_dec_7_200$upper_ci` days to a
lower bound of `r beta_fixed_time_dec_7_200$lower_ci` days) with seven
measurements and a sample size of 100.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Interval Decreasing Spacing in Experiment 2}
  \label{fig:exp2_plot_time_decreasing}
  \includegraphics{Figures/exp2_plot_days_time-interval decreasing} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-time-dec} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp2-time-dec, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'time_dec', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Interval Decreasing Spacing in Experiment 2',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_time_decreasing}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_time_decreasing}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_time_decreasing}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_time_decreasing}D)'))
```

#### Middle-and-Extreme Spacing

```{r plots-mid-ext-exp2, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_2_analytical, 
                               target_col = 'measurement_spacing', target_value = 'Middle-and-extreme spacing',
                                                             x_axis_name = expression("Sample size ("*italic(N)*")"), 

                               x_axis_var = 'sample_size', exp_num = 'exp2_', beta_lower = 160, beta_upper = 210,
                               ticks = 5)
```

```{r text-values-mid-ext-exp2, echo=F}
#fixed-effect days-to-halfway elevation parameter (5, 30)
beta_fixed_mid_ext_5_30 <- exp_2_analytical$days %>% 
  filter(measurement_spacing == 'Middle-and-extreme spacing', number_measurements == 5, sample_size == 30, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```


Figure \ref{fig:exp2_plot_time_mid_ext} shows the parameter estimation
plots for the day-unit parameters when equal spacing was used (error
bars represent the middle 95% of estimated values and shaded horizontal
lines indicate the population values). Panels A--B show the parameter
estimation plots for the fixed-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{fixed}$ and
$\upgamma_{fixed}$), respectively. Panels C--D show the parameter
estimation plots for the random-effect days-to-halfway elevation and
triquarter-halfway delta parameters ($\upbeta_{random}$ and
$\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp2-mid-ext} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters under middle-and-extreme spacing. 

For all simulations presented in Figure
\ref{fig:exp2_plot_time_mid_ext}, no instance of bias occurred, as the
average estimated values for all day-unit parameters (as indicated by
the black dots) were close to their respective population values
(indicated by the gray lines).

In addition to showing little to no systematic bias, parameter estimates
also showed little to no variability with middle-and-extreme measurement
spacing. The longest error bar length across all day-unit parameters
across all sample size-measurement-number conditions occurred for the
fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$;
Figure \ref{fig:exp2_plot_time_mid_ext}A), with an error bar length of
`r beta_fixed_mid_ext_5_30$upper_ci - beta_fixed_mid_ext_5_30$lower_ci`
days (upper bound of `r beta_fixed_mid_ext_5_30$upper_ci` days to a
lower bound of `r beta_fixed_mid_ext_5_30$lower_ci` days) with five
measurements and a sample size of 30.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Middle-and-Extreme Spacing in Experiment 2}
  \label{fig:exp2_plot_time_mid_ext}
  \includegraphics{Figures/exp2_plot_days_middle-and-extreme spacing} \hfill{}
  \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp1-alpha-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp2-mid-ext} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp2-mid-ext, echo=F}
print_bias_var_omega_table(exp_data = exp_2_raw, target_col = 'measurement_spacing', target_value = 'mid_ext', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Middle-and-Extreme Spacing in Experiment 2',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp2_plot_time_mid_ext}D)'))
```

```{r plot-summary-exp2, include=F, eval=F}

generate_summary_facet_plot(condition_data = cond_summary_exp_2, lower_y_limit = -6, upper_y_limit = 20, ticks = 2, exp_num = 'Figures/exp2_', 
                            y_axis_var = 'mean_perc_error', y_axis_name = 'Parameter bias (percentage error)', 
                            x_axis_var = 'sample_size', x_axis_name = 'Sample size (*N*)')

```


## Discussion of Experiment 2 

```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# Experiment 3

```{=tex}
\nointerlineskip
\vfill
\newpage
```

Experiment 3 investigated how the modelling accuracy of a nonlinear pattern was different under combinations of measurement number and sample size was affected by different levels of time structuredness (see Table \ref{tab:experimentOverview}). Convergence success rate was computed for each cell and percent bias was computed for each parameter in each cell. Variables held constant were the nature of change (fixed-effect days-to-halfway elevation parameter [$\upbeta_{fixed}$] = 180), measurement spacing (equal), the distribution of errors over time (independent and identically distributed), and absence of missing data. 

```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

## Methods

### Overview of Variables Used in Simulations

### Independent Variables
#### Number of Measurements

(ref:loehlin2017) [@loehlin2017]

The exact set of values used by @coulombe2016 for the number of
measurements could not be used in my simulations because doing so would
have created non-identified models. Specifically, the smallest value
used for the number of measurements in @coulombe2016 of 3 measurements
could not be used in my simulations because it would not have provided
sufficient degrees of freedom for estimating the nonlinear latent growth
curve model in my simulations. The model used in my simulations
estimated 9 parameters (*p* = 9; 4 fixed-effects + 4 random-effects + 1
error) and so the minimum number of measurements (or observed variables)
required for model identification (and to allow model comparison) would
was 4.\footnote{Degrees of freedom is
calculated by multiplying the number of observed variables (\textit{p})
by \textit{p} + 1 and dividing it by 2 (\textit{p}[{\textit{p} +
1}]/2; see (ref:loehlin2017)}Because my proposed simulation experiments
were intended to map onto the manipulations used by @coulombe2016, the
second-smallest value used for the number of measurements in
@coulombe2016 was 5 (see Table \ref{tab:coulombe2016}), and so my
simulations used 5 measurements as the smallest measurement number
value. Importantly, a larger value of 11 was added to test for a
possible effect of a high measurement number. Therefore, my simulation
experiments used the following values in manipulating the number of
measurements: 5, 7, 9, and 11 (see Table \ref{tab:myValues}).


#### Sample Size

Sample size values were borrowed from @coulombe2016 with one difference.
Because my experiments investigated the effects of measurement timing
factors on the ability to model nonlinear patterns, which are inherently
more complex than linear patterns of change, a sample size value of *n*
= 1000 was added as the largest sample size. Therefore, the following
values were used for my sample size manipulation: 30, 50, 100, 200, 500,
and 1000 (see Table \ref{tab:myValues}). Importantly, in experiments where
sample size was not manipulated (i.e., Experiment 1), the sample size
value used corresponded to the average sample size used in
organizational research [*n* = 225\; @bosco2015].

#### Time Structuredness {#sec:time-structuredness}

(ref:coulombe2016f) [@coulombe2016]

The manipulation of time structuredness was adopted from the
manipulation used in @coulombe2016 with a slight modification. In
@coulombe2016, time-unstructured data were generated according to an
exponential pattern such that most data were obtained at the beginning
of the response window, with a smaller amount of data being obtained
towards the end of the response window. Importantly, @coulombe2016
employed a non-continuous function for generating time-unstructured
data: A binning method was employed such that 80% of the data were
obtained within a time period equivalent to 12% (fast response rate) or
30% (slow response rate) of the entire response window. Using a response
window length of 10 days with a fast response rate, the procedure
employed by @coulombe2016 for generating time-unstructured data would
have generated the following percentages of data in each of the four bins [note that, using the data generation procedure for @coulombe2016, the effective response window length was 4 days instead of 10 days]\footnote{The data generation procedure in (ref:coulombe2016) for a fast response rate assumed that all of the data were
collected within the initial 40\% length of the nominal response window length (4 days
in the current example).}:

1)  Bin 1: 60% of the data would be generated in the initial 10% length
    of the response window (0--0.4 day).
2)  Bin 2: 20% of the data would be generated in the next 20% length of
    the response response window (0.4--1.2 days).
3)  Bin 3: 10% of the data would be generated in the next 30% length of
    the response window (1.2--2.4 days).
4)  Bin 4: the remaining 10% of the data would be generated in the
    remaining 40% length of the response window (2.4--4 days).

\noindent Note that, summing the data percentages and time durations
from the first two bins yields an 80% cumulative response rate that is
obtained in the initial 12% length of the full-length response window of 10 days (i.e., $(\frac{1.2}{10})*100\% = 12\%$). Also note
that, in @coulombe2016, a data point in each bin was randomly assigned a
measurement time within the bin's time range. In the current example
where the full-length response window had a length of 10 days, a data point obtained
in the first bin would be randomly assigned a measurement time between
0--0.4).

Although @coulombe2016 generated time-unstructured data to
resemble data collection conditions---response rates have been shown to
follow an exponential pattern [@pan2010; @dillman2014]---the use of a
pseudo-continuous binning function for generating time-unstructured data
lacked ecological validity. Therefore, the simulations here used a
continuous function to create more realistic versions of
time-unstructured data. Specifically, the exponential function shown
below in Equation \@ref(eq:exponential) was used:
\useshortskip
```{=tex}
\begin{align}
  y =  M(1 - e^{-ax})
  (\#eq:exponential)
\end{align}
```
\useshortskip
\noindent  where $x$ stores the time delay for a measurement at a particular time point, $y$ represents the cumulative response percentage achieved at a given $x$ time delay, $a$ sets the rate of growth of the cumulative response percentage over time, and $M$ sets the range of possible $y$ values. Two important points need to be made with respect to the $M$ parameter (range of possible $y$ values) and the response window length used in
the current simulations. First, because the range of possible values for the
cumulative response percentage ($y$) is 0--1 (data can be collected from a 0% to a maximum of 100% of respondents; $\{y: 0 \le y \le 1 \}$), the $M$
parameter had a value of 1 ($M = 1$). Second, the response window length
in the current simulations was 36 days, and so the range of possible time delay values was between 0--36 ($\{x: 0 \le x \le 36 \}$).\footnote{A value of 36 days was used because the generation of time-unstructured data had to remain independent of the manipulation of measurement number (i.e., the response window lengths used in generating time-unstructured data could not vary with the number of measurements). To ensure the manipulations of measurement number and time structuredness remained independent, the reponse window length had to remain constant for all measurement number conditions with equal spacing. Looking at Table @ref(tab:measurementDays), the longest possible response window that fit within all measurement number conditions with equal spacing was the interval length of the 11-measurement condition (i.e., 36 days).}

To replicate the time structuredness manipulation in @coulombe2016 using the continuous exponential function of Equation \@ref(eq:exponential),  the growth rate parameter ($a$) had to be calibrated to achieve a cumulative
response rate of 80% after either 12% or 30% of the response window
length of 36 days. The derivation below solves for $a$, with Equation
\ref{eq:growth-rate} showing the equation for $a$.

```{=tex}
\begin{align} 
y &= M(1 - e^{-ax}) \nonumber \\
y &= M - Me^{-ax} \nonumber \\
y &= 1 - e^{-ax} \nonumber \\
e^{-ax} &=  1 -y \nonumber \\ 
-ax\log(e) &= \log(1 - y) \nonumber \\ 
a &= \frac{\log(1 - y)}{-x} 
  (\#eq:growth-rate)
\end{align}
```

\noindent Because the target response rate was 80%, $y$ took on a value
of .80 ($y = .80$). Given that the response window length in the current
simulations was 36 days, $x$ took on a value of 4.32 (12% of 36) when
time-unstructured data were defined by a fast response rate and 10.80
(30% of 36) when time-unstructured data were defined by a slow response
rate. Using Equation \ref{eq:growth-rate} yielded the following growth
rate parameter values for fast and slow response rates ($a_{fast}$,
$a_{slow}$):

```{=tex}
\begin{align}
a_{fast} &= \frac{\log(1 - .80)}{-4.32} = 0.37 \nonumber \\ 
a_{slow} &= \frac{\log(1 - .80)}{-10.80} = 0.15 
\end{align}
```

\noindent Therefore, to obtain 80% of the data with a fast response rate
(i.e., in 4.32 days), the growth parameter ($a$) needed to have a value
of 0.37 ($a_{fast} = 0.37$) and, to obtain 80% of the data with a slow
response rate (i.e., in 10.80 days), the growth parameter ($a$) needed
to have a value of 0.15 ($a_{slow} = 0.15$). Using the above growth rate
values derived for the fast and slow response growth rate parameters
($a_{fast}$, $a_{slow}$), the following functions were generated for
fast and slow response rates:

```{=tex}
\begin{align}
  f_{fast}(x) =  M(1 - e^{a_{fast}x}) = M(1 - e^{-0.37x}) and 
  (\#eq:fast-cdf)
\end{align}
```
```{=tex}
\begin{align}
  f_{slow}(x)  =  M(1 - e^{-a_{slow}x}) =  M(1 - e^{-0.15x}). 
  (\#eq:slow-cdf)
\end{align}
```
\noindent Using Equations \ref{eq:fast-cdf}--\ref{eq:slow-cdf}, Figure
\ref{fig:cdf_plots} shows the resulting cumulative distribution
functions (CDF) for time-unstructured data that show the cumulative
response percentage as a function of time. Panel A shows the cumulative
distribution function for a fast response rate (Equation
\ref{eq:fast-cdf}), where an 80% response rate was obtained in 4.32
days. Panel B shows the cumulative distribution function for a slow
response rate (Equation \ref{eq:slow-cdf}), where an 80% response rate
was obtained in 10.80 days.

```{r cdf-response-rate, include=F, eval=F}

#1) Generate CDFs
day <- seq(from = 0, to = 36, by = 0.01)
M <- 1
satiation_value <- 0.8
satiation_point_fast <- 4.32
satiation_point_slow <- 10.80

a_fast <- log(1 - satiation_value)/-satiation_point_fast
a_slow <- log(1 - satiation_value)/-satiation_point_slow

#y data
y_fast <- M*(1 - exp(-a_fast*day))
y_slow <- M*(1 - exp(-a_slow*day))

cdf_data <- data.frame('response_rate' = factor(c(rep('fast', times = length(y_fast)), 
                                           rep('slow', times = length(y_slow)))), 
                       'day' = rep(day, times = 2), 
                       'cumulative_response' = c(y_fast, y_slow))

cdf_data$response_rate <- factor(cdf_data$response_rate, 
                                 labels = c(bquote(expr = "bold(A:~CDF~(Fast~response~rate))"),
                                            bquote(expr =  "bold(B:~CDF~(Slow~response~rate))")))
                       
v_line_data <- data.frame(
  response_rate =c("bold(A:~CDF~(Fast~response~rate))", "bold(B:~CDF~(Slow~response~rate))"), 
  x = c(4.32, 10.80), 
  x_end = c(4.32, 10.80), 
  y = c(0.8, 0.8), 
  y_end = c(0, 0))

h_line_data <- data.frame(
  response_rate = c("bold(A:~CDF~(Fast~response~rate))", "bold(B:~CDF~(Slow~response~rate))"), 
  x = c(0, 0), 
  x_end = c(4.32, 10.80), 
  y = c(0.8, 0.8), 
  y_end = c(0.8, 0.8))

cdf_plots <- ggplot(cdf_data, aes(x = day, y = cumulative_response)) + 
  geom_line(size = 3) + 
  scale_y_continuous(breaks = c(0, 0.25, 0.50, 0.80, 1.00)) +
  theme_classic(base_family = 'Helvetica') + 
  #facet_wrap(facets = ~ response_rate, scales = 'free', labeller = label_parsed) + 
  facet_wrap_custom( ~ response_rate, scales = "free", ncol = 2, nrow = 1 ,
                                          dir = 'h', labeller = label_parsed,
                                          scale_overrides = list(
                                          scale_override(1,
                                            scale_x_continuous(
                                              breaks = c(0, 4.32, 18, 24, 36),
                                              limits = c(0, 36))), 
                                          scale_override(2,
                                            scale_x_continuous(
                                              breaks = c(0, 10.80, 18, 24, 36),
                                              limits = c(0, 36))))) +  #vertical lines 
  geom_segment(data = v_line_data, mapping = aes(x = x, y = y, xend = x_end, yend = y_end), linetype = 2, size = 2) + 
  geom_segment(data = h_line_data, mapping = aes(x = x, y = y, xend = x_end, yend = y_end), linetype = 2, size = 2) + 

  labs(y = 'Cumulative response percentage', x = 'Response window day') + 
  
  theme(strip.text.x = element_text(face = 'bold', hjust = 0, size = 60, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),

              #axis details
      axis.text = element_text(size = 50, color = 'black'),
      axis.title = element_text(size = 70),
      #axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'), 
        strip.background = element_rect(fill = "white", color = "white"), 
        strip.text = element_text(hjust = 0, size = 16))


#create PDF of faceted plot
set_panel_size(p = cdf_plots, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/cdf_plots.pdf')

```

```{=tex}
\begin{figure}
  \caption{Cumulative Distribution Functions (CDF) With Fast and Slow Response Rates}
  \label{fig:cdf_plots}
  \includegraphics{Figures/cdf_plots}
  \caption*{Note. \textup{Panel A shows the cumulative distribution function for a fast response rate (Equation \ref{eq:fast-cdf}), where an 80\% response rate is obtained in 4.32 days.  Panel B shows the cumulative distribution function for a slow response rate (Equation \ref{eq:slow-cdf}), where an 80\% response rate is obtained in 10.80 days.}}
\end{figure}
```


### Dependent Variables

#### Convergence Success Rate


#### Bias

## Overview of Data Generation and Analysis

### Data Generation
.

#### Simulation Time Structuredness

To simulate time-unstructured data, response rates at each collection
point followed an exponential pattern described by either a fast or slow
response rate (for a review, see section on [time
structuredness](#sec:time-structuredness)). Importantly, data generated
for each person at each time point had to be sampled according to a
probability density function defined by either the fast or slow response
rate cumulative distribution function. In the current context, a
**probability density function** describes the probability of sampling
any given time delay value $x$ where the range of time delay values is
0--36 ($\{x : 0 \le x \le  36 \}$). To obtain the probability density functions
for fast and slow response rates, the response rate function shown in
Equation \@ref(eq:exponential) was differentiated with respect to $x$ to
obtain the function shown below in Equation \ref{eq:pdf-function}\footnote{Euler's notation for differentiation is used to represent derivatives. In words, $\frac{\partial f(x)}{\partial x}$ means that the derivative of the function $f(x)$ is taken with respect to $x$.}:

```{=tex}
\begin{align}
f^\prime = \frac{\partial f(x)}{\partial x} &= \frac{\partial}{\partial x}M(1 - e^{-ax}). \nonumber \\
&= M (e^{-ax}a)
(\#eq:pdf-function)
\end {align}
```

\noindent To compute the probability density function for the fast
response rate cumulative distribution function, the growth rate
parameter $a$ was set to 0.37 in Equation \ref{eq:pdf-function} to
obtain the following function in Equation \ref{eq:fast-pdf-function}:

```{=tex}
\begin{align}
f^\prime_{fast}(x) = M (e^{-a_{fast}x}a_{fast}) = M (e^{-0.37x}0.37). 
(\#eq:fast-pdf-function)
\end {align}
```
\noindent To compute the probability density function for the slow
response rate cumulative distribution function, the growth rate
parameter $a$ was set to 0.15 in Equation \ref{eq:pdf-function} to
obtain the following function in Equation \ref{eq:slow-pdf-function}:

```{=tex}
\begin{align}
f^\prime_{slow}(x) = M (e^{-0.15}a_{slow}) = M (e^{-0.15}0.15). 
(\#eq:slow-pdf-function)
\end {align}
```
Figure \ref{fig:cdf-pdf-plots} shows the fast and slow response
cumulative distribution functions (CDF) and their corresponding
probability density functions (PDF). Panel A shows the cumulative
distribution function for the fast response rate (with a growth
parameter value $a$ set to 0.37; see Equation \ref{eq:fast-cdf}) and
Panel B shows the probability density function that results from
computing the derivative of the fast response rate cumulative
distribution function with respect to $x$ (see Equation
\ref{eq:fast-pdf-function}). Panel C shows the cumulative distribution
function for the slow response rate (with a growth parameter value $a$
set to 0.15; see Equation \ref{eq:slow-cdf})) and Panel D shows the
probability density function that results from computing the derivative
of the slow response rate cumulative distribution function with respect
to $x$ (see Equation \ref{eq:slow-pdf-function} and section on [time
structuredness](#sec:time-structuredness) for more discussion). For the
fast response rate functions, an 80% response rate is obtained after
4.32 days or, equivalently, 80% of the area underneath the probability
density function is obtained at 4.32 days
($\int^{4.32}_{0} f_{fast}^\prime (x) = 0.80$; the integral from 0 to 4.32 of the probability density function for a fast response rate $f^\prime(x)_{fast}$ is 0.80). For the slow response
rate functions, an 80% response rate is obtained after 10.80 days or,
equivalently, 80% of the area underneath the probability density
function is obtained at 10.80 days
($\int^{10.80}_{0} f_{slow}^\prime (x) = 0.80$; the integral from 0 to 10.80 of the probability density function for a slow response rate $f^\prime(x)_{slow}$ is 0.80).

```{r pdf-time-structuredness, eval=F, include=F}
#1) Generate CDFs
day <- seq(from = 0, to = 36, by = 0.01)
M <- 1
satiation_value <- 0.8
satiation_point_fast <- 4.32
satiation_point_slow <- 10.80

a_fast <- log(1 - satiation_value)/-satiation_point_fast
a_slow <- log(1 - satiation_value)/-satiation_point_slow

#y data
y_fast <- M*(1 - exp(-a_fast*day))
y_slow <- M*(1 - exp(-a_slow*day))

#probability values
cdf_fast <- expression(M*(1 - exp(-a_fast*day)))
cdf_slow <- expression(M*(1 - exp(-a_slow*day)))

pdf_fast <- D(expr = cdf_fast, 'day')
pdf_slow <- D(expr = cdf_slow, 'day')

probability_values_fast<- eval(pdf_fast)
probability_values_slow <- eval(pdf_slow)


cdf_pdf_data <- data.frame('response_rate' = factor(c(rep('fast', times = length(y_fast)), 
                                           rep('slow', times = length(y_slow)))), 
                       'day' = rep(day, times = 2), 
                       'CDF' = c(y_fast, y_slow), 
                       'PDF' = c(probability_values_fast, probability_values_slow))

cdf_pdf_data_long <- cdf_pdf_data %>%
  pivot_longer(cols = c(CDF, PDF), names_to = 'prob_dist',
  names_ptypes = factor(levels = c('CDF', 'PDF'))) %>% 
  unite('dist_type', c('response_rate', 'prob_dist')) %>%
  mutate(dist_type = factor(dist_type, levels = c('fast_CDF', 'slow_CDF','fast_PDF', 
                                                   'slow_PDF')))

cdf_pdf_data_long$dist_type <- recode_factor(cdf_pdf_data_long$dist_type,   
                                             fast_CDF = 'bold(A:~CDF~(Fast~response~rate))', 
                                             slow_CDF = 'bold(C:~CDF~(Slow~response~rate))', 
                                             fast_PDF = 'bold(B:~PDF~(Fast~response~rate))', 
                                             slow_PDF = 'bold(D:~PDF~(Slow~response~rate))')
                
#lines showing 80% mark    
v_line_data <- data.frame(
  dist_type =c("bold(A:~CDF~(Fast~response~rate))", "bold(C:~CDF~(Slow~response~rate))"), 
  x = c(4.32, 10.80), 
  x_end = c(4.32, 10.80), 
  y = c(0.8, 0.8), 
  y_end = c(0, 0))

h_line_data <- data.frame(
  dist_type = c("bold(A:~CDF~(Fast~response~rate))", "bold(C:~CDF~(Slow~response~rate))"), 
  x = c(0, 0), 
  x_end = c(4.32, 10.80), 
  y = c(0.8, 0.8), 
  y_end = c(0.8, 0.8))

#needed for shading
pdf_shading_data <- cdf_pdf_data_long %>% 
  filter(str_detect(dist_type, pattern = 'bold\\(B') & day <= 4.32 | 
         str_detect(dist_type, pattern = 'bold\\(D') & day <= 10.80)

#needed for points 
point_data <- data.frame(
  dist_type =c(rep("bold(A:~CDF~(Fast~response~rate))", times = 1), 
               rep("bold(C:~CDF~(Slow~response~rate))", times = 1)),
  x = c(4.32, 10.80), 
  y = c(0.8, 0.8))

#arrows 
arrow_data <- data.frame(
  dist_type =c(rep("bold(A:~CDF~(Fast~response~rate))", times = 1), 
               rep("bold(C:~CDF~(Slow~response~rate))", times = 1)),
  xmin = c(4.32, 10.80), 
  xmax = c(10, 17), 
  ymin = c(0.8, 0.8), 
  ymax = c(0.25, 0.25))


#equations 
equation_data <- data.frame(
  dist_type =c(rep("bold(A:~CDF~(Fast~response~rate))", times = 4), 
               rep("bold(C:~CDF~(Slow~response~rate))", times = 4), 
               rep("bold(B:~PDF~(Fast~response~rate))", times = 3), 
               rep("bold(D:~PDF~(Slow~response~rate))", times = 3)),
  
  label = c("f[fast](x) == M(1 - e^{-a[fast]~x})", "a[fast] == 0.37", "M ==1", "0.80 == 1(1-e^{-0.37(4.32)})", 
            "f[slow](x) == M(1 - e^{-a[slow]~x})", "a[slow] == 0.15", "M == 1", "0.80 == 1(1-e^{-0.15(10.80)})",
            
            "f[fast]^{phantom() * minute }(x) * {phantom() == phantom()} * frac(partialdiff *f[fast](x),  partialdiff *x) * {phantom() == phantom()} * M(e^{a[fast]*x} * a[fast])",
            "integral(f[fast]^{phantom() * minute}, 0, 4.32)*(x) == f[fast](4.32) - f[fast](0)", "phantom() == 0.80", 
            
            "f[slow]^{phantom() * minute }(x) * {phantom() == phantom()} * frac(partialdiff *f[slow](x),  partialdiff *x) * {phantom() == phantom()} * M(e^{a[slow]*x} * a[slow])",
            "integral(f[slow]^{phantom() * minute}, 0, 10.80)*(x) == f[slow](10.80) - f[slow](0)", "phantom() == 0.80"), 
  x = c(18, 17, 18, 18, 
        24, 25, 24, 24, 
        22, 22, 20.5, 
        22, 22, 20.5),
  y = c(rep(c(0.8, 0.65, 0.50, 0.2), times = 2), 
        0.35, 0.15, 0.10, 
        0.35, 0.15, 0.10))


           
cdf_pdf_plot <- ggplot(cdf_pdf_data_long, aes(x = day, y = value)) + 
  geom_line(size = 1.5) + 
  scale_y_continuous(breaks = c(0, 0.25, 0.50, 0.80, 1.00)) +
  theme_classic(base_family = 'Helvetica') + 
  
  geom_area(data = pdf_shading_data, mapping = aes(x = day, y = value), 
                show.legend = 'bin',  fill="grey", alpha = 1, color = 'black', size = 1.5) +
  geom_text(data = equation_data, inherit.aes = F, mapping = aes(x = x, y = y, label = label), parse = T, size = 9) + 
  geom_point(data = point_data, inherit.aes = F, mapping = aes(x = x , y = y), size = 4) + 
  
  #arrows
  geom_segment(data = arrow_data, inherit.aes = F, mapping = aes(x = xmin, xend = xmax, y = ymin, yend = ymax), 
               arrow = arrow(length = unit(0.3, 'cm')), size = 1)  + 
  
    #vertical lines 
  geom_segment(data = v_line_data, mapping = aes(x = x, y = y, xend = x_end, yend = y_end), linetype = 2, size = 1) + 
  geom_segment(data = h_line_data, mapping = aes(x = x, y = y, xend = x_end, yend = y_end), linetype = 2, size = 1)  + 



  facet_wrap_custom( ~ dist_type, scales = "free", ncol = 2, nrow = 2 , dir = 'v',
                     labeller = label_parsed,  

                            scale_overrides = list(
                            scale_override(1,
                              scale_x_continuous(
                                breaks = c(0, 4.32, 18, 24, 36),
                                limits = c(0, 36))), 
                          
                            scale_override(which = 2,
                              scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4),
                                limits = c(0, 0.4))),
                             scale_override(which = 2,
                              scale_x_continuous(breaks = c(0, 4.32, 18, 24, 36),
                                limits = c(0, 36))),
                            
                            scale_override(which = 3,
                               scale_x_continuous(
                                breaks = c(0, 10.80, 18, 24, 36),
                                limits = c(0, 36))), 
                         
                            scale_override(4,
                              scale_x_continuous(
                                breaks = c(0, 10.80, 18, 24, 36),
                                limits = c(0, 36))), 
                            scale_override(which = 4,
                                scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4),
                                limits = c(0, 0.4))))) +  


  labs( x = 'Response window day') + 
  
  theme(strip.text.x = element_text(face = 'bold', hjust = 0, size = 30, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),
        strip.background = element_rect(fill = "white", color = "white"), 

        #axis details
        axis.text = element_text(size = 22, color = 'black'),
        axis.title = element_text(size = 28),
        axis.line = element_line(size = 1),
        axis.ticks.length.x = unit(x = 0.5, units = 'cm'), 
        axis.title.x = element_text(margin = unit(c(1, 0, 0, 0), "cm")),
        axis.ticks = element_line(size = 1, colour = 'black'),

      panel.spacing.y = unit(x = 2, units = 'cm'),
      panel.spacing.x = unit(x = 2, units = 'cm'))
       

g <- ggplotGrob(cdf_pdf_plot)

#customize y-axis label 
g$grobs[[28]]$children$GRID.text.24299$label <- paste("Density (probability, f'(x))", str_pad('', width = 13), "Response percentage (f(x))")
g$grobs[[28]]$children$GRID.text.24299$y <- grid::unit(0.52,"npc")
g$grobs[[28]]$children$GRID.text.24299$x <- grid::unit(-0.2,"npc")

plot_converted <- as_ggplot(g)

#create PDF of faceted plot
set_panel_size(p = plot_converted, height = unit(x = 32, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/cdf_pdf_plots.pdf')
```

```{=tex}
\begin{figure}[H]
  \caption{Cumulative Distribution Functions (CDF) and Probability Density Functions (PDF) for Fast and Slow Response Rates}
  \label{fig:cdf-pdf-plots}
  \includegraphics{Figures/cdf_pdf_plots} \hfill{}
  \caption*{Note. \textup{Panel A shows the cumulative distribution function for the fast response rate (with a growth parameter value $a$ set to 0.37; see Equation \ref{eq:fast-cdf}) and Panel B shows the probability density function that results from computing the derivative of the fast response rate cumulative distribution function with respect to $x$ (see Equation \ref{eq:fast-pdf-function}). Panel C shows the cumulative distribution function for the slow response rate (with a growth parameter value $a$ set to 0.15; see Equation \ref{eq:slow-cdf}) and Panel D shows the probability density function that results from computing the derivative of the slow response rate cumulative distribution function with respect to $x$ (see Equation \ref{eq:slow-pdf-function} and section \ref{sec:time-structuredness} for more discussion on time structuredness). For the fast response rate functions, an 80\% response rate is obtained after 4.32 days or, equivalently, 80\% of the area underneath the probability density function is obtained at 4.32 days ($\int^{4.32}_{0} f_{fast}^\prime (x) = 0.80$). For the slow response rate functions, an 80\% response rate is obtained after 10.80 days or, equivalently, 80\% of the area underneath the probability density function is obtained at 10.80 days ($\int^{10.80}_{0} f_{slow}^\prime (x) = 0.80$).}}
\end{figure}
```
Having computed probability density functions for fast and slow response
rates, time delays could be generated to create time-unstructured data. To generate time-unstructured data for a
person at a given time point, a time delay wasfirst
generated by sampling values according to the probability density function defined by either a fast or slow response rate (Equations \ref{eq:fast-pdf-function}--\ref{eq:slow-pdf-function}). The sampled time delay was then added to the value of the current measurement day, with the combined measurement day then being plugged into the logistic function (Equation \ref{eq:logFunction-generation}) along with a set of person-specific parameter values to generate an observed score at a given time point for a given person. 

#### Population Values Used for Logistic Function Parameters

### Nonlinear Latent Growth Curve Model Used to Analyze Each Generated Data Set

## Analysis of Dependent Variables

### Analysis of Convergence Success Rate

### Analysis of Bias

### Analysis of Variability in Parameter Estimation


## Results of Experiment 3

### Time-Structured Data

```{r plots-time-structured-exp3, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_3_analytical, 
                               target_col = 'time_structuredness', target_value = 'Time structured',
                                                              x_axis_name = expression("Sample size ("*italic(N)*")"), 

                               x_axis_var = 'sample_size', exp_num = 'exp3_', beta_lower = 160, beta_upper = 210,
                               ticks = 5)
```

```{r text-values-time-structured-exp3, echo=F}
#fixed-effect halfway-triquarter delta (5, 100)
gamma_fixed_equal_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 5, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_equal_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 5, sample_size == 200, grepl('beta\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter(7, 100)
gamma_fixed_equal_7_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 7, sample_size == 200, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))  

#fixed-effect halfway-triquarter delta (5, 200)
gamma_rand_equal_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time structured', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci),
         estimate = as.integer(estimate))

```

Figure \ref{fig:exp3_plot_days_time_structured} shows the parameter
estimation plots with time-structured data (error bars represent the
middle 95% of estimated values and shaded horizontal lines indicate the
population values). Panels A--B show the parameter estimation plots for
the fixed-effect days-to-halfway elevation and triquarter-halfway delta
parameters ($\upbeta_{fixed}$ and $\upgamma_{fixed}$),
respectively.Panels C--D show the parameter estimation plots for the
random-effect days-to-halfway elevation and triquarter-halfway delta
parameters ($\upbeta_{random}$ and $\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp3-time-structured} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters with time-structured data.

For all simulations presented in Figure
\ref{fig:exp3_plot_days_time_structured}, one instance of bias occurred.
With five measurements and a sample size no larger than 200, estimation
of the random-effect halfway-triquarter delta parameter
($\upgamma_{random}$) was considerably overestimated. For example, the
average population value estimated for the random-effect
halfway-triquarter delta parameter ($\upgamma_{random}$; Figure
\ref{fig:exp3_plot_days_time_structured}D) parameter was
`r gamma_rand_equal_5_200$estimate` days with five measurements and a
sample size of 200 (relative to a population value of 4.00 days). Across
all other conditions in Figure \ref{fig:exp3_plot_days_time_structured},
no instance of bias occurred, as the average estimated values for all
day-unit parameters (as indicated by the black dots) were close to their
respective population values (indicated by the gray lines).

With respect to variability, two general patterns of results emerged in
the estimation of day-unit parameters with equal spacing. First, for the
halfway-triquarter delta parameters ($\upgamma_{fixed}$ and
$\upgamma_{random}$), variability was considerably high with five
measurements and a sample size no larger than 200. For example, the
error bar length for the fixed-effect halfway-triquarter delta parameter
($\upgamma_{fixed}$; Figure \ref{fig:exp3_plot_days_time_structured}B)
spanned approximately
`r gamma_fixed_equal_5_200$upper_ci -gamma_fixed_equal_5_200$lower_ci`
days (upper bound of `r gamma_fixed_equal_5_200$upper_ci` days to a
lower bound of `r gamma_fixed_equal_5_200$lower_ci` days) with five
measurements and a sample size of 200. Note that, in contrast,
variability for the fixed- and random-effects of the days-to-halfway
elevation parameters ($\beta_{fixed}$ and $\beta_{random}$) was
considerably lower than the variability for the halfway-triquarter delta
parameters. As an example, the error bar length for the fixed-effect
days-to-halfway elevation parameter($\beta_{fixed}$; Figure
\ref{fig:exp3_plot_days_time_structured}A) spanned approximately
`r beta_fixed_equal_5_200$upper_ci - beta_fixed_equal_5_200$lower_ci`
days (upper bound of `r beta_fixed_equal_5_200$upper_ci` days to a lower
bound of `r beta_fixed_equal_5_200$lower_ci` days) with five
measurements and a sample size of 200. Second, variability across all
day-unit parameters was nearly trivial across all sample size levels
with seven or more measurements. For example, the error bar length for
the the fixed-effect halfway-triquarter delta parameter
($\upgamma_{fixed}$; Figure \ref{fig:exp3_plot_days_time_structured}B)
spanned approximately
`r gamma_fixed_equal_7_200$upper_ci - gamma_fixed_equal_7_200$lower_ci`
(upper bound of `r gamma_fixed_equal_7_200$upper_ci` days to a lower
bound of `r gamma_fixed_equal_7_200$lower_ci` days) days with seven
measurements and a sample size of 200.

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Structured Data in Experiment 3}
  \label{fig:exp3_plot_days_time_structured}
  \includegraphics{Figures/exp3_plot_days_time structured} \hfill{}
    \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-time-structured} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp3-time-structured, echo=F}
print_bias_var_omega_table(exp_data = exp_3_raw, target_col = 'time_structuredness', target_value = 'time_structured', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Structured Data in Experiment 3',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 100), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp3_plot_days_time_structured}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp3_plot_days_time_structured}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp3_plot_days_time_structured}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp3_plot_days_time_structured}D)'))

```

One additional point to mention concerns how the pattern of results
observed here mirror the results observed in Experiment 2 when equal
spacing was used. In both Experiment 2 (equal spacing) and Experiment 3
(time-structured data), the same conditions were simulated: Measurement
spacing was equal, data were time structured, sample size levels were
30, 50, 100, 200, 500, and 1000, and levels for the number of
measurements were 5, 7, 9, and 11. Thus, it is unsurprising that the
results across Experiment 2 (equal spacing) and Experiment 3
(time-structured data) are identical.

### Time-Unstructured Data (Fast Response)

```{r plots-fast-response-exp3, include=F, eval=F}
generate_day_likert_facet_plot(analytical_data = exp_3_analytical, 
                               target_col = 'time_structuredness', target_value = 'Time unstructured (fast response)',
                              x_axis_name = expression("Sample size ("*italic(N)*")"), 
                               x_axis_var = 'sample_size', exp_num = 'exp3_', beta_lower = 160, beta_upper = 210,
                               ticks = 5)
```

```{r text-values-fast-response-exp3, echo=F}
#fixed-effect triquarter-halfway delta parameter (5, 1000)
gamma_fixed_fast_5_1000 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 5, sample_size == 1000, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect triquarter-halfway delta parameter (5, 200)
gamma_random_fast_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci), 
         estimate = as.integer(estimate))

#random-effect triquarter-halfway delta parameter (5, 200)
gamma_random_fast_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect triquarter-halfway delta parameter (7, 200)
gamma_random_fast_7_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (fast response)', number_measurements == 7, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp3_plot_days_fast_response} shows the parameter
estimation plots with time-structured data (error bars represent the
middle 95% of estimated values and shaded horizontal lines indicate the
population values). Panels A--B show the parameter estimation plots for
the fixed-effect days-to-halfway elevation and triquarter-halfway delta
parameters ($\upbeta_{fixed}$ and $\upgamma_{fixed}$),
respectively.Panels C--D show the parameter estimation plots for the
random-effect days-to-halfway elevation and triquarter-halfway delta
parameters ($\upbeta_{random}$ and $\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp3-fast-response} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters with time-unstructured data defined by fast response rates.  

For all simulations presented in Figure
\ref{fig:exp3_plot_days_time_structured}, two instances of bias
occurred. First, estimation of the fixed-effect days-to-halfway
elevation parameter ($\upbeta_{fixed}$; Figure
\ref{fig:exp3_plot_days_time_structured}A) was slightly underestimated
across all conditions (all black dots lied slightly below the gray
line). Second, the random-effect halfway-triquarter delta parameter
($\upgamma_{random}$) was considerably overestimated with five
measurements and a sample size no larger than 200. For example, the
average population value estimated for the random-effect
halfway-triquarter delta parameter ($\upgamma_{random}$; Figure
\ref{fig:exp3_plot_days_fast_response}D) parameter was approximately
`r gamma_random_fast_5_200$estimate` days with five measurements and a
sample size of 200 (relative to a population value of 4.00 days). Across
all other conditions in Figure \ref{fig:exp3_plot_days_fast_response},
no instance of bias occurred, as the average estimated values for all
day-unit parameters (as indicated by the black dots) were close to their
respective population values (indicated by the gray lines).

With respect to variability, three general patterns of results emerged
in the estimation of day-unit parameters with time-unstructured data
defined by fast response rates. First, variability across all sample
sizes for the fixed-effect triquarter-halfway delta parameter
($\upgamma_{fixed}$) remained high with five measurements. For example,
the error bar length for the fixed-effect triquarter-halfway delta
parameter ($\upgamma_{fixed}$; Figure
\ref{fig:exp3_plot_days_time_structured}B) had a length of
`r gamma_fixed_fast_5_1000$upper_ci - gamma_fixed_fast_5_1000$lower_ci`
days (upper bound of `r gamma_fixed_fast_5_1000$upper_ci` days to lower
bound of `r gamma_fixed_fast_5_1000$upper_ci` days) with a sample size
of 1000 with 5 measurements. Note that the same pattern of variability
existed to a lesser extent for the random-effect days-to-halfway
elevation parameter ($\upbeta_{random}$; Figure
\ref{fig:exp3_plot_days_time_structured}C). Second, variability for the
random-effect triquarter-halfway delta parameter ($\upgamma_{random}$)
remained high with five measurements and a sample size no larger than
200. For example, the error bar length for the random-effect
triquarter-halfway delta parameter ($\upgamma_{random}$; Figure
\ref{fig:exp3_plot_days_time_structured}D) had a length of
`r gamma_random_fast_5_200$upper_ci - gamma_random_fast_5_200$lower_ci`
days (upper bound of `r gamma_random_fast_5_200$upper_ci` days to lower
bound of `r gamma_random_fast_5_200$lower_ci` days) with a sample size
of 200 and five measurements. Third, variability across all day-unit
parameters was nearly trivial across all sample size levels with seven
measurements. For example, the error bar length for the the
random-effect halfway-triquarter delta parameter ($\upgamma_{random}$;
Figure \ref{fig:exp3_plot_days_time_structured}D) spanned approximately
`r gamma_random_fast_7_200$upper_ci - gamma_random_fast_7_200$lower_ci`
(upper bound of `r gamma_random_fast_7_200$upper_ci` days to a lower
bound of `r gamma_random_fast_7_200$lower_ci` days) days with seven
measurements and a sample size of 200 and had an average estimated value
of `r gamma_random_fast_7_200$estimate` days (relative to a population
value of 4.00).

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Unstructured Data (Fast Response Rate) in Experiment 3}
  \label{fig:exp3_plot_days_fast_response}
  \includegraphics{Figures/exp3_plot_days_time unstructured (fast response)} \hfill{}
    \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-fast-response} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp3-fast-response, echo=F}
print_bias_var_omega_table(exp_data = exp_3_raw, target_col = 'time_structuredness', target_value = 'fast_response', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Unstructured Data (Fast Response Rate) in Experiment 3',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp3_plot_days_fast_response}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp3_plot_days_fast_response}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp3_plot_days_fast_response}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp3_plot_days_fast_response}D)'))

```

### Time-Unstructured Data (Slow Response)

```{r plots-slow-response-exp3, include=F, echo=F}
generate_day_likert_facet_plot(analytical_data = exp_3_analytical, 
                               target_col = 'time_structuredness', target_value = 'Time unstructured (slow response)',
                               x_axis_name = expression("Sample size ("*italic(N)*")"), 
                               x_axis_var = 'sample_size', exp_num = 'exp3_', beta_lower = 160, beta_upper = 210,
                               ticks = 5)
```

```{r text-values-slow-response-exp3, echoF}
#fixed-effect triquarter-halfway delta parameter (5, 1000)
gamma_fixed_slow_5_1000 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 1000, grepl('gamma\\[fixed\\]', parameter)) %>%
  select(lower_ci, upper_ci) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect triquarter-halfway delta parameter (5, 200)
gamma_random_slow_5_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

#fixed-effect days-to-halfway elevation parameter (5, 100)
beta_fixed_slow_5_100 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci))

beta_fixed_slow_5_30 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 30, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

beta_fixed_slow_11_1000 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 11, sample_size == 1000, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect triquarter-halfway delta parameter (5, 100)
gamma_random_slow_5_100 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 5, sample_size == 100, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 

#random-effect triquarter-halfway delta parameter (7, 100)
gamma_random_slow_7_200 <- exp_3_analytical$days %>% 
  filter(time_structuredness == 'Time unstructured (slow response)', number_measurements == 7, sample_size == 200, grepl('gamma\\[random\\]', parameter)) %>%
  select(lower_ci, upper_ci, estimate) %>%
  mutate(lower_ci = as.integer(lower_ci), 
         upper_ci = as.integer(upper_ci)) 
```

Figure \ref{fig:exp3_plot_days_slow_response} shows the parameter
estimation plots with time-structured data (error bars represent the
middle 95% of estimated values and shaded horizontal lines indicate the
population values). Panels A--B show the parameter estimation plots for
the fixed-effect days-to-halfway elevation and triquarter-halfway delta
parameters ($\upbeta_{fixed}$ and $\upgamma_{fixed}$),
respectively.Panels C--D show the parameter estimation plots for the
random-effect days-to-halfway elevation and triquarter-halfway delta
parameters ($\upbeta_{random}$ and $\upgamma_{random}$), respectively. Note that Table \ref{tab:omega-exp3-slow-response} provides the partial $\upomega^2$ values for the experimental variables for the day-unit parameters with time-unstructured data defined by fast response rates. 

For all simulations presented in Figure
\ref{fig:exp3_plot_days_slow_response}, two instances of bias occurred
in the estimation of day-unit parameters. First, estimation of the
random-effect halfway-triquarter delta parameter ($\upgamma_{random}$)
incurred bias with five measurements and a sample size no larger than
100. For example, the random-effect halfway-triquarter delta parameter
($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_slow_response}D)
had an average estimated value of `r gamma_random_slow_5_100$estimate`
days with a sample size of 100 with five measurements. Second,
estimation of the fixed-effect days-to halfway elevation parameter
($\upbeta_{fixed}$) incurred bias under all sample size-measurement
number conditions. For example, the fixed-effect days-to halfway
elevation parameter ($\upbeta_{fixed}$; Figure
\ref{fig:exp3_plot_days_slow_response}A) had an average estimated value
of `r beta_fixed_slow_5_30$estimate` days with a sample size of 30 and
five measurements and an average estimated value of
`r beta_fixed_slow_11_1000$estimate` days with a sample size of 1000 and
11 measurements (relative to a population value of 180.00). Across all
other conditions in Figure \ref{fig:exp3_plot_days_slow_response}, no
instance of bias occurred, as the average estimated values for all
day-unit parameters (as indicated by the black dots) were close to their
respective population values (indicated by the gray lines).

First, variability across all sample sizes for the fixed-effect
triquarter-halfway delta parameter ($\upgamma_{fixed}$) remained high
with five measurements. For example, the error bar length for the
fixed-effect triquarter-halfway delta parameter ($\upgamma_{fixed}$;
Figure \ref{fig:exp3_plot_days_slow_response}B) had a length of
`r gamma_fixed_fast_5_1000$upper_ci - gamma_fixed_fast_5_1000$lower_ci`
days (upper bound of `r gamma_fixed_fast_5_1000$upper_ci` days to lower
bound of `r gamma_fixed_fast_5_1000$upper_ci` days) with a sample size
of 1000 with 5 measurements. Note that the same pattern of variability
existed to a lesser extent for the random-effect days-to-halfway
elevation parameter ($\upbeta_{random}$). Second, variability for the
random-effect triquarter-halfway delta parameter ($\upgamma_{random}$)
remained high with five measurements and a sample size no larger than
200. For example, the error bar length for the random-effect
triquarter-halfway delta parameter ($\upgamma_{random}$; Figure
\ref{fig:exp3_plot_days_slow_response}D) had a length of
`r gamma_random_fast_5_200$upper_ci - gamma_random_fast_5_200$lower_ci`
days (upper bound of `r gamma_random_fast_5_200$upper_ci` days to lower
bound of `r gamma_random_fast_5_200$lower_ci` days) with a sample size
of 200 and five measurements. Third, variability across all day-unit
parameters was nearly trivial across all sample size levels with seven
measurements. For example, the error bar length for the the
random-effect halfway-triquarter delta parameter ($\upgamma_{random}$;
Figure \ref{fig:exp3_plot_days_slow_response}D) spanned approximately
`r gamma_random_fast_7_200$upper_ci - gamma_random_fast_7_200$lower_ci`
(upper bound of `r gamma_random_fast_7_200$upper_ci` days to a lower
bound of `r gamma_random_fast_7_200$lower_ci` days) days with seven
measurements and a sample size of 200 and had an average estimated value
of `r gamma_random_fast_7_200$estimate` days (relative to a population
value of 4.00).

With respect to variability, three general patterns of results emerged
in the estimation of day-unit parameters with time-unstructured data
defined by slow response rates. First, variability across all sample
sizes for the fixed-effect triquarter-halfway delta parameter
($\upgamma_{fixed}$) remained high with five measurements. For example,
the error bar length for the the fixed-effect triquarter-halfway delta
parameter ($\upgamma_{fixed}$; Figure
\ref{fig:exp3_plot_days_slow_response}B) spanned approximately
`r gamma_fixed_slow_5_1000$upper_ci - gamma_fixed_slow_5_1000$lower_ci`
days (upper bound of `r gamma_fixed_slow_5_1000$upper_ci` days to lower
bound of `r gamma_fixed_slow_5_1000$upper_ci` days) with a sample size
of 1000 and five measurements. Note that the same pattern of variability
existed to a lesser extent for the random-effect days-to-halfway
elevation parameter ($\upbeta_{random}$). Second, variability for the
random-effect triquarter-halfway delta parameter ($\upgamma_{random}$)
remained high with five measurements and a sample size no larger than
200. For example, the error bar length for the random-effect
triquarter-halfway delta parameter ($\upgamma_{random}$; Figure
\ref{fig:exp3_plot_days_slow_response}D) had a length of
`r gamma_random_slow_5_200$upper_ci - gamma_random_slow_5_200$lower_ci`
days (upper bound of `r gamma_random_slow_5_200$upper_ci` days to lower
bound of `r gamma_random_slow_5_200$lower_ci` days) with a sample size
of 200 and five measurements. Third, variability and bias across all
day-unit parameters (except the fixed-effect days-to-halfway elevation
parameter [$\upbeta_{fixed}$]) was nearly trivial across all sample
size levels with seven measurements. For example, the error bar length
for the the random-effect halfway-triquarter delta parameter
($\upgamma_{random}$; Figure \ref{fig:exp3_plot_days_slow_response}D)
spanned approximately
`r gamma_random_slow_7_200$upper_ci - gamma_random_slow_7_200$lower_ci`
(upper bound of `r gamma_random_slow_7_200$upper_ci` days to a lower
bound of `r gamma_random_slow_7_200$lower_ci` days) days with seven
measurements and a sample size of 200 and had an average estimated value
of `r gamma_random_slow_7_200$estimate` days (relative to a population
value of 4 days).

```{=tex}
\begin{figure}[H]
  \caption{Summary of Day-Unit Parameter Estimates for Time-Unstructured Data (Slow Response Rate) in Experiment 3}
  \label{fig:exp3_plot_days_slow_response}
  \includegraphics{Figures/exp3_plot_days_time unstructured (slow response)} \hfill{}
    \caption*{Note. \textup{Errors bars represent the middle 95\% of estimated values. Gray horizontal lines in each panel represent the population value for each parameter. Population values for each parameter are as follows: $\upbeta_{fixed}$ = 180.00, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00, $\upepsilon$ = 0.03. Panels A--B show the parameter estimation plots for
the fixed- and random-effect days-to-halfway elevation  parameters ($\upbeta_{fixed}$ and $\upbeta_{random}$, respectively. Panels C--D show the parameter estimation plots for the fixed- and random-effect triquarter-halfway elevation parameters ($\upgamma_{fixed}$ and $\upgamma_{random}$, respectively. Note that random-effect units are in standard deviation units. See Table \ref{tab:exp3-theta-param-est} for specific values estimated for each parameter and Table \ref{tab:omega-exp3-slow-response} for $\upomega^2$ effect size values.}}
\end{figure}
```

```{r omega-exp3-slow-response, echo=F}
print_bias_var_omega_table(exp_data = exp_3_raw, target_col = 'time_structuredness', target_value = 'slow_response', 
ind_vars = c('number_measurements', 'sample_size'), 
ind_var_acronyms = c('NM', 'S', 'NM x S'), 
caption = 'Partial $\\upomega^2$ Values for Manipulated Variables With Time-Untructured Data (Slow Response Rate) in Experiment 3',
footnote = 'NM = number of measurements (5, 7, 9, 11), S = sample size (30, 50, 100, 200, 500, 1000), NM x S = interaction between number of measurements and sample size.', 
parameter_labels = c('$\\upbeta_{fixed}$ (Figure \\ref{fig:exp3_plot_days_slow_response}A)',
                     '$\\upbeta_{random}$ (Figure \\ref{fig:exp3_plot_days_slow_response}B)',
                     '$\\upgamma_{fixed}$ (Figure \\ref{fig:exp3_plot_days_slow_response}C)',
                     '$\\upgamma_{random}$ (Figure \\ref{fig:exp3_plot_days_slow_response}D)'))

```



## Discussion of Experiment 3 

```{=tex}
\newpage
\vspace*{-\topskip}
\vspace*{\fill}
\nointerlineskip
```

# General Discussion

```{=tex}
\nointerlineskip
\vfill
\newpage
```




# Supplementary Appendix 
# Technical Appendix
## Technical Appendix A: Ergodicity and the Need to Conduct Longitudinal Research

(ref:petersen1983) [for an introduction, see @petersen1983]
(ref:birkhoff1931) @birkhoff1931 [for a review, see @choe2005, Chapter 3]

To understand why cross-sectional results are unlikely to agree with longitudinal results for any given analysis, a discussion of data structure is apropos. Consider an example where a researcher obtains data from 50 people measured over 100 time points such that each row contains a $p$ person's data over the 100 time points and each column contains data from 50 people at a $t$ time point. For didactic purposes, all data are assumed to be sampled from a normal distribution. To understand whether findings in any given cross-sectional data set yield the same findings in any given longitudinal data set, the researcher randomly samples one cross-sectional and one longitudinal data set and computes the mean and variance in each set. To conduct a cross-sectional analysis, the researcher randomly samples the data across the 50 people at a given time point and computes a mean of the scores at the sampled time point ($\bar{X}_t$) using Equation \ref{eq:cross-mean} shown below:

\begin{align}
\bar{X}_t = \frac{1}{P}\sum^P_{p = 1} x_p,
(\#eq:cross-mean)
\end{align}

\noindent where the scores of all $P$ people are summed ($x_p$) and then divided by the number of people ($P$). To compute the variance of the scores at the sampled time point ($S^2_t$), the researcher uses Equation \ref{eq:cross-variance} shown below:

\begin{align}
\S^2_t = \frac{1}{P}\sum^P_{p = 1} (x_p - \bar{X}_t)^2,
(\#eq:cross-variance)
\end{align}

\noindent where the sum of squared differences between each person's score ($x_p$) and the average value at the given $t$ time point ($\bar{X}_t$) is computed and then divided by the number of people ($P$).  To conduct a longitudinal analysis, the researcher randomly samples the data across the 100 time points for a given person and also computes a mean and variance of the scores. To compute the mean across  the $t$ time points of the longitudinal data set ($\bar{X}_p$), the researcher uses Equation \ref{eq:long-mean} shown below:

\begin{align}
\bar{X}_p = \frac{1}{T}\sum^T_{t = 1} x_t,
(\#eq:long-mean)
\end{align}

\noindent where the scores at each $t$ time point are summed ($x_t$) and then divided by the number of time points ($T$). The researcher also computes a variance of the sampled person's scores across all time points ($S^2_p$) using Equation \ref{eq:long-variance} shown below:

\begin{align}
\S^2_p = \frac{1}{T}\sum^T_{t = 1} (x_t - \bar{X}_p)^2,
(\#eq:long-variance)
\end{align}

\noindent where the sum of squared differences between the score at each time point ($x_t$) and the average value of the $p$ person's scores ($\bar{X}_p$) is computed and then divided by the number of time points ($T$). 

If the researcher wants treat the mean and variance values computed from the cross-sectional and longitudinal data sets as interchangeable, then two conditions outlined by ergodic theory must be satisfied [@molenaar2004; @molenaar2009].\footnote{Note that ergodic theory is an entire mathematical discipline (ref:petersen1983). In the current context, the most important ergodic theorems are those proven by (ref:birkhoff1931).} First, a given cross-sectional mean and variance can only closely estimate the mean and variance of any given person's data (i.e., a longitudinal data set) to the extent that each person's data are generated from a normal distribution with the same mean and variance. If each person's data were generated from a different normal distribution, the computing the mean and variance at a given time point would, at best, describe the values of one person. When each person's data are generated from the same normal distribution, the condition of **homogeneity** is met. Importantly, satisfying the condition of homogeneity does not guarantee that the mean and variance obtained from another cross-sectional data set will closely estimate the mean and variance of any given person (i.e., any given longitudinal data set). The mean and variance values computed from any given cross-sectional data set can only closely estimate the values of any given person to the extent that the cross-sectional mean and variance remain constant over time. If the mean and variance of observations remain constant over time, then the the second condition of **stationarity** is satisfied. Therefore, the researcher can only treat means and variances from cross-sectional and longitudinal data sets as interchangeable if each person's data is generated from the same normal distribution (homogeneity) and if the mean and variance remain constant over time (stationarity). When the conditions of homogeneity and stationarity are satisfied, a process is said to be **ergodic**: Analyses of cross-sectional data sets return the same values as analyses on longitudinal data sets.

Given that psychological studies almost never collect data from only one person, one potential reservation may be that the conditions required for ergodicity only hold when a longitudinal data set contains the data of one person. That is, if the researcher used the full data set containing the data of 100 people sampled over 100 time points and computed 100 cross-sectional means and variances (Equation \ref{eq:cross-mean} and Equation \ref{eq:cross-variance}, respectively) and 100 longitudinal means and variances (Equation \ref{eq:long-mean} and Equation \ref{eq:long-variance}, respectively), wouldn't the average of the cross-sectional means and variances be the same as the average of the longitudinal means and variances? Although the averaging the cross-sectional mean returns the same value as averaging the longitudinal means, the average longitudinal variance remains different from the average cross-sectional variance [for several empirical examples, see @fisher2018]. Therefore, the conditions of ergodicity apply even with larger longitudinal and cross-sectional sample sizes. 

(ref:voelkle2014spector2019) [@voelkle2014; for similar discussion, see @spector2019]
(ref:adolf2019medaglia2019) [@adolf2019; @medaglia2019]

The guaranteed differences in cross-sectional and longitudinal variance values that result from non-ergodic processes have far-reaching implications. Almost every analysis employed in organizational research---whether it be correlation, regression, factor analysis, mediation analysis, etc.---analyzes variability, and so, when a process is non-ergodic, cross-sectional variability will differ from longitudinal variability, and the results obtained from applying any given analysis on each of the variabilities will differ as a consequence. Because variability is central to so many analyses, the non-equivalence of longitudinal and cross-sectional variances that results from a non-ergodic process explains why discussions of ergodicity often comment that "for non-ergodic processes, an analysis of the structure of IEV [interindividual variability] will yield results that differ from results obtained in an analogous analysis of IAV [intraindividual variability]"[@molenaar2004, p. 202].\footnote{It is important to note that a violation of one or both ergodic conditions (homogeneity and stationarity) does not mean that an analysis of cross-sectional variability yields results that have no relation to the results gained from applying the analysis on longitudinal variability (i.e., the causes of cross-sectional variability are independent from the causes of longitudinal variability). An analysis of cross-sectional variability can still give insight into temporal dynamics if the causes of non-ergodicity can be identified (ref:voelkle2014spector2019). Thus, conceptualizing ergodicity on a continuum with non-erdogicity and ergodicity on opposite ends provides a more balanced perspective for understanding ergodicity (ref:adolf2019medaglia2019).}

With an understanding of the conditions required for ergodicity, a brief consideration of organizational phenomena finds that these conditions are regularly violated. Focusing only on homogeneity (each person's data are generated from the same distribution), several instances in organizational research violate this condition. As examples of homogeneity violations, employees show different patterns of absenteeism over five years [@magee2016], leadership development over the course of a seminar [@day2011], career stress over the course of 10 years [@igic2017], and job performance in response to organizational restructuring [@miraglia2015]. With respect to stationarity (constant values for statistical parameters across people over time), several examples can be generated by realizing how calendar events affect psychological processes and behaviours throughout the year. As examples of stationarity violations, consider how salespeople, on average, undoubtedly sell more products during holidays, how employees, on average, take more sick days during the winter months, and how accountants, on average, experience more stress during tax season. With ergodic condition violations commonly occurring in organizational psychology, it becomes fitting to echo the commonly held sentiment that few, if any, psychological processes are ergodic [@molenaar2004; @molenaar2008; @molenaar2009; @fisher2018; @curran2011; @wang2015; @hamaker2012]. 


## Technical Appendix B: Using Nonlinear Function in the Structural Equation Modelling Framework 
### Nonlinear Latent Growth Curve Model Used to Analyze Each Generated Data Set{#structured-latent}

The sections that follow will first review the framework used to build
latent growth curve models and then explain how nonlinear functions can
be modified to fit into this framework.

#### Brief Review of the Latent Growth Curve Model Framework

(ref:meredith1990browne1993) [@meredith1990; @browne1993]

(ref:blozis2004) [@blozis2004]

The latent growth curve model proposed by @meredith1990
is briefly reviewed here [for a review, see @preacher2008]. Consider an example where data are collected at five time
points ($T = 5$) to yield five observations for each $p$ person
($\mathbf{y_p} = [y_1, y_2, y_3, y_4, y_5$). A simple model to fit is
one where change over is defined by a straight line and each person's
pattern of change is some variation of this straight line. In modelling
parlance, an intercept-slope model is fit where both the intercept and
slope are random effects whose values are allowed to vary for each
person. Intercept and slope parameters can be algebraically represented
by a two-column matrix that represents the effect of each parameter on
the outcome variable $y$ at each $i$ time point. Because the effect of
the intercept parameter is constant over time, a column of 1s is used to
represent its effect. For the slope parameter, a pattern of linear
growth can be specified filling the second column with a series of
monotonically increasing numbers such as
0--4.\footnote{The set of numbers specified for the slope starts at zero because there is presumably no effect of any variable at the first time point.}The matrix $\mathbf{\Uplambda}$ below shows a two-column matrix that
specifies the effects for an intercept and slope parameter:

$$ 
\mathbf{\Uplambda} = 
\begin{bmatrix}
1 & 0 \\ 
1 & 1 \\ 
1 & 2 \\ 
1 & 3 \\
1 & 4 \\
\end{bmatrix}
$$

\noindent To create a model that allows different linear patterns to be
fit to each person's data, a weight can be applied to each column of
$\mathbf{\Uplambda}$ and each weight can vary across individuals.\footnote{The columns of $\mathbf{\Uplambda}$ are called basis curves (ref:blozis2004) or basis functions (ref:meredith1990browne1993) because each column specifies a particular component of change.}That is, each $p$ person's pattern of change is predicted with a unique set of weights in $\mathbf{\upiota_p}$ that determines the extent to
which each basis column of $\mathbf{\Uplambda}$ contributes to that
person's change over time. Discrepancies between the values predicted by
$\mathbf{\Uplambda\upiota_p}$ and a person's observed scores across all
five time points are stored in an error vector
$\mathbf{\mathcal{E}_p}$. Thus, a person's observed data ($\mathbf{y_p}$) is
constructed using the expression shown below in Equation \ref{eq:sem-framework}:

```{=tex}
\begin{align}
 y_p = \mathbf{\Uplambda\upiota_p} + \mathbf{\mathcal{E}_p}.
 (\#eq:sem-framework)
\end{align}
```

\noindent Note that Equation \ref{eq:sem-framework} defines the general structural equation modelling framework. 

#### Fitting a Nonlinear Function in the Structural Equation Modelling Framework

Unfortunately, the logistic function of Equation
\ref{eq:logFunction-generation}---where each parameter was estimated as a
fixed- and random-effect---could not be directly used in a latent growth curve model because it would have violated the linear nature of the structural equation modelling framework (Equation \ref{eq:sem-framework}). Structural equation models only permit linear combinations---specifically, the
products of matrix-vector and/or matrix-matrix multiplication---and so
directly fitting a nonlinear function such as the logistic function in
Equation \ref{eq:logFunction-generation} would not have been possible.

One solution to fitting the logistic function within the structural equation modelling framework was to implement the structured latent curve modelling approach
[@browne1991; @browne1993; for an excellent review, see @preacher2015]. Briefly, the structured latent curve modelling approach constructs a Taylor series approximation of a nonlinear function so that the nonlinear function can be fit into the structural equation modelling framework (Equation \ref{eq:sem-framework}). The sections that follow will present the structured latent curve modelling approach in four parts such that 1) Taylor series approximations will first be reviewed, 2) a Taylor series approximation will then be constructed for the logistic function, 3) the logistic Taylor series approximation will be modified and fit into the structural equation modelling framework, and 4) the process of parameter estimation will be reviewed. 

##### Taylor Series Approximations

A Taylor series uses derivative information of a nonlinear function to
construct a linear approximation.\footnote{Linear functions are
defined as functions where no parameter exists within its own partial
derivative. For example, none of the parameters in the polynomial
equation of $y = a + bt + ct^2 + dt^3$ exist within their own partial
derivative: $\frac{\partial y}{\partial a} = 1$,
$\frac{\partial y}{\partial b} = t$,
$\frac{\partial y}{\partial c} = t^2$, and
$\frac{\partial y}{\partial d} = t^3$. Conversely, the logistic function
is nonlinear because $\upbeta$ and $\upgamma$ exist in their own
partial derivatives. For example, the derivative of the logistic function  $y = \uptheta + \frac{\upalpha - \uptheta}{1 + e^{\frac{\upbeta - t}{\upgamma}}} $with respect to $\upbeta$ is $\frac{(\uptheta - \upalpha) (e^{\frac{\upbeta - t}{\upgamma}})(\frac{1}{\upgamma})}{1 + (e^{\frac{\upbeta - t}{\upgamma}})^2}$and so is nonlinear because it contains $\upbeta$.}Equation \ref{eq:taylor} shows the general formula for a Taylor series such that

```{=tex}
\begin{align}
P^N(f(x), a)= \sum^{N}_{n = 0} \frac{f^na}{n !}(x-a)^n,
(\#eq:taylor)
\end{align}
```

\noindent where $N$ is the highest derivative order of the function $f(a)$ that is taken beginning from a zero-value derivative order ($n=0$), $a$ is the point where the Taylor series is derived, and $x$ is the point where the
Taylor series is evaluated. As an example, consider  $f(x) = \cos(x)$. Note that, across the continuum of $x$ values (i.e., from $-\infty$ to $\infty$), $\cos(x)$ returns values between -1 and 1 in an oscillatory manner. Computing the second-order Taylor series approximation of $f(x) = \cos(x)$ yields the following function shown in Equation \ref{eq:example-taylor}:

```{=tex}
\begin{align} 
P^2(\cos(x), a) &=  \frac{\frac{\partial^0 \cos(a)}{\partial a^0}}{0!}(x -a)^0 + \frac{\frac{\partial^1 \cos(a)}{\partial a^1}}{1!}(x -a)^1 + \frac{\frac{\partial^2 \cos(a)}{\partial a^2}}{2!} (x -a)^2 \nonumber \\ 
&=  \frac{\cos(0)}{0!}(x -0)^0 - \frac{\sin(0)}{1!}(x -0)^1 - \frac{\cos(0)}{2!}(x -0)^2  \nonumber \\ 
&=  \frac{1}{1}1 - \frac{0}{1}x - \frac{1}{2}x^2  \nonumber \\ 
P^2(\cos(x), 0) &=  1- \frac{1}{2}x^2. 
  (\#eq:example-taylor)
\end{align}
```
\noindent Note that that the second-order Taylor series of $\cos(x)$
perfectly estimates $\cos(x)$ when the point of evaluation $x$ is set
equal to the point of derivation $a$ and estimates $\cos(x)$ with an
increasing amount of error as the difference between $x$ and $a$
increases (see Example \ref{exm:taylor-estimates}).

```{example, taylor-estimates, echo=T}
Estimates of Taylor series approximation of $f(x) = \cos(x)$ as the difference between the point of evaluation $\mathrm{x}$ and the point of derivation $\mathrm{a}$ increases.

 \noindent \textup{Taylor series approximation of $f(x) = \cos(x)$ estimates values that are exactly equal to the values returned by $f(x) = \cos(x)$ when the point of evaluation \textit{x} is set to the point of derivation \textit{a}. The example below computes the value predicted by the Taylor series approximation of $f(x) = \cos(x)$ and by $f(x) = \cos(x)$ when \textit{x} = \textit{a} = 0.}

\useshortskip
\begin{align*}
P^2(\cos(x=0), a=0) &= \cos(x=0) \nonumber \\ 
1- \frac{1}{2}x^2 &=  \cos(0) \nonumber \\ 
1- \frac{1}{2}0^2 &=  1 \nonumber \\ 
1- 0 &=  1 \nonumber \\ 
1 &=  1 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

 \noindent \textup{Taylor series approximation of $f(x) = \cos(x)$ estimates a value that is clearly not equal ($neq$) to the value returned by $f(x) = \cos(x)$when the difference between the point of evaluation \textit{x} and the point of derivation \textit{a} is smaller. The example below computes the value predicted by the Taylor series approximation of $f(x) = \cos(x)$ and by $f(x) = \cos(x)$ when \textit{x} = 1 and  \textit{a} = 0.} 

\useshortskip
\begin{align*}
P^2(\cos(x = 1), 0) &\thickapprox \cos(x = 1) \nonumber \\ 
1- \frac{1}{2}x^2 &\thickapprox   \cos(1) \nonumber \\ 
1- \frac{1}{2}1^2 &\thickapprox   0.54 \nonumber \\ 
1- 0.5 &\thickapprox   0.54 \nonumber \\ 
0.5 &\thickapprox 0.54 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

 \noindent\textup{Taylor series approximation of $f(x) = \cos(x)$ estimates a value that is clearly not equal ($neq$) to the value returned by $f(x) = \cos(x)$ when the difference between the point of evaluation \textit{x} and the point of derivation \textit{a} is larger The example below computes the value predicted by the Taylor series approximation of $f(x) = \cos(x)$ and by $f(x) = \cos(x)$ when \textit{x} = 4 and  \textit{a} = 0.} 


\useshortskip
\begin{align*}
P^2(\cos(x = 4), 0) &\neq \cos(x = 4) \nonumber \\ 
1- \frac{1}{2}x^2 &\neq  \cos(4) \nonumber \\ 
1- \frac{1}{2}4^2 &\neq  -0.65 \nonumber \\ 
1- 16 &\neq  -0.65 \nonumber \\ 
0.5 &\neq  -0.65 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

\noindent \hrulefill
```

\noindent Figure \ref{fig:taylor-vs-nonlin} plots the nonlinear function
of $\cos(x)$ and its second-order Taylor series $P^2(\cos(x)) = 1- \frac{1}{2}x^2$. The
second order Taylor series perfectly estimates $\cos(x)$ when the point
of evaluation ($x$) equals the point of derivation ($a$; $x = a = 0$),
but incurs an increasingly large amount of error as the difference
between the point of evaluation and the point of derivation increases.
For example, at $x = 10$, $\cos(10) = -0.84$, but the Taylor series
outputs a value of -49.50 ($P^2(cos(50)) = 1- \frac{1}{2}10^2 = -49.50$). Therefore, Taylor series' are approximations because they are locally accurate.

```{r taylor-vs-nonlin, include=F, eval=F}

x <- seq(from = 0, to = 10, by = 0.1)
taylor_data <- 1- 0.5*(x)^2
cos_data <- cos(x)

combined_data <- data.frame('x' = x, Taylor = taylor_data, Cos = cos_data, check.names = F)

combined_data_long <- combined_data %>% 
  pivot_longer(cols = c('Cos' , 'Taylor'), names_to = 'curve_type', 
               names_ptypes = factor())

taylor_vs_nonlin_plot <- ggplot(data = combined_data_long, mapping = aes(x = x, y = value, group = curve_type, linetype = curve_type)) + 
  geom_line(size = 1) + 
  labs(linetype = 'Curve type') + 
  annotate(geom = 'text', label = 'P^2*(cos(x)) == 1 - frac(1, 2)*x^2', x = 5, y = -30, parse=T, size = 7) + 
  
  #expression(paste(P^{2}*(cos(x)), ', ', b == 6))

  annotate(geom = 'text', label = 'f(x) == cos(x)', x = 8, y = -10, parse=T, size = 7) + 
  
  geom_segment(inherit.aes = F, mapping = aes(x = 8, xend = 8, y = -9, yend = -1), 
               arrow = arrow(length = unit(0.3, 'cm')), size = 0.5) +
  
  geom_segment(inherit.aes = F, mapping = aes(x = 5, xend = 5, y = -28, yend = -13), 
               arrow = arrow(length = unit(0.3, 'cm')), size = 0.5) +
  scale_y_continuous(name = 'Curve value') + 
  scale_x_continuous(name = 'Point of evaluation (x)', breaks = seq(from = 0, to = 10, by =1)) + 
  theme_classic(base_family = 'Helvetica') + 
  theme(legend.text = element_text(size = 12, color = 'black'), 
        legend.title = element_text(size = 16, color = 'black'), 
        axis.text =element_text(size = 12, color = 'black'), 
        axis.title = element_text(size = 16, color = 'black')) 
  
ggsave(filename = 'Figures/taylor_vs_nonlin.pdf', plot =taylor_vs_nonlin_plot, width = 9, height = 6)


```

```{=tex}
\begin{figure}[H]
  \caption{Estimation Accuracy of Taylor Series Approximation of Nonlinear Function (cos(x))}
  \label{fig:taylor-vs-nonlin}
  \includegraphics{Figures/taylor_vs_nonlin} \hfill{}
  \caption*{Note. \textup{The second order Taylor series perfectly estimates $\cos(x)$ when the point of evaluation ($x$) equals the point of derivation ($a$; $x = a = 0$), but incurs an increasingly large amount of error as the difference between the point of evaluation and the point of derivation increases. For example, at $x = 10$, $\cos(x) = -0.84$, but the Taylor series outputs a value of -49.50 ($P^2(cos(50)) = 1- \frac{1}{2}10^2 = -49.50$). }}
\end{figure}
```

##### Taylor Series Approximation of the Logistic Function

Given that a Taylor series provides a linear approximation of a
nonlinear function and the structural equation modelling framework is linear, the structured latent curve modelling approach uses Taylor series approximations to construct linear representations of nonlinear functions [@browne1991; @browne1993]. In the current simulations, a Taylor series approximation was constructed for the logistic function (Equation \ref{eq:logistic}). Note that, because the logistic function had four parameters ($\uptheta$,
$\upalpha$, $\upbeta$, $\upgamma$), derivatives were computed with
respect to each of the parameters. Using a derivative order set to one
($n = 1$), the following Taylor series was constructed for the logistic
function (Equation \ref{eq:logistic-approx}):

```{=tex}
\begin{align}
 P^1(L(\Uptheta, t)) = L + \frac{\partial L}{\partial \uptheta}(x_{\uptheta}-a_{\uptheta})^1 + \frac{\partial L}{\partial \upalpha}(x_{\upalpha}-a_{\upalpha})^1 + \frac{\partial L}{\partial \upbeta}(x_{\upbeta}-a_{\upbeta})^1 + \frac{\partial L}{\partial \upgamma_{\upgamma}}(x-a_{\upgamma})^1, 
(\#eq:logistic-approx)
\end{align}
```
\noindent where $\mathbf{L(\Uptheta, t)}$ represents the logistic function shown below in
Equation \ref{eq:logistic}:

```{=tex}
\begin{align}
  \mathbf{L(\Uptheta, t)} = \uptheta + \frac{\upalpha - \uptheta}{{1 + e^\frac{\upbeta - t}{\upgamma}}} + \upepsilon, 
(\#eq:logistic)
\end{align}
```

\noindent with $\Uptheta = [\uptheta, \upalpha, \upbeta, \upgamma]$ and  $\mathbf{L(\Uptheta, t)}$ being a vector of scores at all $\mathbf{t}$ time points. In the current context, because each parameter of the logistic function had a unique meaning (see section on [data generation][Data generation]), the point of derivation $a$ differed for each parameter---using the same $a$ value for each parameter to construct the
Taylor series approximation of the logistic function would have yielded
a practically useless equation. Because the logistic Taylor series
approximation (Equation \ref{eq:logistic-approx}) was deployed in a
statistical model (i.e., the structural equation modelling framework), the derivation values
($a_{\uptheta}$, $a_{\upalpha}$, $a_{\upbeta}$, $a_{\upgamma}$) were set
to the mean values estimated by the analysis for each parameter. Thus, the
derivation values were replaced with the following terms:

-   $a_{\uptheta} = \hat{\uptheta}$
-   $a_{\upalpha} = \hat{\upalpha}$
-   $a_{\upbeta} = \hat{\upbeta}$
-   $a_{\upgamma} = \hat{\upgamma}$

\noindent where that a caret $\hat{}$ indicates the mean value estimated for a parameter by the analysis. In order to estimate curves for each $p$ person, the values of
evaluation ($x_{\uptheta}$, $x_{\upalpha}$, $x_{\upbeta}$,
$x_{\upgamma}$) corresponded to the parameter values computed for a
given person ($\uptheta_p$, $\upalpha_p$, $\upbeta_p$, $\upgamma_p$). Thus, the evaluation values were replaced with the following terms:

-   $x_{\uptheta} = \uptheta_p$
-   $x_{\upalpha} = \upalpha_p$
-   $x_{\upbeta} = \upbeta_p$
-   $x_{\upgamma} = \upgamma_p$

\noindent Substituting the above values for the derivation and evaluation values of $x$ and $a$ in the initial logistic Taylor series approximation (Equation \ref{eq:logistic-approx}) yielded the following expression for the logistic Taylor series approximation (Equation \ref{eq:taylor-full}):

```{=tex}
\begin{align}
 P^1(L(\Uptheta, t)) = L(\Uptheta, t) + \frac{\partial L}{\partial \uptheta}(\uptheta_i-\hat{\uptheta})^1 + \frac{\partial L}{\partial \upalpha}(\upalpha_i-\hat{\upalpha_i})^1 + \frac{\partial L}{\partial \upbeta}(\upbeta-\hat{\upbeta})^1 + \frac{\partial L}{\partial \upgamma_{\upgamma}}(\upbeta-\hat{\upbeta})^1.
(\#eq:taylor-full)
\end{align}
```

\noindent Therefore, because the Taylor series was derived using the mean values estimated for each parameter ($\hat{\uptheta}$, $\hat{\upalpha}$, $\hat{\upbeta}$,
$\hat{\upgamma}$), it provided a perfect approximation of the estimated
population curve---the evaluation values for each parameter would have
been set to their corresponding mean estimated value. To estimate the curve of any given $p$ person, the evaluation values could be offset from their corresponding derivation value (i.e., mean estimated value for a parameter) by using the set of parameter values computed for that person ($\uptheta_p$, $\upalpha_p$, $\upbeta_p$, $\upgamma_p$). Note that, because Taylor series approximations are only locally accurate, the predicted curves for any given $p$ person become increasingly inaccurate curves as the difference between the derivation and evaluation values increases (e.g., $\uptheta_i-\hat{\uptheta}$).

##### Fitting the Logistic Taylor Series Approximation Into the Structual Equation Modelling Framework 

Although the logistic Taylor series approximation provides an accurate
estimation of the logistic function, the function in (Equation
\ref{eq:taylor-full}) is modified in the structured latent curve
modelling approach so that it can more effectively fit into the structural equation modelling framework (Equation \ref{eq:sem-framework}). The partial derivative information is stored in the matrix $\mathbf{\Uplambda}$ such that

$$ 
\mathbf{\Uplambda} = 
\begin{bmatrix}
\frac{\partial L(\Uptheta, t_1)}{\partial \uptheta} & \frac{\partial L(\Uptheta, t_1)}{\partial \upalpha}  &  \frac{\partial L(\Uptheta, t_1)}{\partial \upbeta} & \frac{\partial L(\Uptheta, t_1)}{\partial \upgamma}   \\ 
\frac{\partial L(\Uptheta, t_2)}{\partial \uptheta}  & \frac{\partial L(\Uptheta, t_2)}{\partial \upalpha} &  \frac{\partial L(\Uptheta, t_2)}{\partial \upbeta} & \frac{\partial L(\Uptheta, t_2)}{\partial \upgamma} & \\ 
\vdots & \vdots & \vdots & \vdots \\ 
\frac{\partial L(\Uptheta, t_n)}{\partial \uptheta} & \frac{\partial L(\Uptheta, t_n)}{\partial \upalpha}  & \frac{\partial L(\Uptheta, t_n)}{\partial \upbeta} & \frac{\partial L(\Uptheta, t_n)}{\partial \upgamma} \\
\end{bmatrix}.
$$

\noindent As in the structural equation modelling framework where each column of
$\mathbf{\Uplambda}$ specified a basis curve (i.e., a loading of a
growth parameter onto all time points), each column of $\mathbf{\Uplambda}$ here
in the structured latent curve modelling approach contains the loadings
of a logistic function parameter onto all the $n$ time points, with the loadings being determined by the partial derivative of logistic function with respect to that parameter. To predict unique curves for each person, each column can be multiplied by a specific weight $\mathbf{\upiota_p}$ that contains person-specific deviations from each mean estimated parameter value as shown below:

$$ 
\mathbf{\upiota_p} = 
\begin{bmatrix}
\hat{\uptheta} - \uptheta_p   \\ 
\hat{\upalpha} - \upalpha_p   \\ 
\hat{\upbeta} - \upbeta_p \\ 
\hat{\upgamma_i} - \upgamma_p \\
\end{bmatrix},
$$ 


\noindent where a caret ($\hat{}$) indicates the mean value estimated
for a given parameter and a subscript $p$ indicates a parameter value
computed for a person. With a matrix $\mathbf{\Uplambda}$ containing
logistic function parameter loadings and a vector $\mathbf{\upiota_p}$ containing
person-specific weights, the Taylor series of Equation
\ref{eq:taylor-full} that predicted a person's scores over time can be
rewritten to become the following expression of Equation
\ref{eq:slcm-nonsem}:

```{=tex}
\begin{align}
 \mathbf{y_p} = \mathbf{L(\Uptheta, t)} + \mathbf{\Uplambda\upiota_p} + \mathbf{\mathcal{E}_p}.
 (\#eq:slcm-nonsem)
\end{align}
```

\noindent Importantly, because of the logistic function ($\mathbf{L(\Uptheta, t)}$) in the
above expression (Equation \ref{eq:slcm-nonsem}), the model no longer
fits into the general structural equation modelling framework (Equation \ref{eq:sem-framework}). To modify Equation \ref{eq:slcm-nonsem} such that it fits into the structural equation modelling framework, the structured latent curve modelling approach recognizes that the logistic function ($\mathbf{L(\Uptheta, t)}$) is invariant under a scaling constant and uses this property to rewrite  $\mathbf{L(\Uptheta, t)}$ as a weighted sum of the partial derivative loading matrix [$\mathbf{\Uplambda}$\; @shapiro1987]. Briefly, the
logistic function vector $\mathbf{L(\Uptheta, t)}$ is invariant under a constant scaling
property because, given some constant scalar value $k \ge 0$ and a set
of parameter values ($\Uptheta$), there exists another set of parameter
values ($\tilde{\Uptheta}$) that can produce the same values (see
Equation \ref{eq:icsf} and Example \ref{exm:icsf-ex} below).

```{=tex}
\begin{align}
 k\mathbf{L(\Uptheta, t)} = \mathbf{L(\tilde{\Uptheta}, t)}
 (\#eq:icsf)
\end{align}
```

```{example, icsf-ex, echo=T}
Invariability under a constant scaling factor of logistic function (Equation \ref{eq:logistic}).  

\noindent \textup{Given $t = [0, 1, 2, 3]$, $\Uptheta = [\uptheta = 3.00$, $\upalpha = 3.32$, $\upbeta = 180.00$, $\upgamma = 20.00$], and some constant scaling factor $k = 2.00$, then there exists some set of parameter values $\tilde{\Uptheta}$ that produces the same values as $kL(\Uptheta)$. In the current example, $\tilde{\Uptheta} = [\uptheta = 6.00$, $\upalpha = 6.64$, $\upbeta = 180.00$, $\upgamma = 20.00$].} 

\useshortskip
\begin{align*}
\mathbf{kL(\Uptheta, t)} &= \mathbf{L(\tilde{\Uptheta}, t)} \nonumber \\ 
2*[3.00, 3.02, 3.30, 3.32] &=  [6.00, 6.04, 6.60, 6.64] \nonumber \\ 
[6.00, 6.04, 6.60, 6.64]  &= [6.00, 6.04, 6.60, 6.64] 1 \nonumber \\ 
\end{align*}
\useshortskip
\vspace*{-25mm}

\noindent \hrulefill
```

\noindent If a function has the property of being invariant under a
scaling factor, then it can also be expressed as the following
matrix-vector product shown in Equation \ref{eq:logistic-matrix-vector}
[@shapiro1987]:

\begin{align}
 \mathbf{L(\Uptheta, t)} = \mathbf{\Lambda\uptau},
(\#eq:logistic-matrix-vector)
\end{align}

\noindent where $\mathbf{\Uplambda}$ contains the partial derivative loadings\footnote{This is also known as a Jacobian matrix.} and
$\mathbf{\uptau}$ is a vector whose values are otbained by pre-multiplying the output of the logistic function ($\mathbf{L(\Uptheta, t)}$) by the inverse of the partial derivative loading matrix ${\Lambda\uptau}^{-1}$. Solving for $\mathbf{\uptau}$ yields a vector whose contents contain the mean values estimated for parameters that enter the logistic function in a linear way and
zeroes for parameters that enter the function in a nonlinear way (i.e.,
parameters that exist within their own partial derivative). Hence, $\mathbf{\uptau}$ is often called a mean vector [@blozis2004; @preacher2015].  In the current example, $\uptheta$ and $\upalpha$ enter the logistic function
in a linear way and $\upbeta$ and $\upgamma$ enter the logistic function
in a nonlinear way and so the first two entries of $\mathbf{\uptau}$
contain the values estimated for $\uptheta$ and $\upalpha$ (i.e.,
$\hat{\uptheta}$ and $\hat{\upalpha}$) and the last two entries contain zeroes. Example \ref{exm:tau-vector} below shows that the first two
values of $\mathbf{\uptau}$ are indeed the values estimated for
$\uptheta$ and $\upalpha$ and the last two values are zero.

```{example, tau-vector, echo=T}
Computation of mean vector $\mathbf{\uptau}$. 
  
 \noindent \textup{Given the parameter estimates of $\hat{\uptheta} = 3.00$, $\hat{\upalpha} = 3.32$, $\hat{\upbeta} = 180.00$, and $\hat{\upgamma} = 20.00$ and $\mathbf{t}$ = [0, 1, 2, 3], $\mathbf{\uptau}$ = [3.00, 3.32, 0, 0], then } 

\useshortskip
\begin{align*}
\mathbf{L(\Uptheta, t)} &= \mathbf{\Lambda\uptau} \\ 
[3.00, 3.02, 3.30, 3.32] &= \begin{bmatrix}
1.00 & 0.00 & 0.00  & 0.00 \\ 
0.95  & 0.05 & -0.00 & 0.00 \\ 
0.05 & 0.95 & -0.00 & -0.00 \\ 
0.00 & 1.00  & 0.00 & 0.00 \\
\end{bmatrix} \mathbf{\uptau} \\ 
\begin{bmatrix}
1.00 & 0.00 & 0.00  & 0.00 \\ 
0.95  & 0.05 & -0.00 & 0.00 \\ 
0.05 & 0.95 & -0.00 & -0.00 \\ 
0.00 & 1.00  & 0.00 & 0.00 \\
\end{bmatrix}^{-1}
\begin{bmatrix} 
3.00 \\ 3.02 \\ 3.30 \\ 3.32
\end{bmatrix} &=  \mathbf{\Lambda\uptau} \\ 
 \mathbf{\uptau} &= [3.00, 3.32, 0, 0]\\
\end{align*}
\vspace*{-25mm}

\noindent \hrulefill
```

\noindent With $\mathbf{L(\Uptheta, t)} = \mathbf{\Uplambda\uptau}$, Equation \ref{eq:slcm-nonsem} can be rewritten in a linear equation as shown below in Equation \ref{eq:taylor-linear}:

\begin{align}
 \mathbf{y_p} = \mathbf{\Uplambda\uptau} + \mathbf{\Uplambda\upiota_p} + \mathbf{\mathcal{E}_p}.
 (\#eq:taylor-linear)
 \end{align}
 
\noindent The mean vector $\mathbf{\uptau}$ and vector of
person-specific deviations $\mathbf{\upiota_p}$ can be combined into a
new vector $\mathbf{s_p}$ that represents the person-specific weights
applied to the basis curves in $\mathbf{\Uplambda}$ such that

$$  
\mathbf{s_p} = \mathbf{\uptau + \upiota_p} =
\begin{bmatrix} 
\hat{\uptheta} + \hat{\uptheta} - \uptheta_p \\ 
\hat{\upalpha} + \hat{\upalpha} - \upalpha_p \\ 
0 + \hat{\upbeta} - \upbeta_p \\ 
0 + \hat{\upgamma} - \upgamma_p \\ 
\end{bmatrix}
$$

\noindent and 

\begin{align}
\mathbf{y_p} = \mathbf{\Uplambda s_p} + \mathbf{\mathcal{E}_p}. 
(\#eq:taylor-final)
\end{align}

\noindent Because the expected value of the person-specific weights
($\mathbf{s_p}$) is the mean vector ($\mathbf{\uptau}$;
$\mathbb{E}[{\mathbf{s_p}}] = \mathbf{\uptau}$, the expected set
of scores predicted across all people ($\mathbb{E}[{\mathbf{y_p}}]$) gives back the original expression for the logistic function matrix-vector product in Equation
\ref{eq:logistic-matrix-vector} as shown below in Equation \ref{eq:expected-value}:

\begin{align}
 \mathbb{E}[{\mathbf{y_p}}] = \mathbf{\Uplambda\uptau} = \mathbf{L(\Uptheta, t)}. 
(\#eq:expected-value)
\end{align}

\noindent Therefore, the structured latent curve modelling approach
successfully reproduces the output of the nonlinear logistic function
(Equation \ref{eq:logistic}) with the linear function of Equation
\ref{eq:taylor-final}. Note that that no error term exists in Equation \ref{eq:expected-value} because the expected value of the error
values is zero ($\mathbb{E}[{\mathbf{\mathcal{E}_p}}] = 0$).

##### Estimating Parameters in the Structured Latent Curve Modelling Approach 

To estimate parameter values, the full-information maximum
likelihood shown in Equation \ref{eq:fiml-person} was computed for each
person (i.e., likelihood of observing a $p$ person's data given the
estimated parameter values):

\begin{align}
\mathcal{L}_p = k_p \ln(2\pi) + \ln(|\mathbf{\Sigma_p}| + (\mathbf{y_p} - \mathbf{\upmu_p})^\top \mathbf{\Sigma_p}^{-1}(\mathbf{y_p} - \mathbf{\upmu_p}),
(\#eq:fiml-person)
\end{align}


\noindent where $k_p$ is the number of non-missing values for a given
$p$ person, $\mathbf{\Sigma_p}$ is the model-implied covariance matrix
with rows and columns filtered at time points where person $p$ has
missing data, $\mathbf{y_p}$ is a vector containing the data points that
were collected for a $p$ person (i.e., filtered data), and
$\mathbf{\upmu_p}$ is the model-implied mean vector that is filtered at
time points where person $p$ has missing data. Note that, because all
simulations assumed complete data across all times points, no filtering
procedures were executed [for a review of the filtering procedure, see @boker2020, Chapter 5]. Thus, computing the above full-information
maximum likelihood in Equation \ref{eq:fiml-person} was equivalent to
computing the below likelihood function in Equation
\ref{eq:ml-estimation}:

\begin{align}
\mathcal{L}_p = k_p \ln(2\pi) + \ln(|\mathbf{\Sigma}| + (\mathbf{y_p} - \mathbf{\upmu})^\top \mathbf{\Sigma}^{-1}(\mathbf{y_p} - \mathbf{\upmu}),  
(\#eq:ml-estimation)
\end{align}

\noindent where $\mathbf{\Sigma}$ is the model-implied covariance matrix,
$\mathbf{y_p}$ contains the data collected from a $p$ person, and
$\mathbf{\upmu}$ is the model-implied mean vector. The model-implied
covariance matrix $\mathbf{\Sigma}$ is computed using Equation
\ref{eq:covariance} below:

\begin{align}
\mathbf{\Sigma} = \mathbf{\Uplambda\Uppsi\Uplambda} + \mathbf{\Upomega}_{\mathcal{E}},   
(\#eq:covariance)
\end{align}

\noindent where $\mathbf{\Uppsi}$ is the random-effect covariance matrix
and $\mathbf{\Upomega}_{\mathcal{E}}$ contains the error variances at
each time point. The mean vector $\mathbf{\upmu}$ was computed using
Equation \ref{eq:mean-structure} shown below:

\begin{align}
\mathbf{\upmu} = \mathbf{\Uplambda\uptau}. 
(\#eq:mean-structure)
\end{align}

\noindent Parameter estimation was conducted by finding values for the model-implied
covariance matrix $\mathbf{\Sigma}$ and the model-implied mean vector
$\mathbf{\upmu}$ that maximized the sum of log-likelihoods across all $P$ people
(see Equation \ref{eq:max-ll} below):

\begin{align}
\mathcal{L} = \underset{\mathbf{\Sigma},\mathbf{\upmu} }{\argmax} \sum^P_{p = 1} \mathcal{L}_p.
(\#eq:max-ll)
\end{align}

\noindent In OpenMx, the above problem was solved using the sequential
least squares quadratic program [for a review, see @kraft1994].



# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

```{=tex}
\endgroup
```

