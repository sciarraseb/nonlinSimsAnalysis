---
shorttitle        : "Measurement timing"
format          : "pandoc"
header-includes:
  - \usepackage{nccmath}
  - \usepackage{caption}
  - \usepackage{textcomp} #for copyright symbol on title page
  - \usepackage{longtable}
  - \usepackage{makecell}
  - \usepackage[section]{placeins}
  - \usepackage{setspace}
  - \usepackage{biblatex}
  - \usepackage{booktabs}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{xcolor}
  - \usepackage{amsthm} 
  - \usepackage{amsmath} ##needed for argmax
  - \usepackage{bm}  #thicker bold in math 
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \usepackage{setspace} #needed to doublespace caption text (using \doublespacing)
  - \usepackage[labelfont = {bf, up}]{caption} 
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \usepackage{upgreek}  #required for non-italicized Greek letters
  - \usepackage{subcaption}
  - \captionsetup[figure]{labelfont={normalfont, bf}, singlelinecheck=false, labelsep=newline}
  - \DeclareCaptionFont{figCaptionFont}{\fontfamily{phv}\doublespacing} #sets caption font to sans serif font of Helvetica 
  - \DeclareCaptionFont{figCaptionSize}{\fontsize{11pt}{13.2pt}\selectfont} #set caption font size to footnote 
  - \DeclareCaptionFont{tabCaptionSize}{\small} #caption size for table title
  - \DeclareCaptionFont{figCaptionStyle}{\textup}  #set caption font to non-italicized font  
  - \DeclareCaptionLabelSeparator{captionSep}{\newline} #separates figure label and figure title with required white space
  - \captionsetup[figure]{labelfont={figCaptionStyle, bf}, font = {figCaptionFont,figCaptionSize, figCaptionStyle}, labelsep = captionSep, justification= raggedright}
  - \captionsetup[table]{labelfont={tabCaptionSize, bf}, font = {figCaptionFont, tabCaptionSize, figCaptionStyle}, labelsep = captionSep, justification= raggedright}
  - \newenvironment{helvenv}{\fontfamily{phv}\selectfont}{}
  - \raggedbottom #ensures text starts from top of page and any white space is at the botom

#both are needed to change font type of table footnotes
  - \usepackage{anyfontsize}
  - \AtBeginEnvironment{ThreePartTable}{\fontfamily{phv} \fontsize{10.5pt}{12pt}\selectfont} 
  - \AtBeginEnvironment{tablenotes}{\fontsize{9.5pt}{11.4pt}\selectfont} 

#environment numbering 
  - \setcounter{section}{0} 
  - \makeatletter \renewcommand\thesection{}\renewcommand\thesubsection{\@arabic\c@section.\@arabic\c@subsection} \makeatother

#set table line widths 
  - \setlength\cmidrulewidth{1pt} #line thickness of lines within table and in multi-row headers
  - \setlength\lightrulewidth{1pt} #line thickness of bottom line in header 

  - \newtheorem{theorem}{Theorem}
  - \newtheorem{example}[theorem]{Example}
  - \renewcommand\theadfont{} #sets cell font to be same as table font 
  
  #set figure title text
  - \newcommand{\figurefootnote}{\raggedright\linespread{2}\fontfamily{phv}\fontsize{9.5pt}{11.4pt}\selectfont \textit{Note. }}
  
  #modifies heading levels of 4-5 to follow apa7
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  
  
author: 
  - name          : "Sebastian Sciarra "
    affiliation   : "1"
    corresponding : yes    
    email         : "ssciarra@uoguelph.ca"
affiliation: 
  - id            : "1"
    institution   : "University of Guelph"
keywords          : "measurement timing, nonlinear "
wordcount         : "5554 words"
floatsintext      : yes
linkcolor         : blue
figsintext        : yes 
figurelist        : no
tablelist         : no
footnotelist      : no
numbersections    : yes
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa7"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
classoption       : "man"
output            : papaja::apa6_pdf
keep_tex: true
editor_options: 
  markdown: 
    wrap: 72
bibliography: dissertation_references.bib
---

```{r package_loading, include=F}
#load packages
library(easypackages)
packages <- c('devtools','tidyverse', 'RColorBrewer', 'parallel', 'data.table', 'kableExtra', 'ggtext', 'egg', 'nonlinSims','papaja', 'ggbrace', 'cowplot')
libraries(packages)
load_all()
```

```{r knitting_setup, echo=F, message = F, warning = F}
#import raw data files (needed for computing variances)
exp_1_raw <- convert_raw_var_to_sd(raw_data = read_csv('data/exp_1_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "measurement_spacing", "midpoint"), factor)
  
exp_2_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_2_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "measurement_spacing", "sample_size"), factor)

exp_3_raw <-convert_raw_var_to_sd(raw_data = read_csv('data/exp_3_data.csv')) %>%
  mutate_at(.vars = c("number_measurements", "time_structuredness", "sample_size"), factor)

#unfiltered data 
param_summary_exp_1 <- readRDS(file = 'data/uf_param_summary_exp_1.RData')
param_summary_exp_2 <- readRDS(file = 'data/uf_param_summary_exp_2.RData')
param_summary_exp_3 <- readRDS(file = 'data/uf_param_summary_exp_3.RData')

#create analytical versions of summary data + converts vars to sds
exp_1_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_1, exp_num = '1')
exp_2_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_2, exp_num = '2')
exp_3_analytical <- generate_likert_days_data_sets(summary_data = param_summary_exp_3, exp_num = '3')

combined_analytical_exp_1 <- rbind(exp_1_analytical$likert, exp_1_analytical$days)
combined_analytical_exp_2 <- rbind(exp_2_analytical$likert, exp_2_analytical$days)
combined_analytical_exp_3 <- rbind(exp_3_analytical$likert, exp_3_analytical$days)

#create condition summary data sets 
cond_summary_exp_1 <- compute_condition_summary(param_summary_data = combined_analytical_exp_1, facet_var = 'measurement_spacing', 
                          ind_vars = c('number_measurements', 'measurement_spacing', 'midpoint'))
cond_summary_exp_2 <- compute_condition_summary(param_summary_data = combined_analytical_exp_2, facet_var = 'measurement_spacing', 
                  ind_vars = c('number_measurements', 'measurement_spacing', 'sample_size'))

cond_summary_exp_3 <- compute_condition_summary(param_summary_data = combined_analytical_exp_3, facet_var = 'time_structuredness', 
                          ind_vars = c('number_measurements', 'sample_size', 'time_structuredness'))
```

```{r pre_knitting_setup_unfiltered, echo=F, eval=F, include=F}
#code should be computed before knitting to decrease knitting time 
#load data from experiments
exp_1 <- read_csv(file = 'data/exp_1_data.csv')
exp_2 <- read_csv(file = 'data/exp_2_data.csv')
exp_3 <- read_csv(file = 'data/exp_3_data.csv')

#compute parameter summary statistics  
param_summary_exp_1 <- compute_parameter_summary(data = exp_1, exp_num = 1)
param_summary_exp_2 <- compute_parameter_summary(data = exp_2, exp_num = 2)
param_summary_exp_3 <- compute_parameter_summary(data = exp_3, exp_num = 3)

#necessary factor conversions 
param_summary_exp_1$number_measurements <- factor(param_summary_exp_1$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_1$midpoint <- factor(param_summary_exp_1$midpoint, levels = c(80, 180,280))

param_summary_exp_2$number_measurements <- factor(param_summary_exp_2$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_2$sample_size <- factor(param_summary_exp_2$sample_size, levels = c(30, 50, 100, 200, 500, 1000))

param_summary_exp_3$number_measurements <- factor(param_summary_exp_3$number_measurements, levels = c(5, 7, 9,11))
param_summary_exp_3$sample_size <- factor(param_summary_exp_3$sample_size, levels = c(30, 50, 100, 200, 500, 1000))

#write data sets 
#save parameter summary files as RData files so that metadata are correctly stored (e.g., factor levels, variable types)
saveRDS(object = param_summary_exp_1, file = 'data/uf_param_summary_exp_1.RData')
saveRDS(object = param_summary_exp_2, file = 'data/uf_param_summary_exp_2.RData')
saveRDS(object = param_summary_exp_3, file = 'data/uf_param_summary_exp_3.RData')
```

### When the Nature of Change is Suspected, How Should Measurements be Spaced?

Table \ref{tab:summary-table-exp1-nc} lists the nature-of-change value that results in the highest modelling accuracy for each spacing schedule. Text in the 'Optimal Nature-of-Change Value' column indicates the nature-of-change value that results in the highest modelling accuracy for each spacing schedule. The 'Error Bar Summary' columns list the error bar lengths obtained for each day-unit parameter using the nature-of-change value listed in the 'Optimal Nature-of-Change Value' column.\footnote{Bias values are not presented because the differences across the schedules are negligible.} Note that the error bar lengths are obtained by computing the average error bar length across all manipulated measurement numbers for the optimal nature-of-change value. The following nature-of-change values yield the highest modelling accuracy for each spacing schedule:  

* equal spacing: $\upbeta_{fixed}$ = 180
* time-interval increasing spacing: $\upbeta_{fixed}$ = 80
* time-interval decreasing spacing: $\upbeta_{fixed}$ = 280
* middle-and-extreme spacing: $\upbeta_{fixed}$ = 180

To understand why the modelling accuracy of each spacing schedule is highest with a specific nature of change, it is important to consider the locations on the curve where each schedule samples data. Figure \ref{fig:midpoint_plot} shows the measurement locations (indicates by dots) where each spacing schedule samples data for each manipulated nature of change ($\upbeta_{fixed} \in$ {80, 180, 180}). In Figure \ref{fig:midpoint_plot}A, data are sampled according to the equal spacing schedule. In Figure \ref{fig:midpoint_plot}B, data are sampled according to the time-interval increasing spacing schedule. In Figure \ref{fig:midpoint_plot}C, data are sampled according to the time-interval decreasing spacing schedule. In Figure \ref{fig:midpoint_plot}D, data are sampled according to the middle-and-extreme spacing schedule. Black curves indicate natures of change that lead to the highest modelling accuracy, and so are optimal. Gray curves indicate natures of change that lead to suboptimal modelling accuracy. Error bar lengths from Table \ref{tab:summary-table-exp1-nc} have been printed on each panel to provide a reference. 

Before explaining why each spacing schedule has an optimal curve, it is important to define change. For the purpose of this discussion, change occurs when the first derivative of the logistic function has a nonzero value, with larger absolute first derivative values implying greater change. Figure \ref{fig:logistic_function_first_dev} shows each nature of change used in Experiment 1 (solid line) along with its corresponding first derivative curve (dotted line). For each nature of change, the first derivative value reaches its peak at the value set for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$). In Figure \ref{fig:logistic_function_first_dev}A, the first derivative is greatest at day 80. In Figure \ref{fig:logistic_function_first_dev}B, the first derivative is greatest at day 180. In Figure \ref{fig:logistic_function_first_dev}C, the first derivative is greatest at day 280.  Therefore, for each manipulated nature of change, change is greatest at the value set for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$). 

```{r first-deriv-curve, echo=F, eval=F}
#log function data 
t <- 0:360
theta <- 0
alpha <- 0.25
beta_80 <- 80
beta_180 <- 180
beta_280 <- 280
gamma <- 20

theta_deriv <- 0
alpha_deriv <- 10

#log function
log_function_80 <- expression(theta + (alpha - theta)/(1 + exp((beta_80 - t)/gamma)))
log_function_180 <- expression(theta + (alpha - theta)/(1 + exp((beta_180 - t)/gamma)))
log_function_280 <- expression(theta + (alpha - theta)/(1 + exp((beta_280 - t)/gamma)))

#expressions for derivatives (allows lines to occupy same y-axis range)
log_function_80_der <- expression(theta_deriv + (alpha_deriv - theta_deriv)/(1 + exp((beta_80 - t)/gamma)))
log_function_180_der <- expression(theta_deriv + (alpha_deriv - theta_deriv)/(1 + exp((beta_180 - t)/gamma)))
log_function_280_der <- expression(theta_deriv + (alpha_deriv - theta_deriv)/(1 + exp((beta_280 - t)/gamma)))

#compute derivatives
log_function_list <- list(log_function_80_der, log_function_180_der, log_function_280_der)
log_function_deriv_ls <- rapply(object = log_function_list, f = D, name = 't', how = 'unlist')

log_function_df <- data.frame('time' = t,
                              'log_function_80' = eval(log_function_80),
                              'log_function_180' = eval(log_function_180),
                              'log_function_280' = eval(log_function_280))

deriv_function_df <- data.frame('time' = t, 
  'log_function_80' =  eval(log_function_deriv_ls[[1]]), 
                                'log_function_180' =  eval(log_function_deriv_ls[[2]]), 
                                'log_function_280' =  eval(log_function_deriv_ls[[3]]))

#convert long function and deriv curves to long and then join on log_curve
log_func_long <- log_function_df %>% 
  pivot_longer(cols = log_function_80:log_function_280, names_to = 'log_curve', values_to = 'curve_value', names_transform = factor) 

deriv_func_long <- deriv_function_df %>% 
  pivot_longer(cols = log_function_80:log_function_280, names_to = 'log_curve', values_to = 'deriv_value', names_transform = factor)

log_deriv_func_long <- log_func_long %>% 
  left_join(y = deriv_func_long, by = c('log_curve', 'time')) 
log_deriv_func_long$log_curve <- recode_factor(.x =log_deriv_func_long$ log_curve, 
                     'log_function_80' = 'bold(A:beta[fixed]==80)', 
                     'log_function_180' = 'bold(B:beta[fixed]==180)', 
                     'log_function_280' = 'bold(C:beta[fixed]==280)')

log_deriv_func_long <- log_deriv_func_long %>% pivot_longer(cols = 'curve_value':'deriv_value', values_to = 'value', names_to = 'curve_type', 
                                     names_transform = factor)

deriv_plot <- ggplot(data = log_deriv_func_long, mapping = aes(x = time, y = value, 
                                                               group = curve_type, linetype = curve_type)) + 
  geom_line(size = 4) + 
  theme_classic() +
  scale_y_continuous(name = ' ', breaks = NULL) + 
  scale_linetype_manual(name = 'Curve Type', values = c(1, 3), 
                        labels = c('Logistic curve', 'Rate of change \n (first derivative')) + 
 facet_wrap_custom( ~ log_curve, scales = "free", ncol = 1, nrow = 3 , dir = 'h',
                     labeller = label_parsed,  

                            scale_overrides = list(
                            scale_override(1,
                              scale_x_continuous(name = 'Day', 
                                breaks = c(0, 80, 180, 240, 300, 360),
                                limits = c(0, 360))), 
                          
                             scale_override(which = 2,
                              scale_x_continuous(name = 'Day', 
                                breaks = c(0, 60, 120, 180, 240, 300, 360),
                                limits = c(0, 360))), 
                            
                            scale_override(which = 3,
                              scale_x_continuous(name = 'Day', 
                                breaks = c(0, 60, 120, 180, 280, 360),
                                limits = c(0, 360))))) + 
    
                         
  theme_classic(base_family = 'Helvetica') + 
  theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 60, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
       
      #axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #legend details
      legend.text = element_text(size = 50),
      legend.margin = margin(unit(c(0, 0, 0, 10), "cm")),
      legend.title = element_text(size = 60),
      legend.key.size = unit(3, 'cm'),
     # legend.position = c(0.2, 0.3), 
     legend.position = 'right', 
      legend.direction = 'vertical',
      legend.box.background = element_rect(colour = 'black', size = 3),

      #panel details
      panel.spacing.y = unit(x = 4, units = 'cm'))

set_panel_size(p = deriv_plot, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/deriv_plot.pdf')

```
 
```{=tex}
\begin{figure}[H]
  \caption{Rate of Change (First Derivative Curve) for Each Nature of Change Curve Manipulated in Experiment 1}
    \includegraphics[width=6in, height = 8in]{Figures/deriv_plot} 
    \label{fig:logistic_function_first_dev}
\end{figure}
\begin{figure}[H]
  \figurefootnote{Panel A: Logistic curve defined by $\upbeta_{fixed}$ = 80, with first-derivative curve peaking at day 80. Panel B: Logistic curve defined by $\upbeta_{fixed}$ = 180, with first-derivative curve peaking at day 180. Panel C: Logistic curve defined by $\upbeta_{fixed}$ = 280, with first-derivative curve peaking at day 280.} 
\end{figure}
```


Figure \ref{fig:midpoint_plot} provides three reasons to suggest that sampling measurements closer to the period of greatest change increases modelling accuracy. First, for each spacing schedule, more measurements lie closer to the area of greatest change on the optimal black curve than on the suboptimal gray curves. One clear example can be observed for the measurement locations under middle-and-extreme spacing (see Figure \ref{fig:midpoint_plot}D). In looking across the nature-of-change curves, only the measurement locations of the middle three measurements on each curve are different. For the optimal black nature of change, the middle three measurements are centered on the period of greatest change. For the gray suboptimal nature-of-change curves, the middle three measurements are taken near regions of little change (near-zero first derivative). Therefore, nature-of-change curves are optimal for their respective spacing schedules because measurements are taken closest to the curves' periods of change.

Second, modelling accuracy under time-interval increasing and decreasing spacing is nearly identical because each spacing schedule samples data at the exact same rates of change. In looking at Table \ref{tab:summary-table-exp1-nc}, it is important to realize that the error bar lengths for all the day-unit parameters obtained with time-interval increasing and decreasing spacing schedules for their optimal curves lie within a decimal point of each other. As an example, the average error bar length obtained for the fixed-effect days-to-halfway elevation ($\upbeta_{fixed}$) is 5.80 days with time-interval increasing spacing and a nature-of-change value of 80 and 5.84 days with time-interval decreasing spacing and a nature-of-change value of 280. The near equivalence of the error bar lengths between time-interval increasing and decreasing spacing schedules occurs because the rates of change (i.e., first derivative values) at the sampled locations are the exact same. Table \ref{tab:first-deriv} lists the curve value and measurements days when the time-interval increasing and decreasing spacing schedules sample the same first-derivative values. Note that, because the time-interval increasing and decreasing spacing schedules sample data in opposite orders, the first-derivative values are sampled in opposite orders. In summary, although the time-interval increasing and decreasing spacing schedules sample data on different days on their respective optimal curves, they obtain (nearly) identical modelling accuracy because the first-derivative values of the optimal curves are the exact same at the sampled data points. 

```{r inc-dec-first-dev, echo=F}
#log function data 
t <- 0:360
theta <- 3
alpha <- 3.32
beta_80 <- 80
beta_180 <- 180
beta_280 <- 280
gamma <- 20

#log function
log_function <- expression(theta + (alpha - theta)/(1 + exp((beta_180 - t)/gamma)))
log_function_deriv <- eval(D(log_function, name = 't'))

log_function_df <- data.frame('time' = t, 
                              'log_function' = round(eval(log_function), digits = 2), 
                              'first_deriv' = log_function_deriv)

#compute measurements days 
time_inc_days <- compute_measurement_schedule(time_period = 360, num_measurements = 5, smallest_int_length = 30, measurement_spacing = 'time_inc')$measurement_days
time_dec_days <- compute_measurement_schedule(time_period = 360, num_measurements = 5, smallest_int_length = 30, measurement_spacing = 'time_dec')$measurement_days

#combine measurement days into df
time_inc_table <- data.frame('id' = 1:length(time_inc_days),
           'time_inc' = time_inc_days) %>%
  pivot_longer(cols = 2, names_to = 'schedule', values_to = 'measurement_day', names_transform = factor) %>%
  left_join(y = log_function_df, by = c('measurement_day' = 'time')) %>%
   pivot_wider(names_from = 'schedule', values_from = c('measurement_day', 'first_deriv', 'log_function')) %>% 
  select(first_deriv_time_inc, log_function_time_inc, measurement_day_time_inc)


time_dec_table <- data.frame('id' = 1:length(time_dec_days),
           'time_dec' = time_dec_days) %>%
  pivot_longer(cols = 2, names_to = 'schedule', values_to = 'measurement_day', names_transform = factor) %>%
  left_join(y = log_function_df, by = c('measurement_day' = 'time')) %>%
   pivot_wider(names_from = 'schedule', values_from = c('measurement_day', 'first_deriv', 'log_function')) %>%
  select(first_deriv_time_dec, log_function_time_dec, measurement_day_time_dec) %>% 
  arrange(desc(measurement_day_time_dec))

time_inc_table$first_deriv_time_inc <- signif(x = time_inc_table$first_deriv_time_inc, digits = 3)
time_dec_table$first_deriv_time_dec <- signif(x = time_dec_table$first_deriv_time_dec, digits = 3)

combined_table <- cbind(time_inc_table, time_dec_table) %>% 
  select(first_deriv_time_inc, log_function_time_inc, measurement_day_time_inc, 
         log_function_time_dec, measurement_day_time_dec)

kbl(x = combined_table, format = 'latex', 
   longtable = T, booktabs = T, centering = T, escape = F, 
   col.names = c('First Derivative Value', 'Curve Value', 'Measurement Day', 'Curve Value', 'Measurement Day'), 
    align = c('l', rep(x = 'c', times = ncol(combined_table) - 1)), 
    caption = 'Identical First-Derivative Sampling of Time-Interval Increasing and Decreasing Spacing Schedules') %>% 
  kable_styling(position = 'left') %>% 
   #header
  column_spec(column = 1, width = '4cm') %>%
  column_spec(column = 2:5, width = '2.5cm') %>%
  add_header_above(header = c(' ' = 1, 'Time-Interval Increasing' = 2,' Time-Interval Decreasing' = 2)) %>% 
   footnote(escape = F, threeparttable = T, general_title =  ' ', general = " ")
```


Third, middle-and-extreme spacing obtains higher modelling accuracy than equal spacing for the same nature of change because it samples data more times near periods of change. 

The finding that modelling accurayc increases when data are measured near periods of change is certainly not new. cite papers that have discussed the idea under contexts of mediation and correlation. Cite Timmons & Preacher (2015)

Therefore, the results of Experiment 1 suggest that. 


```{r summary-table-exp1, echo=F}
#error bar lengths for each spacing schedule
errorbar_lengths_equal <- compute_errorbar_lengths(exp_analytical_days = exp_1_analytical$days, iv_level = 'Equal spacing', num_measurements = 7)$errorbar_length

errorbar_lengths_time_inc_nm9 <- compute_errorbar_lengths(exp_analytical_days = exp_1_analytical$days, iv_level = 'Time-interval increasing', num_measurements = 9)$errorbar_length

errorbar_lengths_time_dec_nm9<- compute_errorbar_lengths(exp_analytical_days = exp_1_analytical$days, iv_level = 'Time-interval decreasing', num_measurements = 9)$errorbar_length

errorbar_lengths_mid_ext_nm9<- compute_errorbar_lengths(exp_analytical_days = exp_1_analytical$days, iv_level = 'Middle-and-extreme spacing', num_measurements = 9)$errorbar_length

#combine vectors into a list 
errorbar_lengths_list <- list('equal' = errorbar_lengths_equal, 
                              'time_inc' = errorbar_lengths_time_inc_nm9, 
                              'time_dec' = errorbar_lengths_time_dec_nm9, 
                              'mid_ext' = errorbar_lengths_mid_ext_nm9)

beta_fixed_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[1])})))
gamma_fixed_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[2])})))
beta_rand_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[3])})))
gamma_rand_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[4])})))


summary_table <- data.frame('Spacing Schedule' = c('\\thead[lt]{Equal \\\\ Figure \\ref{fig:exp1_plot_equal}}',
                                                   '\\thead[lt]{Time-interval increasing \\\\ Figure \\ref{fig:exp1_plot_time_inc}}', 
                                                   '\\thead[lt]{Time-interval decreasing \\\\ Figure \\ref{fig:exp1_plot_time_dec}}', 
                                                   '\\thead[lt]{Middle-and-extreme \\\\ Figure \\ref{fig:exp1_plot_time_mid_ext}}'), 
                            'Low Bias' = c('NM $\\ge$ 9', 
                                           'NM $\\ge$ 9', 
                                           'NM $\\ge$ 9', 
                                           'NM = 11'), 
                            'High Precision' = c('No cells', 
                                            'No cells', 
                                            'No cells',
                                            'No cells'), 
                            'Qualitative Summary' = c('Largest improvements in bias and precision with NM = 7', 
                                                      'Largest improvements in bias and precision with NM = 9', 
                                                      'Largest improvements in bias and precision with NM = 9', 
                                                      'Largest improvements in bias and precision with NM = 9'), 
                            
                            '$\\upbeta_{fixed}$' = beta_fixed_errorbar_lengths, 
                            '$\\upgamma_{fixed}$' = gamma_fixed_errorbar_lengths, 
                            '$\\upbeta_{random}$' = beta_rand_errorbar_lengths, 
                            '$\\upgamma_{random}$' = gamma_rand_errorbar_lengths, 
                            
                            
                            check.names = F)

kbl(x = summary_table, format = 'latex',
    linesep = c('\\cmidrule{1-8}', '\\cmidrule{1-8}', '\\cmidrule{1-8}'), #\\cmidrule(l{0.25cm}r{0.25cm}){1-8}
       longtable = T, booktabs = T, centering = T, escape = F,
    caption = 'Concise Summary of Results Across All Spacing Schedule Levels in Experiment 1', 
   align = c(rep('l', times = 4), rep('c', times = 4))) %>%
     #header
  column_spec(column = 1, width = '4.86cm') %>%
  column_spec(column = 2, width = '2cm') %>%
  column_spec(column = 3, width = '2cm') %>%
  column_spec(column = 4, width = '7cm') %>%
  column_spec(column = 5:8, width = '1cm') %>%
  add_header_above(header = c(' ' = 4, 'Error Bar Summary' = 4)) %>%
  
  #row highlighting 
  row_spec(1, background = "#DFDEDE") %>%

   #footnotes
  footnote(escape = F, threeparttable = T, general_title = '\\\\textit{Note.}\\\\hspace{-1.1pc}',
           general = "`Qualitative Summary' column indicates the number of measurements that obtains the greatest improvements in bias and precision across all day-unit parameters and manipulated nature-of-change values. `Error Bar Summary' columns list the error bar lengths that result for each day-unit parameter using the measurement number listed in the `Qualitative Summary' column. Note that error bar lengths were calculated by computing the average length across all manipulated measurement numbers for the nature-of-chang value listed in the `Optimal Nature-of-Change Value` column. Parameter names and population values are as follows: $\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter $\\\\in$ \\\\{80, 180, 280\\\\}; $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter = 20; $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter = 10; $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter = 4. NM = number of measurements.") %>%
  kable_styling(position = 'left') %>%
  landscape(margin = '2.54cm')

```

```{r summary-table-exp1-nc, echo=F}

#param_midpoint_equal <- exp_1_analytical$days %>%
#  filter(str_detect(string = measurement_spacing, pattern = 'Equal')) %>%
#  group_by(parameter, midpoint) %>%
#  summarize(errorbar_length = mean(errorbar_length, digits = 2)) %>%
#            #bias = round(mean(estimate) - mean(pop_value), digits = 2)) %>%
#  pivot_wider(names_from = midpoint, values_from =  errorbar_length)

#data set containing error bar widths for each parameter under each spacing schedule 
param_midpoint_equal <- exp_1_analytical$days %>%
  filter(str_detect(string = measurement_spacing, pattern = 'Equal')) %>%
  group_by(parameter, midpoint) %>%
  summarize(errorbar_length = mean(errorbar_length, digits = 2)) %>%
  pivot_wider(names_from = midpoint, values_from =  errorbar_length)


param_midpoint_time_inc <- exp_1_analytical$days %>%
  filter(str_detect(string = measurement_spacing, pattern = 'increasing')) %>%
  group_by(parameter, midpoint) %>%
  summarize(errorbar_length = mean(errorbar_length, digits = 2)) %>%
  pivot_wider(names_from = midpoint, values_from =  errorbar_length)


param_midpoint_time_dec <- exp_1_analytical$days %>%
  filter(str_detect(string = measurement_spacing, pattern = 'decreasing')) %>%
  group_by(parameter, midpoint) %>%
  summarize(errorbar_length = mean(errorbar_length, digits = 2)) %>%
  pivot_wider(names_from = midpoint, values_from =  errorbar_length)


param_midpoint_mid_ext <- exp_1_analytical$days %>%
  filter(str_detect(string = measurement_spacing, pattern = 'Middle-and')) %>%
  group_by(parameter, midpoint) %>%
  summarize(errorbar_length = mean(errorbar_length, digits = 2)) %>%
  pivot_wider(names_from = midpoint, values_from =  errorbar_length)

#combine vectors into a list 
errorbar_lengths_list <- list('equal' = param_midpoint_equal[[3]], 
                              'time_inc' = param_midpoint_time_inc[[2]], 
                              'time_dec' =  param_midpoint_time_dec[[4]], 
                              'mid_ext' =  param_midpoint_mid_ext[[3]])

beta_fixed_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[1])})))
gamma_fixed_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[2])})))
beta_rand_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[3])})))
gamma_rand_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[4])})))


summary_table <- data.frame('Spacing Schedule' = c('\\thead[lt]{Equal \\\\ 
                                                   (see Figure \\ref{fig:exp1_plot_equal} and Table \\ref{tab:errorbar-equal-nc})}',
                                                   '\\thead[lt]{Time-interval increasing \\\\ 
                                                   (see Figure \\ref{fig:exp1_plot_time_inc} and Table \\ref{tab:errorbar-time-inc-nc})}', 
                                                   '\\thead[lt]{Time-interval decreasing \\\\ 
                                                   (see Figure \\ref{fig:exp1_plot_time_dec} and Table \\ref{tab:errorbar-time-dec-nc})}', 
                                                   '\\thead[lt]{Middle-and-extreme \\\\ 
                                                   (see Figure \\ref{fig:exp1_plot_time_mid_ext} and Table \\ref{tab:errorbar-mid-ext-nc})}'), 
                            'Optimal Nature-of-Change Value' = c('$\\upbeta_{fixed}$ = 180', 
                                                         '$\\upbeta_{fixed}$ = 80', 
                                                         '$\\upbeta_{fixed}$ = 280', 
                                                         '$\\upbeta_{fixed}$ = 180'), 
                        
                            '$\\upbeta_{fixed}$' = beta_fixed_errorbar_lengths, 
                            '$\\upgamma_{fixed}$' = gamma_fixed_errorbar_lengths, 
                            '$\\upbeta_{random}$' = beta_rand_errorbar_lengths, 
                            '$\\upgamma_{random}$' = gamma_rand_errorbar_lengths, 
                            
                            
                            check.names = F)

kbl(x = summary_table, format = 'latex', digits = 2,
    linesep = c('\\cmidrule{1-6}', '\\cmidrule{1-6}', '\\cmidrule{1-6}'), 
       longtable = T, booktabs = T, centering = T, escape = F,
    caption = 'Nature-of-Change Values That Lead to Highest Modelling Accuracy for Each Spacing Schedule in Experiment 1', 
   align = c(rep('l', times = 1), rep('c', times = 4))) %>%
     #header
    add_header_above(header = c(' ' = 2, 'Error Bar Summary' = 4)) %>%
  column_spec(column = 1, width = '4.86cm') %>%
  column_spec(column = 2, width = '6cm') %>%
  column_spec(column = 3:6, width = '1.5cm') %>%
    footnote(escape = F, threeparttable = T, general_title = '\\\\textit{Note.}\\\\hspace{-1pc}',
           general = "`Error Bar Summary' lists error bar lengths for each day-unit parameter such that error bar lenghts are computed by taking the average error bar length value across all the number-of-measurement (NM) values (NM $\\\\in$ \\\\{5, 7, 9, 11\\\\}). Parameter names and population values are as follows: $\\\\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter $\\\\in$ \\\\{80, 180, 280\\\\}; $\\\\upgamma_{fixed}$ = fixed-effect halfway-triquarter delta parameter = 20; $\\\\upbeta_{random}$ = random-effect days-to-halfway elevation parameter = 10; $\\\\upgamma_{random}$ = random-effect halfway-triquarter delta parameter = 4.") %>%
  kable_styling(position = 'left') %>%
  landscape(margin = '2.54cm')
```

```{r schedule-nc-plot, echo=F, eval=F}
#log function data 
t <- 0:360
theta <- 3
alpha <- 3.32
beta_80 <- 80
beta_180 <- 180
beta_280 <- 280
gamma <- 20

#log function
log_function <- expression(theta + (alpha - theta)/(1 + exp((beta_180 - t)/gamma)))


log_function_80 <- theta + (alpha - theta)/((1 + exp((beta_80 - t)/gamma)))
log_function_180 <- theta + (alpha - theta)/((1 + exp((beta_180 - t)/gamma)))
log_function_280 <- theta + (alpha - theta)/((1 + exp((beta_280 - t)/gamma)))
log_function_180b <- theta + (alpha - theta)/((1 + exp((beta_280 - t)/gamma)))

log_func_wide <- data.frame(cbind(t, log_function_180, log_function_80, log_function_280, log_function_180b,
                                  'midpoint_80' = log_function_80,
                                  'midpoint_180' = log_function_180,
                                  'midpoint_280' = log_function_280))

log_func_long <- log_func_wide %>%
  pivot_longer(cols = 2:5,
               #names_pattern = c('^log', '^midpoint'), 
               names_to = c('midpoint'),# 'midpoint_grouping'), 
               values_to = c('curve_value'), 
                names_transform = factor) %>% 
  pivot_longer(cols = 2:4, 
             names_to = c('midpoint_grouping'), 
            values_to = c('curve_value_group'), 
             names_transform = factor)

#add grouping variable for curve_value_group to indicate if the curve is the optimal one or not
##find the rows where the number in midpoint matches that in midpoint_grouping 
#1) Extract number from midpoint column 

log_func_long$optimal <- factor(extract_numeric(x = log_func_long$midpoint) != extract_numeric(x = log_func_long$midpoint_grouping))

log_func_long$midpoint <-  recode_factor(log_func_long$midpoint,   
                                    'log_function_180' = "atop(bold(A:beta[fixed]==180~With~Equal~Spacing), phantom(randomtextforalignment))",
                                    'log_function_80' = "atop(bold(B:beta[fixed]==80~With~`Time-Interval`~ phantom(randomtextfor)), bold(Increasing~Spacing)~phantom(text~forleftaligniextrattt))",
                                    'log_function_280' = "atop(bold(C:beta[fixed]==280~With~`Time-Interval`~ phantom(randomtextfor)), bold(Decreasing~Spacing)~phantom(text~forleftaligniextratta))",
                                    'log_function_180b' = "atop(bold(D:beta[fixed]==180~With~`Middle-and-`~phantom(text~forleftaligni)), bold(`Extreme`~Spacing)~phantom(text~forleftalandalmorerand))")


#spacing schedule data
num_measurements <- 5
time_period <- 360 
smallest_int_length <- 30
schedules <- c('equal', 'time_inc', 'time_dec', 'mid_ext')

list_out <- unlist(do.call(rbind, pmap(.l = list('num_measurements' = num_measurements, 
               'time_period' = time_period, 
               'smallest_int_length' = smallest_int_length, 
               measurement_spacing = schedules), 
     .f = compute_measurement_schedule))[, 2])

measurement_days_wide <- data.frame(matrix(data = list_out, nrow = 4, ncol = num_measurements, byrow = T, 
       dimnames = list(schedules, sprintf('measurement_day_%d', 1:num_measurements)))) %>%
  rownames_to_column(var = 'schedule')

measurement_days_wide$midpoint <- c('180', '80', '280', '180a')

#add midpoint 
measurement_days_wide$midpoint <-  recode_factor(measurement_days_wide$midpoint,  
                                    '180' = "atop(bold(A:beta[fixed]==180~With~Equal~Spacing), phantom(randomtextforalignment))",
                                    '80' = "atop(bold(B:beta[fixed]==80~With~`Time-Interval`~ phantom(randomtextfor)), bold(Increasing~Spacing)~phantom(text~forleftaligniextrattt))",
                                    '280' = "atop(bold(C:beta[fixed]==280~With~`Time-Interval`~ phantom(randomtextfor)), bold(Decreasing~Spacing)~phantom(text~forleftaligniextratta))",
                                    '180a' = "atop(bold(D:beta[fixed]==180~With~`Middle-and-`~phantom(text~forleftaligni)), bold(`Extreme`~Spacing)~phantom(text~forleftalandalmorerand))")

measurement_days_long <- measurement_days_wide %>% 
  pivot_longer(cols = 2:(num_measurements + 1), names_to = 'measurement_num', values_to = 'day_number') %>%
  mutate_if(.predicate = is.character, .funs = factor)

#add curve values; match log_func_long on midpoint and then add the corresponding curve value 
measurement_days_long <- left_join(x = measurement_days_long, y = log_func_long, by = c('midpoint', 'day_number' = 't'))



#combine vectors into a list 
errorbar_lengths_list <- list('equal' = round(param_midpoint_equal[[3]], digits = 2),  
                              'time_inc' = round(param_midpoint_time_inc[[2]], digits = 2), 
                              'time_dec' =  round(param_midpoint_time_dec[[4]], digits = 2), 
                              'mid_ext' =  round(param_midpoint_mid_ext[[3]], digits = 2))

beta_fixed_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[1])})))
gamma_fixed_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[2])})))
beta_rand_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[3])})))
gamma_rand_errorbar_lengths <- as.numeric(unlist(lapply(X = errorbar_lengths_list, FUN = function(x){return(x[4])})))

equation_data <- data.frame(
  midpoint =c(rep("atop(bold(A:beta[fixed]==180~With~Equal~Spacing), phantom(randomtextforalignment))", times = 4), 
               rep("atop(bold(B:beta[fixed]==80~With~`Time-Interval`~ phantom(randomtextfor)), bold(Increasing~Spacing)~phantom(text~forleftaligniextrattt))", times = 4), 
               rep("atop(bold(C:beta[fixed]==280~With~`Time-Interval`~ phantom(randomtextfor)), bold(Decreasing~Spacing)~phantom(text~forleftaligniextratta))", times = 4), 
               rep("atop(bold(D:beta[fixed]==180~With~`Middle-and-`~phantom(text~forleftaligni)), bold(`Extreme`~Spacing)~phantom(text~forleftalandalmorerand))", times = 4)),
  
  label = c(paste("beta[fixed] == ", beta_fixed_errorbar_lengths[1], sep = ''), 
            paste("gamma[fixed] == ", gamma_fixed_errorbar_lengths[1], sep = ''), 
            paste("beta[random] == ", beta_rand_errorbar_lengths[1], sep = ''), 
            paste("gamma[random] == ", gamma_rand_errorbar_lengths[1], sep = ''), 
            
            paste("beta[fixed] == ", beta_fixed_errorbar_lengths[2], sep = ''), 
            paste("gamma[fixed] == ", gamma_fixed_errorbar_lengths[2], sep = ''), 
            paste("beta[random] == ", beta_rand_errorbar_lengths[2], sep = ''), 
            paste("gamma[random] == ", gamma_rand_errorbar_lengths[2], sep = ''), 
            
            paste("beta[fixed] == ", beta_fixed_errorbar_lengths[3], sep = ''), 
            paste("gamma[fixed] == ", gamma_fixed_errorbar_lengths[3], sep = ''), 
            paste("beta[random] == ", beta_rand_errorbar_lengths[3], sep = ''), 
            paste("gamma[random] == ", gamma_rand_errorbar_lengths[3], sep = ''), 
            
            paste("beta[fixed] == ", beta_fixed_errorbar_lengths[4], sep = ''), 
            paste("gamma[fixed] == ", gamma_fixed_errorbar_lengths[4], sep = ''), 
            paste("beta[random] == ", beta_rand_errorbar_lengths[4], sep = ''), 
            paste("gamma[random] == ", gamma_rand_errorbar_lengths[4], sep = '')), 

  x = c(rep(c(-10, -10, -3, 0), times = 4)),
        #280, 280, 287, 290, 
        #rep(c(80, 80, 87, 90), times = 2)), 
  y = c(rep(c(3.24, 3.20, 3.16, 3.12), times = 4)))


#add optimal spacing midpoint column
midpoint_plot <- ggplot(data = log_func_long, mapping = aes(x = t, y = curve_value_group, 
                                                            group = midpoint_grouping, color = optimal, alpha = optimal)) + 
  geom_line(size = 4) +
  scale_y_continuous(name = 'Curve Value (Likert Units [Scale 1-5])') + 
  geom_point(data = measurement_days_long, aes(x = day_number, y = curve_value_group,
                                               group = midpoint_grouping, color = optimal, alpha = optimal), size = 15) +
  scale_color_manual(name = 'Optimal Curve?', values = c('#0C0301', '#e0e0e0'),  labels = c('Yes', 'No')) + 
  scale_alpha_manual(name = 'Optimal Curve?', values = c(1, 0.65),labels = c('Yes', 'No')) + 
  
  geom_text(data = equation_data, inherit.aes = F, mapping = aes(x = x, y = y, label = label), parse = T, size = 17.5) +
  facet_wrap_custom( ~ midpoint, scales = "free", ncol = 2, nrow = 2 , dir = 'h',
                     labeller = label_parsed,  

                            scale_overrides = list(
                            scale_override(1,
                              scale_x_continuous(name = 'Measurement Number', 
                                breaks = c(0, 80, 120, 180, 240, 300, 360),
                                limits = c(-60, 360))), 
                          
                             scale_override(which = 2,
                              scale_x_continuous(name = 'Measurement Number', 
                                breaks = c(0, 60, 120, 180, 240, 300, 360),
                                limits = c(-60, 360))), 
                            
                            scale_override(which = 3,
                              scale_x_continuous(name = 'Measurement Number', 
                                breaks = c(0, 60, 120, 180, 280, 360),
                                limits = c(-60, 360))), 
                            
                              
                            scale_override(which = 4,
                              scale_x_continuous(name = 'Measurement Number', 
                                breaks = c(0, 60, 120, 180, 240, 300, 360),
                                limits = c(-60, 360))))) +
  
     
                         
  theme_classic(base_family = 'Helvetica') + 
  theme(
      #panel details
      strip.background = element_rect(fill = "white", color = "white"),
      #original text size = 60, 150 for pre-results figures
      strip.text.x = element_text(face = 'bold', hjust = 0, size = 60, margin = unit(c(t = 0, r = 0, b = 1, l = 0), "cm")),

      #axis details
      axis.text = element_text(size = 60, color = 'black'),
      axis.title = element_text(size = 70),
       
      #axis.title.x.bottom = element_markdown(),
      axis.line = element_line(size = 2),
      axis.ticks.length.x = unit(x = 1, units = 'cm'),
      axis.title.x = element_text(margin = unit(c(3, 0, 0, 0), "cm")),
      axis.title.y = element_text(margin = unit(c(t = 0, r = 3, b = 0, l = 0), units = 'cm')),
      axis.ticks = element_line(size = 2, colour = 'black'),
      axis.ticks.length.y =  unit(x = 1, units = 'cm'),

      #legend details
      legend.text = element_text(size = 50),
      legend.margin = margin(unit(c(0, 0, 0, 10), "cm")),
      legend.title = element_text(size = 60),
      legend.key.size = unit(3, 'cm'),
      legend.position = c(0.5, 0.52), 
      legend.direction = 'vertical',
      legend.box.background = element_rect(colour = 'black', size = 3),

      #panel details
      panel.spacing.y = unit(x = 18, units = 'cm'), 
       panel.spacing.x = unit(x = 2, units = 'cm')) 


set_panel_size(p = midpoint_plot, height = unit(x = 28, units = 'cm'),
                 width = unit(x = 40, units = 'cm'),
                 file =  'Figures/midpoint_plot.pdf')
```

### When the Nature of Change is Unknown, How Should Measurements be Spaced?

Table \ref{tab:summary-table-exp1} provides a summary of the results for each spacing schedule. Text within the 'Low Bias' column indicates the number of measurements needed to obtain low bias in the estimation of all day-unit parameters across all manipulated nature-of-change values for each spacing schedule. Text within the 'Qualitative Summary' column indicates the number of measurements that obtains the largest improvements in bias and precision across all manipulated nature-of-change values for each spacing schedule. The 'Error Bar Summary' columns list the error bar lengths obtained for each day-unit parameter of the logistic function using the measurement number listed in the 'Qualitative Summary' column. Importantly, the error bar lengths in the 'Error Bar Summary' column are obtained by computing the average length across all manipulated nature-of-change values for the measurement number listed Qualitative Summary' column.  The following number of measurements are needed to obtain low bias and the greatest improvements in bias and precision across all manipulated nature-of-change values for all day-unit parameters under each spacing schedule: 

* equal spacing: nine or more measurements to obtain low bias and seven measurements to obtain the greatest improvements in bias and precision. 
* time-interval increasing spacing: nine or more measurements to obtain low bias and nine measurements to obtain the greatest improvements in bias and precision. 
* time-interval decreasing spacing: nine or more measurements to obtain low bias and nine measurements to obtain the greatest improvements in bias and precision. 
* middle-and-extreme spacing: 11  measurements to obtain low bias and nine measurements to obtain the greatest improvements in bias and precision. 

An important point to mention is that the error bar lengths for each day-unit parameter across each spacing schedule are comparable.  That is, each spacing schedule obtains similar modelling accuracy when using the number of measurements listed in the 'Qualitative Summary` column. Because modelling accuracy is similar across the spacing schedules, then the schedule that uses the fewest number of measurements to obtain the greatest improvements in bias and precision can be said to most effectively model change when the nature of change is unknown. With equal spacing using fewer measurements than all the other manipulated spacing schedules to obtain similar modelling accuracy---using seven measurements instead of the nine measurements use by all other spacing schedules---then equal spacing is the most effective schedule to use when the nature of change is unknown. 


The finding that equal spacing is best is certainly not unexpected.

Also mention that a longitudinal study may need far more measurements that three. 


```{=tex}
\begin{figure}[H]
  \caption{Nature-of-Change Curves That Result in the Highest Modelling Accuracy for Each Spacing Schedule in Experiment 1}
  \label{fig:midpoint_plot}
  \includegraphics{Figures/midpoint_plot} 
\end{figure}
\begin{figure}[H]
 \figurefootnote{Panel A: Measurement sampling locations on each manipulated nature-of-change curve under equal spacing. Panel B: Measurement sampling locations on each manipulated nature-of-change curve under time-interval increasing spacing. Panel C: Measurement sampling locations on each manipulated nature-of-change curve under time-interval decreasing spacing. Panel D: Measurement sampling locations on each manipulated nature-of-change curve under middle-and-extreme spacing. Black curves indicate the natures of change that lead to the highest modelling accuracy for each spacing schedule, and so are optimal. Gray curves indicate the natures of change that lead to suboptimal modelling accuracy for each spacing schedule, and so are not optimal. Text on each panel indicates the error bar lengths that result from modelling the optimal nature of change (see Table \ref{fig:summary-table-exp1-nc}).}
\end{figure}
```








